{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "nFAoqyGGrxVL",
        "outputId": "346b54a9-520f-463d-dd2a-9c8fd31d545d"
      },
      "outputs": [],
      "source": [
        "%load_ext dotenv\n",
        "%dotenv\n",
        "import os\n",
        "\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "os.environ['WANDB_NOTEBOOK_NAME'] = 'New GAN.ipynb'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDr49Pil6qRB",
        "outputId": "80ee8653-a152-4046-9091-8e64a5daf18d"
      },
      "outputs": [],
      "source": [
        "!pip install -U keras tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdbhS3aw_CdD"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCSGKZcJDnx7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from keras.utils import (\n",
        "    to_categorical,\n",
        ")  # Only for categorical one hot encoding\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorboard\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow_datasets.core.utils import gcs_utils\n",
        "\n",
        "gcs_utils._is_gcs_disabled = True\n",
        "import datetime\n",
        "from IPython.display import clear_output\n",
        "import tqdm\n",
        "import pickle\n",
        "import timeit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M-WLAyP6s5x",
        "outputId": "958e4297-9911-4a48-c5e0-298ed9bc1aaa"
      },
      "outputs": [],
      "source": [
        "print(tf.__version__, \"GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIGZKH0erxVO"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./logs/\n",
        "RANDOM_SEED=42\n",
        "MODEL_NAME = 'NewGAN'\n",
        "# MODEL_PATH = os.path.join('/content/gdrive/My Drive/AI Research/GANs/models/', MODEL_NAME)\n",
        "MODEL_PATH = os.path.join('models', MODEL_NAME)\n",
        "TRAIN_LOGDIR = os.path.join(\"logs\", \"tensorflow\", MODEL_NAME) # Sets up a log directory.\n",
        "# Make sure these directories exist\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "os.makedirs(TRAIN_LOGDIR, exist_ok=True)\n",
        "# Start a profiler server before your model runs.\n",
        "tf.profiler.experimental.server.start(6009)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdn22plHrxVO",
        "outputId": "bed30202-1e11-4513-f45f-fdb0129ba2a7"
      },
      "outputs": [],
      "source": [
        "# Some Optimizations\n",
        "import ctypes\n",
        "\n",
        "_libcudart = ctypes.CDLL('libcudart.so')\n",
        "# Set device limit on the current device\n",
        "# cudaLimitMaxL2FetchGranularity = 0x05\n",
        "pValue = ctypes.cast((ctypes.c_int*1)(), ctypes.POINTER(ctypes.c_int))\n",
        "_libcudart.cudaDeviceSetLimit(ctypes.c_int(0x05), ctypes.c_int(128))\n",
        "_libcudart.cudaDeviceGetLimit(pValue, ctypes.c_int(0x05))\n",
        "assert pValue.contents.value == 128\n",
        "tf.compat.v1.ConfigProto.force_gpu_compatible=True\n",
        "\n",
        "keras.config.set_image_data_format('channels_last')\n",
        "\n",
        "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
        "os.environ['TF_GPU_THREAD_COUNT']='1'\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "\n",
        "keras.config.enable_unsafe_deserialization()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzC1njsS_VmS"
      },
      "outputs": [],
      "source": [
        "# tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "# keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "precision_policy = keras.mixed_precision.Policy('float32')\n",
        "keras.mixed_precision.set_global_policy(precision_policy)\n",
        "tf.keras.mixed_precision.set_global_policy(precision_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eQiTgWLrkAT"
      },
      "outputs": [],
      "source": [
        "normalizeImage = lambda x: tf.divide(tf.subtract(tf.cast(x, dtype=precision_policy.compute_dtype), 127.5), 127.5)\n",
        "denormalizeImage = lambda x: tf.cast(tf.add(tf.multiply(x, 127.5), 127.5), dtype=tf.uint8)\n",
        "\n",
        "def plotImages(imgs):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "    for i in range(imgs.shape[0]):\n",
        "        plt.subplot(8, 8, i + 1)\n",
        "        plt.imshow(tf.cast(denormalizeImage(imgs[i, :, :, :]), tf.uint8))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6l7-LaorxVO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class EqualizeLearningRate(layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Reference from WeightNormalization implementation of TF Addons\n",
        "    EqualizeLearningRate wrapper works for keras CNN and Dense (RNN not tested).\n",
        "    ```python\n",
        "      net = EqualizeLearningRate(\n",
        "          layers.Conv2D(2, 2, activation='relu'),\n",
        "          input_shape=(32, 32, 3),\n",
        "          data_init=True)(x)\n",
        "      net = EqualizeLearningRate(\n",
        "          layers.Conv2D(16, 5, activation='relu'),\n",
        "          data_init=True)(net)\n",
        "      net = EqualizeLearningRate(\n",
        "          layers.Dense(120, activation='relu'),\n",
        "          data_init=True)(net)\n",
        "      net = EqualizeLearningRate(\n",
        "          layers.Dense(n_classes),\n",
        "          data_init=True)(net)\n",
        "    ```\n",
        "    Arguments:\n",
        "      layer: a layer instance.\n",
        "    Raises:\n",
        "      ValueError: If `Layer` does not contain a `kernel` of weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        super(EqualizeLearningRate, self).__init__(layer, **kwargs)\n",
        "        self._track_trackable(layer, name=\"layer\")\n",
        "        self.is_rnn = isinstance(self.layer, layers.RNN)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build `Layer`\"\"\"\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        self.input_spec = layers.InputSpec(shape=[None] + input_shape[1:])\n",
        "\n",
        "        if not self.layer.built:\n",
        "            self.layer.build(input_shape)\n",
        "\n",
        "        kernel_layer = self.layer.cell if self.is_rnn else self.layer\n",
        "\n",
        "        if not hasattr(kernel_layer, \"kernel\"):\n",
        "            raise ValueError(\n",
        "                \"`EqualizeLearningRate` must wrap a layer that\"\n",
        "                \" contains a `kernel` for weights\"\n",
        "            )\n",
        "\n",
        "        if self.is_rnn:\n",
        "            kernel = kernel_layer.recurrent_kernel\n",
        "        else:\n",
        "            kernel = kernel_layer.kernel\n",
        "\n",
        "        # He constant\n",
        "        self.fan_in, self.fan_out = self._compute_fans(kernel.shape)\n",
        "        self.he_constant = tf.Variable(\n",
        "            1.0 / np.sqrt(self.fan_in), dtype=tf.float32, trainable=False\n",
        "        )\n",
        "\n",
        "        self.v = kernel\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        \"\"\"Call `Layer`\"\"\"\n",
        "        # Multiply the kernel with the he constant.\n",
        "        kernel = self.v  # * self.he_constant\n",
        "\n",
        "        if self.is_rnn:\n",
        "            print(self.is_rnn)\n",
        "            self.layer.cell.recurrent_kernel = kernel\n",
        "            update_kernel = tf.identity(self.layer.cell.recurrent_kernel)\n",
        "        else:\n",
        "            self.layer.kernel = kernel\n",
        "            # update_kernel = tf.identity(self.layer.kernel)\n",
        "\n",
        "        # Ensure we calculate result after updating kernel.\n",
        "        # with tf.control_dependencies([update_kernel]):\n",
        "        outputs = self.layer(inputs)\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape(self.layer.compute_output_shape(input_shape).as_list())\n",
        "\n",
        "    def _compute_fans(self, shape, data_format=\"channels_last\"):\n",
        "        \"\"\"\n",
        "        From Official Keras implementation\n",
        "        Computes the number of input and output units for a weight shape.\n",
        "        # Arguments\n",
        "            shape: Integer shape tuple.\n",
        "            data_format: Image data format to use for convolution kernels.\n",
        "                Note that all kernels in Keras are standardized on the\n",
        "                `channels_last` ordering (even when inputs are set\n",
        "                to `channels_first`).\n",
        "        # Returns\n",
        "            A tuple of scalars, `(fan_in, fan_out)`.\n",
        "        # Raises\n",
        "            ValueError: in case of invalid `data_format` argument.\n",
        "        \"\"\"\n",
        "        if len(shape) == 2:\n",
        "            fan_in = shape[0]\n",
        "            fan_out = shape[1]\n",
        "        elif len(shape) in {3, 4, 5}:\n",
        "            # Assuming convolution kernels (1D, 2D or 3D).\n",
        "            # TH kernel shape: (depth, input_depth, ...)\n",
        "            # TF kernel shape: (..., input_depth, depth)\n",
        "            if data_format == \"channels_first\":\n",
        "                receptive_field_size = np.prod(shape[2:])\n",
        "                fan_in = shape[1] * receptive_field_size\n",
        "                fan_out = shape[0] * receptive_field_size\n",
        "            elif data_format == \"channels_last\":\n",
        "                receptive_field_size = np.prod(shape[:-2])\n",
        "                fan_in = shape[-2] * receptive_field_size\n",
        "                fan_out = shape[-1] * receptive_field_size\n",
        "            else:\n",
        "                raise ValueError(\"Invalid data_format: \" + data_format)\n",
        "        else:\n",
        "            # No specific assumptions.\n",
        "            fan_in = np.sqrt(np.prod(shape))\n",
        "            fan_out = np.sqrt(np.prod(shape))\n",
        "        return fan_in, fan_out\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class SelfAttention(layers.Layer):\n",
        "    def __init__(self, channelReduce=1, name=None, initializers=\"glorot_uniform\", **kwargs):\n",
        "        super(SelfAttention, self).__init__(name=name, **kwargs)\n",
        "        self.channelReduce = channelReduce\n",
        "        self.initializers = initializers\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"name\": self.name}\n",
        "        base_config = super(SelfAttention, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def get_weights(self):\n",
        "        return [self.gamma, self.kernel_f, self.kernel_g, self.kernel_h]\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.channels = input_shape[-1]\n",
        "        self.filters_f_g = self.channels // self.channelReduce\n",
        "        self.filters_h = self.channels\n",
        "\n",
        "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
        "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
        "\n",
        "        # Create a trainable weight variable for this layer:\n",
        "        self.gamma = self.add_weight(\n",
        "            name=\"gamma\", shape=[1], initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "        self.kernel_f = self.add_weight(\n",
        "            shape=kernel_shape_f_g,\n",
        "            initializer=self.initializers,\n",
        "            name=\"kernel_f\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.kernel_g = self.add_weight(\n",
        "            shape=kernel_shape_f_g,\n",
        "            initializer=self.initializers,\n",
        "            name=\"kernel_g\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.kernel_h = self.add_weight(\n",
        "            shape=kernel_shape_h,\n",
        "            initializer=self.initializers,\n",
        "            name=\"kernel_h\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        super(SelfAttention, self).build(input_shape)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, input):\n",
        "        # input = [NHWC]\n",
        "        f_x = tf.nn.conv2d(input, filters=self.kernel_f, strides=(1, 1), padding=\"SAME\")\n",
        "        g_x = tf.nn.conv2d(input, filters=self.kernel_g, strides=(1, 1), padding=\"SAME\")\n",
        "        h_x = tf.nn.conv2d(input, filters=self.kernel_h, strides=(1, 1), padding=\"SAME\")\n",
        "\n",
        "        f_x_flat = hw_flatten(f_x)  # [N(HW)C]\n",
        "        g_x_flat = hw_flatten(g_x)  # [N(HW)C]\n",
        "\n",
        "        s = keras.backend.batch_dot(g_x_flat, keras.backend.permute_dimensions(f_x_flat, (0, 2, 1)))\n",
        "\n",
        "        beta = tf.nn.softmax(s, axis=-1)\n",
        "        o = keras.backend.batch_dot(beta, hw_flatten(h_x))\n",
        "\n",
        "        o = tf.reshape(o, shape=tf.shape(input))  # [bs, h, w, C]\n",
        "        x = self.gamma * o + input\n",
        "        return x\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class PixelNorm(keras.Layer):\n",
        "    def __init__(self, epsilon=1e-8, **kwargs):\n",
        "        super(PixelNorm, self).__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calculate the square of each element\n",
        "        squared_values = tf.square(inputs)\n",
        "        # Calculate the mean of the squared values across the channel dimension\n",
        "        mean_squared_values = tf.reduce_mean(squared_values, axis=-1, keepdims=True)\n",
        "        # Add epsilon for numerical stability before taking the square root\n",
        "        scaling_factor = tf.math.rsqrt(mean_squared_values + self.epsilon)\n",
        "        # Normalize the inputs by the scaling factor\n",
        "        normalized_inputs = inputs * scaling_factor\n",
        "        return normalized_inputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PixelNorm, self).get_config()\n",
        "        config.update({'epsilon': self.epsilon})\n",
        "        return config\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class FadeAdd(layers.Layer):\n",
        "    def __init__(self, alpha = None, **kwargs):\n",
        "        super(FadeAdd, self).__init__(**kwargs)\n",
        "        if alpha is None:\n",
        "            self.alpha = tf.Variable(initial_value=0.0, trainable=False)\n",
        "        elif not isinstance(alpha, tf.Variable):\n",
        "            self.alpha = tf.Variable(initial_value=alpha, trainable=False)\n",
        "        else:\n",
        "            self.alpha = alpha\n",
        "\n",
        "    def incrementAlpha(self, step=0.1):\n",
        "        self.alpha.assign(tf.minimum(self.alpha + step, 1.0))\n",
        "        # Debug print to check the value of alpha if needed\n",
        "        # print(\"New Alpha: \", self.alpha)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        new, old = inputs\n",
        "        return (new * self.alpha) + (old * (1 - self.alpha))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"alpha\": self.alpha.numpy()})\n",
        "        return config\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # Assuming the input is a list of two elements, both having the same shape\n",
        "        if isinstance(input_shape, list) and len(input_shape) == 2:\n",
        "            return input_shape[0]  # both 'new' and 'old' inputs should have the same shape\n",
        "        else:\n",
        "            raise ValueError(\"FadeAdd layer should be called on a list of two inputs.\")\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class MinibatchStddev(layers.Layer):\n",
        "    def __init__(self, group_size=4, name=None, **kwargs):\n",
        "        super(MinibatchStddev, self).__init__(name=name, **kwargs)\n",
        "        self.group_size = group_size\n",
        "\n",
        "    def call(self, inputs):\n",
        "        group_size = tf.minimum(self.group_size, tf.shape(inputs)[0])\n",
        "        shape = tf.shape(inputs)\n",
        "        minibatch = tf.reshape(inputs, (group_size, -1, shape[1], shape[2], shape[3]))\n",
        "        stddev = tf.sqrt(tf.reduce_mean(tf.square(minibatch - tf.reduce_mean(minibatch, axis=0)), axis=0) + 1e-8)\n",
        "        stddev = tf.reduce_mean(stddev, axis=[1, 2, 3], keepdims=True)\n",
        "        stddev = tf.tile(stddev, [group_size, shape[1], shape[2], 1])\n",
        "        return tf.concat([inputs, stddev], axis=-1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MinibatchStddev, self).get_config()\n",
        "        config.update({'group_size': self.group_size})\n",
        "        return config\n",
        "\n",
        "@tf.function(jit_compile=True)\n",
        "def hw_flatten(x):\n",
        "    inp_shape = tf.shape(x)\n",
        "    # inp_shape = x.shape\n",
        "    batch_size, height, width, channels = inp_shape[0], inp_shape[1], inp_shape[2], inp_shape[3]\n",
        "    shape = [batch_size, height * width, channels]\n",
        "    return tf.reshape(x, shape=shape)\n",
        "\n",
        "def layer_init_stddev(shape, gain=np.sqrt(2)):\n",
        "    \"\"\"Get the He initialization scaling term.\"\"\"\n",
        "    fan_in = np.prod(shape[:-1])\n",
        "    return gain / np.sqrt(fan_in)\n",
        "\n",
        "# @keras.saving.register_keras_serializable()\n",
        "class FakeLayer(layers.Layer):\n",
        "    def __init__(self, layer, name=None, **kwargs):\n",
        "        super(FakeLayer, self).__init__(name=name, **kwargs)\n",
        "        self.layer = layer\n",
        "        # self.trainable = True\n",
        "\n",
        "    def call(self, input):\n",
        "        return self.layer(input)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"name\": self.name, \"layer\": self.layer}\n",
        "        base_config = super(FakeLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def buildModel(layers, inpTensor):\n",
        "    layer = inpTensor\n",
        "    # print(\"Rebasing\")\n",
        "    for i in range(len(layers)):\n",
        "        # print(\"Adding layer: \", layer, \" -> \", layers[i])\n",
        "        layer = layers[i](layer)\n",
        "    # print(\"Done\")\n",
        "    return layer\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class PixelReshuffle(keras.Layer):\n",
        "    def __init__(self, upscale_factor, **kwargs):\n",
        "        super(PixelReshuffle, self).__init__(**kwargs)\n",
        "        self.upscale_factor = upscale_factor\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.depth_to_space(inputs, self.upscale_factor)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        height, width, channels = input_shape[1], input_shape[2], input_shape[3]\n",
        "        new_height = height * self.upscale_factor\n",
        "        new_width = width * self.upscale_factor\n",
        "        new_channels = channels // (self.upscale_factor ** 2)\n",
        "        return (input_shape[0], new_height, new_width, new_channels)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PixelReshuffle, self).get_config()\n",
        "        config.update({\"upscale_factor\": self.upscale_factor})\n",
        "        return config\n",
        "\n",
        "# def pixelReshuffle(x, upscale_factor):\n",
        "#     return tf.nn.depth_to_space(x, upscale_factor)\n",
        "\n",
        "class DepthToSpace(layers.Layer):\n",
        "    def __init__(self, block_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def call(self, input):\n",
        "        batch, height, width, depth = keras.shape(input)\n",
        "        depth = depth // (self.block_size**2)\n",
        "\n",
        "        x = keras.ops.reshape(\n",
        "            input, [batch, height, width, self.block_size, self.block_size, depth]\n",
        "        )\n",
        "        x = keras.transpose(x, [0, 1, 3, 2, 4, 5])\n",
        "        x = keras.reshape(\n",
        "            x, [batch, height * self.block_size, width * self.block_size, depth]\n",
        "        )\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFfIHKsuBrmN"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(1.0, dtype=precision_policy.compute_dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL3Qe-aerxVQ"
      },
      "outputs": [],
      "source": [
        "# from keras.src.utils.numerical_utils import normalize\n",
        "\n",
        "# @tf.function\n",
        "# @tf.function(jit_compile=True)\n",
        "def normalized_weights(vector_u: tf.Variable, kernel: tf.Variable, kernel_shape: tf.shape, power_iterations: int):\n",
        "    \"\"\"Generate spectral normalized weights.\n",
        "    This method returns the updated value for `self.kernel` with the\n",
        "    spectral normalized value, so that the layer is ready for `call()`.\n",
        "    \"\"\"\n",
        "    weights = tf.reshape(kernel, [-1, kernel_shape[-1]])\n",
        "    vec_u_val = vector_u.value\n",
        "\n",
        "    for _ in range(power_iterations):\n",
        "        vector_v = tf.nn.l2_normalize(\n",
        "            tf.matmul(vec_u_val, weights, transpose_b=True), axis=None\n",
        "        )\n",
        "        vec_u_val = tf.nn.l2_normalize(tf.matmul(vector_v, weights), axis=None)\n",
        "    # vector_u = tf.stop_gradient(vector_u)\n",
        "    # vector_v = tf.stop_gradient(vector_v)\n",
        "    sigma = tf.matmul(\n",
        "        tf.matmul(vector_v, weights), vec_u_val, transpose_b=True\n",
        "    )\n",
        "    kernel_val = tf.reshape(tf.divide(kernel, sigma), kernel_shape)\n",
        "    return vec_u_val, kernel_val\n",
        "\n",
        "# @tf.function\n",
        "def spectral_normalize_weights(vector_u: tf.Variable, kernel: tf.Variable, kernel_shape: tf.shape, power_iterations: int):\n",
        "    # new_vector_u, new_kernel = tf.cond(\n",
        "    #     tf.reduce_sum(kernel.value) == 0.0,\n",
        "    #     lambda: (vector_u.value, kernel.value),\n",
        "    #     lambda: normalized_weights(vector_u, kernel, kernel_shape, power_iterations)\n",
        "    # )\n",
        "    new_vector_u, new_kernel = normalized_weights(vector_u, kernel, kernel_shape, power_iterations)\n",
        "    vector_u.assign(new_vector_u)\n",
        "    kernel.assign(new_kernel)\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class SpectralNormalization(layers.Wrapper):\n",
        "    def __init__(self, layer, power_iterations=1, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        if power_iterations <= 0:\n",
        "            raise ValueError(\n",
        "                \"`power_iterations` should be greater than zero. Received: \"\n",
        "                f\"`power_iterations={power_iterations}`\"\n",
        "            )\n",
        "        self.power_iterations = power_iterations\n",
        "        self.is_separable_conv = isinstance(layer, keras.layers.SeparableConv2D)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        self.input_spec = layers.InputSpec(shape=[None] + list(input_shape[1:]))\n",
        "\n",
        "        if self.is_separable_conv:\n",
        "            self.kernel_depthwise = self.layer.depthwise_kernel\n",
        "            self.kernel_pointwise = self.layer.pointwise_kernel\n",
        "            self.kernel_depthwise_shape = self.kernel_depthwise.shape\n",
        "            self.kernel_pointwise_shape = self.kernel_pointwise.shape\n",
        "            self.vector_u_depthwise = self.add_weight(\n",
        "                shape=(1, self.kernel_depthwise_shape[-1]),\n",
        "                initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "                trainable=False,\n",
        "                name=\"vector_u_depthwise\",\n",
        "                dtype=self.kernel_depthwise.dtype,\n",
        "            )\n",
        "            self.vector_u_pointwise = self.add_weight(\n",
        "                shape=(1, self.kernel_pointwise_shape[-1]),\n",
        "                initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "                trainable=False,\n",
        "                name=\"vector_u_pointwise\",\n",
        "                dtype=self.kernel_pointwise.dtype,\n",
        "            )\n",
        "        else:\n",
        "            if hasattr(self.layer, \"kernel\"):\n",
        "                self.kernel = self.layer.kernel\n",
        "            elif hasattr(self.layer, \"embeddings\"):\n",
        "                self.kernel = self.layer.embeddings\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"{type(self.layer).__name__} object has no attribute 'kernel' \"\n",
        "                    \"nor 'embeddings'\"\n",
        "                )\n",
        "            self.kernel_shape = self.kernel.shape\n",
        "            self.vector_u = self.add_weight(\n",
        "                shape=(1, self.kernel_shape[-1]),\n",
        "                initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "                trainable=False,\n",
        "                name=\"vector_u\",\n",
        "                dtype=self.kernel.dtype,\n",
        "            )\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        if training:\n",
        "            if self.is_separable_conv:\n",
        "                spectral_normalize_weights(\n",
        "                    self.vector_u_depthwise,\n",
        "                    self.kernel_depthwise,\n",
        "                    self.kernel_depthwise_shape,\n",
        "                    self.power_iterations,\n",
        "                )\n",
        "                spectral_normalize_weights(\n",
        "                    self.vector_u_pointwise,\n",
        "                    self.kernel_pointwise,\n",
        "                    self.kernel_pointwise_shape,\n",
        "                    self.power_iterations,\n",
        "                )\n",
        "            else:\n",
        "                spectral_normalize_weights(\n",
        "                    self.vector_u, self.kernel, self.kernel_shape, self.power_iterations\n",
        "                )\n",
        "\n",
        "        output = self.layer(inputs)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return self.layer.compute_output_shape(input_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"power_iterations\": self.power_iterations}\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, **config}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXHcWA9BrxVQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "fmap_base = 8192\n",
        "fmap_max = 512\n",
        "fmap_decay = 1.0\n",
        "\n",
        "def nf(stage):\n",
        "    return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
        "\n",
        "class GeneratorBuilder():\n",
        "    def __init__(self,\n",
        "                latent_dim=512, start_res=4,\n",
        "                conv_normalizer: layers.Layer=FakeLayer,\n",
        "                normal_normalizer=PixelNorm,\n",
        "                activation_builder: lambda : layers.Layer=lambda : layers.LeakyReLU(alpha=0.2),\n",
        "                upsampling_builder: lambda : layers.Layer=lambda : layers.UpSampling2D((2, 2), interpolation='bilinear'),\n",
        "                alpha:tf.Variable=None,\n",
        "                kernel_constraint=None,\n",
        "                kernel_initializer_builder=lambda:keras.initializers.RandomNormal(mean=0.0, stddev=0.02)(),\n",
        "                freeze_oldlayers=False,\n",
        "                use_sepconv=False,\n",
        "                deviation_loss=keras.losses.mean_absolute_error,\n",
        "                **kwargs,\n",
        "            ):\n",
        "        self.conv_normalizer = conv_normalizer\n",
        "        self.activation_builder = activation_builder\n",
        "        self.normal_normalizer = normal_normalizer\n",
        "        self.general_layer_configs = {\n",
        "            \"kernel_constraint\": kernel_constraint,\n",
        "            \"kernel_initializer\": kernel_initializer_builder(),\n",
        "            \"use_bias\": False,\n",
        "        }\n",
        "        self.conv_layer_configs = {\n",
        "            **self.general_layer_configs,\n",
        "            \"padding\": \"same\",\n",
        "        }\n",
        "        self.separableconv_layer_configs = {\n",
        "            \"use_bias\": False,\n",
        "            \"padding\": \"same\",\n",
        "            \"depthwise_constraint\": kernel_constraint,\n",
        "            \"depthwise_initializer\": kernel_initializer_builder(),\n",
        "            \"pointwise_constraint\": kernel_constraint,\n",
        "            \"pointwise_initializer\": kernel_initializer_builder(),\n",
        "        }\n",
        "        self.current_res = None\n",
        "        self.start_res = start_res\n",
        "        self.latent_dim = latent_dim\n",
        "        self.upsampling_builder = upsampling_builder\n",
        "        self.alpha = alpha\n",
        "        self.freeze_oldlayers = freeze_oldlayers\n",
        "        self.use_sepconv = use_sepconv\n",
        "        self.downscaler = layers.AveragePooling2D((2, 2))\n",
        "        self.global_block_id = 0\n",
        "        self.deviation_loss = deviation_loss\n",
        "\n",
        "    def gen_block_name(self, res, blocktype, suffix='', use_blockid=True):\n",
        "        if use_blockid == True:\n",
        "            blockid = self.global_block_id\n",
        "            self.global_block_id += 1\n",
        "            if suffix == '':\n",
        "                return f\"{res}x{res}_{blocktype}-{blockid}\"\n",
        "            return f\"{res}x{res}_{blocktype}-{blockid}_{suffix}\"\n",
        "        else:\n",
        "            if suffix == '':\n",
        "                return f\"{res}x{res}_{blocktype}\"\n",
        "            return f\"{res}x{res}_{blocktype}_{suffix}\"\n",
        "\n",
        "    def freeze_layers(self):\n",
        "        for layer in self.current_model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    def unfreeze_layers(self):\n",
        "        for layer in self.current_model.layers:\n",
        "            layer.trainable = True\n",
        "\n",
        "    def make_conv(self, res, features, kernel_size=(3, 3), blockid=0, suffix=\"\"):\n",
        "        if self.use_sepconv == True:\n",
        "            return self.conv_normalizer(layers.SeparableConv2D(features, kernel_size=kernel_size, **self.separableconv_layer_configs), name=self.gen_block_name(res, \"sepconv\", suffix))\n",
        "        else:\n",
        "            return self.conv_normalizer(layers.Conv2D(features, kernel_size=kernel_size, **self.conv_layer_configs), name=self.gen_block_name(res, \"conv\", suffix))\n",
        "\n",
        "    def make_output_block(self, res, input_tensor):\n",
        "        output_tensor = self.conv_normalizer(\n",
        "            layers.Conv2D(3, kernel_size=(1, 1), **self.conv_layer_configs),\n",
        "            name=self.gen_block_name(res, 'conv', 'output', use_blockid=False)\n",
        "        )(input_tensor)\n",
        "        output_tensor = layers.Activation('tanh', name=self.gen_block_name(res, \"output\", use_blockid=False))(output_tensor)\n",
        "        return output_tensor\n",
        "\n",
        "    def build_base(self, latent_dim, start_res):\n",
        "        log_res = int(np.log2(start_res))\n",
        "        features = nf(log_res - 1)\n",
        "\n",
        "        input_tensor = layers.Input(shape=(latent_dim,))\n",
        "\n",
        "        x = layers.Reshape((start_res, start_res, latent_dim // ((start_res)**2)))(input_tensor)\n",
        "        x = PixelNorm()(x)\n",
        "        # PixelReshuffle(2)\n",
        "        x = self.conv_normalizer(layers.Conv2D(features, kernel_size=(1, 1),**self.conv_layer_configs), name=self.gen_block_name(start_res, \"conv\", \"base_first\"))(x)\n",
        "        x = self.activation_builder()(x)\n",
        "        x = self.normal_normalizer()(x)\n",
        "\n",
        "        x = self.make_conv(start_res, features, kernel_size=(3, 3), suffix=\"base\")(x)\n",
        "        x = self.activation_builder()(x)\n",
        "        x = self.normal_normalizer()(x)\n",
        "\n",
        "        x = self.make_conv(start_res, features, kernel_size=(3, 3), suffix=\"base\")(x)\n",
        "        x = self.activation_builder()(x)\n",
        "        x = self.normal_normalizer(name=self.gen_block_name(start_res, \"final\", use_blockid=False))(x)\n",
        "\n",
        "        output_tensor = self.make_output_block(start_res, x)\n",
        "\n",
        "        print(\"Generator base built with shape \", output_tensor.shape)\n",
        "        self.current_model = keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
        "        self.current_model.compile(run_eagerly=False, jit_compile=True)\n",
        "        self.higher_model = None\n",
        "        self.current_res = start_res\n",
        "\n",
        "    def add_stage(self, res=0, freeze_previous_output=False):\n",
        "        if res == 0:\n",
        "            res = self.current_res * 2\n",
        "            assert res == self.current_model.output.shape[1] * 2\n",
        "        log_res = int(np.log2(res))\n",
        "        features = nf(log_res - 1)\n",
        "        # We need to add stage after the last layer\n",
        "\n",
        "        if self.freeze_oldlayers:\n",
        "            self.freeze_layers()\n",
        "\n",
        "        last_final_tensor = self.current_model.get_layer(self.gen_block_name(res // 2, \"final\", use_blockid=False)).output\n",
        "        print(\"Adding stage \", res)\n",
        "        x = PixelReshuffle(2)(last_final_tensor)    # Upscale\n",
        "        # x = pixelReshuffle(last_final_tensor, 2)\n",
        "\n",
        "        x = self.make_conv(res, features, kernel_size=(3, 3))(x)\n",
        "        x = self.activation_builder()(x)\n",
        "        x = self.normal_normalizer()(x)\n",
        "\n",
        "        x = self.make_conv(res, features, kernel_size=(3, 3), blockid=1)(x)\n",
        "        x = self.activation_builder()(x)\n",
        "        x = self.normal_normalizer(name=self.gen_block_name(res, \"final\", use_blockid=False))(x)\n",
        "\n",
        "        output_tensor = self.make_output_block(res, x)      # 16x16 RGB Image Output\n",
        "        input_tensor = self.current_model.input\n",
        "        last_output_tensor = self.current_model.get_layer(self.gen_block_name(res // 2, \"output\", use_blockid=False)).output\n",
        "\n",
        "        self.current_model = keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
        "        self.current_model.compile(run_eagerly=False, jit_compile=True)\n",
        "        self.lower_model = keras.Model(inputs=input_tensor, outputs=last_final_tensor)\n",
        "        self.lower_model.compile(run_eagerly=False, jit_compile=True)\n",
        "        self.higher_model = keras.Model(inputs=last_final_tensor, outputs=[last_output_tensor, output_tensor])\n",
        "        self.higher_model.compile(run_eagerly=False, jit_compile=True)\n",
        "\n",
        "        self.current_res = res\n",
        "        if freeze_previous_output == True:\n",
        "            self.set_last_output_trainable(False)\n",
        "\n",
        "    def set_last_output_trainable(self, trainable=True):\n",
        "        if self.higher_model == None:\n",
        "            return\n",
        "        # print(\"Setting last output trainable: \", trainable, \"for res \", self.current_res)\n",
        "        last_conv_output_layer : keras.layers.Layer = self.higher_model.get_layer(self.gen_block_name(self.current_res // 2, \"conv\", \"output\", use_blockid=False))\n",
        "        last_conv_output_layer.trainable = trainable\n",
        "\n",
        "    def grow(self, res=0, freeze_previous_output=False):\n",
        "        if self.current_res is None:\n",
        "            self.build_base(self.latent_dim, self.start_res)\n",
        "        else:\n",
        "            self.add_stage(res, freeze_previous_output)\n",
        "        # return self.model\n",
        "\n",
        "    def get_layers(self):\n",
        "        if self.higher_model == None:\n",
        "            return self.current_model.layers\n",
        "        return self.lower_model.layers + self.higher_model.layers\n",
        "\n",
        "    def predict(self, input, training=False) -> tf.Tensor:\n",
        "        return self.current_model(input, training=training)\n",
        "\n",
        "    def composite_predict(self, input, training=True):\n",
        "        if self.higher_model == None:\n",
        "            preds = self.predict(input, training=training)\n",
        "            return preds, None, None, 0\n",
        "        preds = self.lower_model(input, training=training)\n",
        "        lower_out, higher_out = self.higher_model(preds, training=True)\n",
        "        high_to_low = self.downscaler(higher_out)\n",
        "        # print(lower_out.shape, higher_out.shape, high_to_low.shape)\n",
        "        dev_loss = self.deviation_loss(lower_out, high_to_low)\n",
        "        return higher_out, lower_out, high_to_low, tf.reduce_mean(dev_loss)\n",
        "\n",
        "    def get_trainable_variables(self):\n",
        "        if self.higher_model == None:\n",
        "            return self.current_model.trainable_variables\n",
        "        else:\n",
        "            return self.higher_model.trainable_variables + self.lower_model.trainable_variables\n",
        "\n",
        "class DescriminatorBuilder():\n",
        "    def __init__(self,\n",
        "                start_res=4,\n",
        "                conv_normalizer: layers.Layer=SpectralNormalization,\n",
        "                normal_normalizer=lambda: FakeLayer(lambda x: x),\n",
        "                activation_builder: lambda : layers.Layer=lambda : layers.LeakyReLU(alpha=0.2),\n",
        "                alpha:tf.Variable=tf.Variable(0.0, dtype=precision_policy.compute_dtype),\n",
        "                kernel_constraint=None,\n",
        "                kernel_initializer_builder=lambda:keras.initializers.RandomNormal(mean=0.0, stddev=0.02)(),\n",
        "                freeze_oldlayers=False,\n",
        "                use_sepconv=False,\n",
        "                **kwargs,\n",
        "            ):\n",
        "        self.conv_normalizer = conv_normalizer\n",
        "        self.activation_builder = activation_builder\n",
        "        self.normal_normalizer = normal_normalizer\n",
        "        self.general_layer_configs = {\n",
        "            \"kernel_constraint\": kernel_constraint,\n",
        "            \"kernel_initializer\": kernel_initializer_builder(),\n",
        "            \"use_bias\": False,\n",
        "        }\n",
        "        self.conv_layer_configs = {\n",
        "            **self.general_layer_configs,\n",
        "            \"padding\": \"same\",\n",
        "        }\n",
        "        self.separableconv_layer_configs = {\n",
        "            \"use_bias\": False,\n",
        "            \"padding\": \"same\",\n",
        "            \"depthwise_constraint\": kernel_constraint,\n",
        "            \"depthwise_initializer\": kernel_initializer_builder(),\n",
        "            \"pointwise_constraint\": kernel_constraint,\n",
        "            \"pointwise_initializer\": kernel_initializer_builder(),\n",
        "        }\n",
        "        self.current_res = None\n",
        "        self.start_res = start_res\n",
        "        self.alpha = alpha\n",
        "        self.freeze_oldlayers = freeze_oldlayers\n",
        "        self.use_sepconv = use_sepconv\n",
        "        self.global_block_id = 0\n",
        "        print(\"Use SepConv: \", self.use_sepconv)\n",
        "\n",
        "    def gen_block_name(self, res, blocktype, suffix='', use_blockid=True):\n",
        "        if use_blockid == True:\n",
        "            blockid = self.global_block_id\n",
        "            self.global_block_id += 1\n",
        "            if suffix == '':\n",
        "                return f\"{res}x{res}_{blocktype}-{blockid}\"\n",
        "            return f\"{res}x{res}_{blocktype}-{blockid}_{suffix}\"\n",
        "        else:\n",
        "            if suffix == '':\n",
        "                return f\"{res}x{res}_{blocktype}\"\n",
        "            return f\"{res}x{res}_{blocktype}_{suffix}\"\n",
        "\n",
        "    def freeze_layers(self):\n",
        "        for layer in self.model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    def unfreeze_layers(self):\n",
        "        for layer in self.model.layers:\n",
        "            layer.trainable = True\n",
        "\n",
        "    def make_conv(self, res, features, kernel_size=(3, 3), blockid=0, suffix=\"\"):\n",
        "        if self.use_sepconv == True:\n",
        "            return self.conv_normalizer(layers.SeparableConv2D(features, kernel_size=kernel_size, **self.separableconv_layer_configs), name=self.gen_block_name(res, \"sepconv\", suffix))\n",
        "        else:\n",
        "            return self.conv_normalizer(layers.Conv2D(features, kernel_size=kernel_size, **self.conv_layer_configs), name=self.gen_block_name(res, \"conv\", suffix))\n",
        "\n",
        "    def build_base(self, start_res):\n",
        "        log_res = int(np.log2(start_res))\n",
        "        features = nf(log_res - 1)\n",
        "\n",
        "        input_tensor = layers.Input(shape=(start_res, start_res, 3))\n",
        "\n",
        "        initial_layers = []\n",
        "        # initial_layers.append(MinibatchStddev())\n",
        "        initial_layers.append(self.make_conv(start_res, features, kernel_size=(1, 1), suffix=\"base_extractor\"))\n",
        "        initial_layers.append(self.normal_normalizer())\n",
        "        initial_layers.append(self.activation_builder())\n",
        "\n",
        "        main_layers = []\n",
        "        main_layers.append(MinibatchStddev())\n",
        "        main_layers.append(self.make_conv(start_res, features, kernel_size=(3, 3), suffix=\"base\"))\n",
        "        main_layers.append(self.normal_normalizer())\n",
        "        main_layers.append(self.activation_builder())\n",
        "\n",
        "        main_layers.append(layers.Flatten())\n",
        "        # layers.GlobalAveragePooling2D())\n",
        "        main_layers.append(layers.Dense(1, name=\"output_logit\", **self.general_layer_configs))\n",
        "\n",
        "        output_tensor = buildModel(initial_layers + main_layers, input_tensor)\n",
        "\n",
        "        self.main_layers = main_layers\n",
        "        self.initial_layers = initial_layers\n",
        "\n",
        "        self.model = keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
        "        self.model.compile(run_eagerly=False, jit_compile=True)\n",
        "        self.current_res = start_res\n",
        "        self.old_model_cloned = self.model\n",
        "\n",
        "    def add_stage(self, res=0):\n",
        "        if res == 0:\n",
        "            res = self.current_res * 2\n",
        "            assert res == self.model.input.shape[1] * 2\n",
        "        log_res = int(np.log2(res))\n",
        "        features = nf(log_res - 1)\n",
        "        lowerFeatures = nf(log_res - 2)\n",
        "\n",
        "        if self.freeze_oldlayers:\n",
        "            for layer in self.initial_layers:\n",
        "                layer.trainable = False\n",
        "\n",
        "            for layer in self.main_layers:\n",
        "                layer.trainable = False\n",
        "\n",
        "        # We need to add stage before all the main layers\n",
        "\n",
        "        input_layer = layers.Input(shape=(res, res, 3))\n",
        "        print(\"Adding stage \", res)\n",
        "        downscaled = layers.AveragePooling2D((2, 2))(input_layer)\n",
        "        old_downscaled_tensor = buildModel(self.initial_layers, downscaled)\n",
        "\n",
        "        new_initial_layers = []\n",
        "        # new_initial_layers.append(MinibatchStddev())\n",
        "        new_initial_layers.append(self.make_conv(res, features, kernel_size=(1, 1), suffix=\"extractor\"))\n",
        "        new_initial_layers.append(self.normal_normalizer())\n",
        "        new_initial_layers.append(self.activation_builder())\n",
        "\n",
        "        new_main_layers = []\n",
        "        # Add self-attention layer conditionally based on resolution\n",
        "        # if res >= 16:\n",
        "        #     new_SelfAttention(name=\"attention_{res}\".format(res=res)))\n",
        "\n",
        "        # new_MinibatchStddev())\n",
        "        new_main_layers.append(self.make_conv(res, features, kernel_size=(3, 3), suffix=\"\"))\n",
        "        new_main_layers.append(self.normal_normalizer())\n",
        "        new_main_layers.append(self.activation_builder())\n",
        "\n",
        "        # Add self-attention layer conditionally based on resolution\n",
        "        # if res >= 16:\n",
        "        #     new_SelfAttention(name=\"attention2_{res}\".format(res=res)))\n",
        "\n",
        "        new_main_layers.append(self.make_conv(res, lowerFeatures, kernel_size=(3, 3), suffix=\"\"))\n",
        "        new_main_layers.append(self.normal_normalizer())\n",
        "        new_main_layers.append(self.activation_builder())\n",
        "\n",
        "        new_main_layers.append(layers.AveragePooling2D((2, 2)))\n",
        "\n",
        "        new_downscaled_tensor = buildModel(new_initial_layers + new_main_layers, input_layer)\n",
        "\n",
        "        alpha = FadeAdd(self.alpha)\n",
        "        new_downscaled_tensor = alpha([new_downscaled_tensor, old_downscaled_tensor])\n",
        "\n",
        "        print(\"New downscaled tensor: \", new_downscaled_tensor)\n",
        "\n",
        "        output_tensor = buildModel(self.main_layers, new_downscaled_tensor)\n",
        "\n",
        "        self.main_layers = new_main_layers + self.main_layers\n",
        "        self.initial_layers = new_initial_layers\n",
        "        # self.old_model_cloned = keras.models.clone_model(self.model)\n",
        "        self.model.save('tmp.keras')\n",
        "        self.old_model_cloned = keras.models.load_model('tmp.keras', custom_objects={\n",
        "            \"FadeAdd\": FadeAdd,\n",
        "            \"MinibatchStddev\": MinibatchStddev,\n",
        "            \"SelfAttention\": SelfAttention,\n",
        "            \"SpectralNormalization\": SpectralNormalization,\n",
        "            \"FakeLayer\": FakeLayer,\n",
        "        })\n",
        "        self.old_model_cloned.trainable = False\n",
        "        self.old_model_cloned.compile(run_eagerly=False, jit_compile=True)\n",
        "        self.old_model_original = self.model\n",
        "        self.model = keras.Model(inputs=input_layer, outputs=output_tensor)\n",
        "        self.model.compile(run_eagerly=False, jit_compile=True)\n",
        "        self.current_res = res\n",
        "\n",
        "    def grow(self, res=0) -> keras.Model:\n",
        "        if self.current_res is None:\n",
        "            self.build_base(self.start_res)\n",
        "        else:\n",
        "            self.add_stage(res)\n",
        "        return self.model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D31JaVm2rxVR"
      },
      "outputs": [],
      "source": [
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "def generate_input_data(batch_size=64):\n",
        "    return tf.random.normal([batch_size, 512])\n",
        "\n",
        "def discriminator_wgan_loss(real_output, fake_output):\n",
        "    # wgan_loss = fake_output - real_output\n",
        "    # return wgan_loss\n",
        "    real_loss = tf.reduce_mean(real_output)\n",
        "    fake_loss = tf.reduce_mean(fake_output)\n",
        "    return fake_loss - real_loss\n",
        "\n",
        "def generator_wgan_loss(fake_output):\n",
        "    total_loss = -tf.reduce_mean(fake_output)\n",
        "    return total_loss\n",
        "\n",
        "# @tf.function\n",
        "def descriminator_hinge_loss(real_output, fake_output):\n",
        "    real_loss = tf.reduce_mean(tf.nn.relu(1.0 - real_output))\n",
        "    fake_loss = tf.reduce_mean(tf.nn.relu(1.0 + fake_output))\n",
        "    des_loss = real_loss + fake_loss\n",
        "    return des_loss\n",
        "\n",
        "# @tf.function\n",
        "def generator_hinge_loss(fake_output):\n",
        "    total_loss = -tf.reduce_mean(fake_output)\n",
        "    return total_loss\n",
        "\n",
        "# @tf.function\n",
        "def descriminator_softplus_loss(real_output, fake_output):\n",
        "    des_loss = tf.nn.softplus(fake_output) + tf.nn.softplus(-real_output)\n",
        "    return tf.reduce_mean(des_loss)\n",
        "\n",
        "# @tf.function\n",
        "def generator_softplus_loss(fake_output):\n",
        "    return tf.reduce_mean(tf.nn.softplus(-fake_output))\n",
        "\n",
        "def generator_enc_loss(real, fake):\n",
        "    # return tf.reduce_mean(tf.abs(real - fake))\n",
        "    return tf.abs(real - fake)\n",
        "\n",
        "# @tf.function\n",
        "def gradientPenalty(des, real_images, fake_images):\n",
        "    with tf.GradientTape() as tape:\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        epsilon = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0, dtype=fake_images.dtype)\n",
        "        interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n",
        "        tape.watch(interpolates)\n",
        "        pred = des(interpolates, training=True)\n",
        "    grads = tape.gradient(pred, [interpolates])[0]\n",
        "    l2norm = tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.size(tf.shape(grads))))\n",
        "    norm = tf.sqrt(l2norm)\n",
        "    loss = tf.square(norm - 1)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "# @tf.function\n",
        "def gradientPenaltyAlt(des, real_images, fake_images):\n",
        "    with tf.GradientTape() as tape:\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        epsilon = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0, dtype=fake_images.dtype)\n",
        "        interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n",
        "        tape.watch(interpolates)\n",
        "        pred = des(interpolates, training=True)\n",
        "    grads = tape.gradient(pred, [interpolates])[0]\n",
        "    l2norm = tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.size(tf.shape(grads))))\n",
        "    # loss = l2norm\n",
        "    loss = tf.reduce_mean(l2norm)\n",
        "    return loss\n",
        "\n",
        "# @tf.function\n",
        "def r1Penalty(des, real_images, fake_images):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(real_images)\n",
        "        pred = des(real_images, training=True)\n",
        "        # pred = tf.reduce_sum(pred)\n",
        "    grads = tape.gradient(pred, [real_images])[0]\n",
        "    l2norm = tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.size(tf.shape(grads))))\n",
        "    loss = tf.reduce_mean(l2norm)\n",
        "    print(\"Type of loss: \", loss.dtype, real_images.dtype, grads.dtype)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvSeGlfOrxVS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def augmenter(size, alpha=None, method='area'):\n",
        "  if alpha is not None:\n",
        "    @tf.function()\n",
        "    def augment(sample):\n",
        "      image = (tf.cast(sample['image'], precision_policy.compute_dtype) - 127.5) / 127.5\n",
        "      big = tf.image.resize(image, [size, size], method=method, antialias=True)\n",
        "      small =  tf.image.resize(image, [size//2, size//2], method=method, antialias=True)\n",
        "      small = tf.image.resize(small, [size, size], method='area')\n",
        "      image = (big*alpha.alpha) + (small*(1-alpha.alpha))\n",
        "      image = tf.image.random_flip_left_right(image)\n",
        "      return {'image':image}\n",
        "    return augment\n",
        "  else:\n",
        "    @tf.function()\n",
        "    def augment(sample):\n",
        "      image = (tf.cast(sample['image'], precision_policy.compute_dtype) - 127.5) / 127.5\n",
        "      image = tf.image.resize(image, [size, size], method=method, antialias=True)\n",
        "      image = tf.image.random_flip_left_right(image)\n",
        "      return {'image':image}\n",
        "    return augment\n",
        "\n",
        "\n",
        "class ProGANMonitor(keras.callbacks.Callback):\n",
        "    metrics_description_table = {\n",
        "        \"des_loss\": \"Descriminator Loss Raw\",\n",
        "        \"des_main_loss\": \"Descriminator core loss\",\n",
        "        \"des_gradient_penalty\":\"Descriminator gradient penalty\",\n",
        "        \"des_drift_loss\": \"Descriminator drift loss\",\n",
        "\n",
        "        \"gen_loss\": \"Generator Loss Raw\",\n",
        "        \"gen_anchor_loss\": \"Generator Anchor Loss\",\n",
        "        \"gen_deviation_loss\": \"Generator Deviation Loss\",\n",
        "        \"gen_main_loss\": \"Generator core loss\",\n",
        "    }\n",
        "    def __init__(self, batch_size, sample_real_data, name=None, tf_log_dir=None, sample_noise=None):\n",
        "        super().__init__()\n",
        "        self.sample_real_data = sample_real_data\n",
        "        if sample_noise != None:\n",
        "            self.sample_noise = sample_noise\n",
        "        else:\n",
        "            self.sample_noise = generate_input_data(batch_size=batch_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.results = []\n",
        "        self.tf_writer = tf.summary.create_file_writer(os.path.join(tf_log_dir, 'train'))\n",
        "\n",
        "\n",
        "    def evaluate(self, batches=10):\n",
        "        desAcc = 0\n",
        "        genLoss = 0\n",
        "        gen = self.model.gen\n",
        "        des = self.model.des\n",
        "        for _ in range(batches):\n",
        "            real = self.sample_real_data\n",
        "            fake = gen.predict(generate_input_data(batch_size=self.batch_size), training=False)\n",
        "\n",
        "            real_output = des(real, training=False)\n",
        "            fake_output = des(fake, training=False)\n",
        "\n",
        "            output = tf.concat((fake_output, real_output), axis=0)\n",
        "\n",
        "            labels = tf.reshape(\n",
        "                tf.concat(\n",
        "                    (tf.zeros_like(fake_output), tf.ones_like(real_output)), axis=0\n",
        "                ),\n",
        "                [-1],\n",
        "            )\n",
        "            output = tf.reshape(output, [-1])\n",
        "\n",
        "            acc = keras.metrics.binary_accuracy(labels, output, threshold=0.5)\n",
        "            desAcc += acc.numpy()\n",
        "            genLoss += (\n",
        "                tf.reduce_sum(generator_wgan_loss(fake_output)).numpy() / self.batch_size\n",
        "            )\n",
        "        return desAcc / batches, genLoss / batches\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        print(\"Original Samples\")\n",
        "        plotImages(self.sample_real_data)\n",
        "        fake = self.model.gen.predict(self.sample_noise, training=False)\n",
        "        print(\"Generated Samples\")\n",
        "        plotImages(fake)\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch_start_time = time.time()\n",
        "        print(\"Starting epoch \", epoch)\n",
        "\n",
        "        if epoch >= self.model.alpha_delay_epochs:\n",
        "            self.model.can_update_alpha.assign(True)\n",
        "            # we increment the alpha by the multiplier\n",
        "            self.model.alpha_incr_rate.assign(\n",
        "                self.model.alpha_incr_rate * self.model.alpha_multiplier\n",
        "            )\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 5 == 0:\n",
        "            clear_output()\n",
        "\n",
        "        print(\"\\nEpoch complete. Epoch Time: \", time.time() - self.epoch_start_time)\n",
        "        desAcc, genLoss = self.evaluate()\n",
        "        self.results.append({\"desAcc\": desAcc, \"genLoss\": genLoss})\n",
        "        fake = self.model.gen.predict(self.sample_noise, training=False)\n",
        "        print(\"Epoch \", epoch, \"Descriminator Accuracy \", desAcc, \"Generator Loss \", genLoss)\n",
        "        with self.tf_writer.as_default():\n",
        "            tf.summary.scalar(\"Generator Loss at Epoch\", genLoss, step=epoch)\n",
        "            # Share images of fake data generated\n",
        "            tf.summary.image(\n",
        "                f\"Fake Images {self.model.current_res}x{self.model.current_res}\", tf.cast(denormalizeImage(fake), tf.uint8), step=epoch, max_outputs=10\n",
        "            )\n",
        "            # Share the layers of the model as histograms\n",
        "            for layer in self.model.des.layers:\n",
        "                # If the layer has a 'kernel' or a 'embeddings' attribute\n",
        "                weights = layer.get_weights()\n",
        "                for i, weight in enumerate(weights):\n",
        "                    # print(\"Des Layer: \", layer.name, \"Weight: \", i, \"Shape: \", weight.shape)\n",
        "                    tf.summary.histogram(f\"Weights/Descriminator/{layer.name}/{i}\", weight, step=epoch)\n",
        "                    # tf.summary.image(f\"Weights/Descriminator/{layer.name}{i}/image\", tf.expand_dims(weight, axis=1), step=epoch)\n",
        "            for layer in self.model.gen.get_layers():\n",
        "                weights = layer.get_weights()\n",
        "                for i, weight in enumerate(weights):\n",
        "                    # print(\"Gen Layer: \", layer.name, \"Weight: \", i, \"Shape: \", weight.shape)\n",
        "                    tf.summary.histogram(f\"Weights/Generator/{layer.name}/{i}\", weight, step=epoch)\n",
        "                    # tf.summary.image(f\"Weights/Generator/{layer.name}/{i}/image\", tf.expand_dims(weight, axis=1), step=epoch)\n",
        "        print(\"Real: \")\n",
        "        plotImages(self.sample_real_data)\n",
        "        print(\"Fake: \")\n",
        "        plotImages(fake)\n",
        "        print(\"Final Alpha: \", self.model.alpha.value())\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        if tf.math.mod(self.model.train_step_counter.value(), 20) == 0:\n",
        "            with self.tf_writer.as_default():\n",
        "                tf.summary.scalar(\"Batch\", batch, step=self.model.train_step_counter)\n",
        "                tf.summary.scalar(\n",
        "                    \"Alpha\",\n",
        "                    self.model.alpha.value(),\n",
        "                    step=int(self.model.train_step_counter / self.model.steps_per_epoch),\n",
        "                )\n",
        "                tf.summary.scalar(\"Generator Loss\",self.model.g_loss_metric.result(),step=self.model.train_step_counter)\n",
        "                tf.summary.scalar(\"Descriminator Loss\",self.model.d_loss_metric.result(),step=self.model.train_step_counter)\n",
        "                for key, value in logs.items():\n",
        "                    better_key = self.metrics_description_table.get(key, key)\n",
        "                    tf.summary.scalar(better_key, value, step=self.model.train_step_counter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_yr2SpQrxVS"
      },
      "outputs": [],
      "source": [
        "class ProGAN(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        start_res=4,\n",
        "        name=None,\n",
        "        gen_loss=generator_wgan_loss,\n",
        "        des_loss=discriminator_wgan_loss,\n",
        "        gradient_penalty_calculator=gradientPenalty,\n",
        "        des_steps=1,\n",
        "        gen_steps=1,\n",
        "        gen_config:dict={},\n",
        "        des_config:dict={},\n",
        "        loss_weights:dict={\"gradient_penalty\": 10, \"drift\": 0.001},\n",
        "        initial_alpha=0.0,\n",
        "        should_train_separately=False,\n",
        "    ):\n",
        "        super().__init__(name=name)\n",
        "        # gen, des, enc, baseLayers = generateBaseModels()\n",
        "        self.alpha = tf.Variable(initial_alpha, trainable=False, dtype=self.compute_dtype)\n",
        "        self.initial_alpha = initial_alpha\n",
        "        self.desBuilder = DescriminatorBuilder(alpha=self.alpha, start_res=start_res, **des_config)\n",
        "        self.genBuilder = GeneratorBuilder(alpha=self.alpha, start_res=start_res, **gen_config)\n",
        "        self.genBuilder.grow()\n",
        "        gen = self.genBuilder\n",
        "        des = self.desBuilder.grow()\n",
        "        self.gen = gen\n",
        "        self.des = des\n",
        "        self.gen_loss = gen_loss\n",
        "        self.des_loss = des_loss\n",
        "        self.des_steps = des_steps\n",
        "        self.gen_steps = gen_steps\n",
        "        self.current_res = start_res\n",
        "        self.train_step_counter = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
        "        self.loss_weights = loss_weights\n",
        "        self.g_optimizer = None\n",
        "        self.d_optimizer = None\n",
        "        self.can_update_alpha = tf.Variable(False, dtype=tf.bool, trainable=False)\n",
        "        self.gradient_penalty_calculator = gradient_penalty_calculator\n",
        "        self.should_train_separately = should_train_separately\n",
        "        self.unfreeze_lower_layer_epoch = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
        "        self.dev_loss_coeff = tf.constant(loss_weights.get(\"dev_loss\", 0.0), dtype=self.compute_dtype)\n",
        "        self.dev_loss_decay = tf.constant(loss_weights.get(\"dev_loss_decay\", 0.0), dtype=self.compute_dtype)\n",
        "        self.anchor_loss_coeff = tf.constant(loss_weights.get(\"anchor_loss\", 0.0), dtype=self.compute_dtype)\n",
        "        self.gradient_loss_coeff = tf.constant(loss_weights.get(\"gradient_penalty\", 0.0), dtype=self.compute_dtype)\n",
        "        self.drift_loss_coeff = tf.constant(loss_weights.get(\"drift\", 0.0), dtype=self.compute_dtype)\n",
        "\n",
        "    \"\"\"\n",
        "    Grows the model by doubling the resolution\n",
        "    \"\"\"\n",
        "\n",
        "    def grow_model(self,\n",
        "            gen_grow_config: dict={},\n",
        "            des_grow_config: dict={},\n",
        "        ):\n",
        "        self.current_res *= 2\n",
        "        # Reset the alpha\n",
        "        self.alpha.assign(self.initial_alpha)\n",
        "        # Add a new stage to the Des and Gen\n",
        "        des = self.desBuilder.grow(**des_grow_config)\n",
        "        self.genBuilder.grow(**gen_grow_config)\n",
        "        self.des = des\n",
        "        pass\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        res: int,\n",
        "        steps_per_epoch: int,\n",
        "        g_optimizer: keras.Optimizer,\n",
        "        d_optimizer: keras.Optimizer,\n",
        "        alpha_incr_rate: float = 0.1,\n",
        "        alpha_delay_epochs: int = 0,\n",
        "        alpha_multiplier: float = 1.1,\n",
        "        gen_grow_config: dict={},\n",
        "        des_grow_config: dict={},\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # self.loss_weights = kwargs.pop(\"loss_weights\", self.loss_weights)\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.alpha_incr_rate = tf.Variable(alpha_incr_rate, dtype=self.compute_dtype, trainable=False)\n",
        "        self.alpha_delay_epochs = alpha_delay_epochs\n",
        "        self.alpha_multiplier = alpha_multiplier\n",
        "\n",
        "        self.can_update_alpha.assign(False)\n",
        "\n",
        "        if g_optimizer is None:\n",
        "            g_optimizer = keras.optimizers.Adam(\n",
        "                learning_rate=1e-4, beta_1=0, beta_2=0.999\n",
        "            )\n",
        "        if d_optimizer is None:\n",
        "            d_optimizer = keras.optimizers.Adam(\n",
        "                learning_rate=1e-4, beta_1=0, beta_2=0.999\n",
        "            )\n",
        "\n",
        "        # self.train_step_counter.assign(0)\n",
        "        # self.train_step_counter = 0\n",
        "\n",
        "        if res != self.current_res:\n",
        "            self.g_optimizer = g_optimizer\n",
        "            self.d_optimizer = d_optimizer\n",
        "            self.grow_model(gen_grow_config, des_grow_config)\n",
        "        elif self.g_optimizer is None:\n",
        "            self.g_optimizer = g_optimizer\n",
        "            self.d_optimizer = d_optimizer\n",
        "\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"des_loss_metric\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"gen_loss_metric\")\n",
        "        self.tf_train_step = tf.function(self._train_step, jit_compile=True)\n",
        "        self.train_discriminator = tf.function(self._train_discriminator, jit_compile=True)\n",
        "        self.train_generator = tf.function(self._train_generator, jit_compile=True)\n",
        "\n",
        "        if 'unfreeze_lower_layer_epoch' in gen_grow_config:\n",
        "            self.unfreeze_lower_layer_epoch.assign(gen_grow_config['unfreeze_lower_layer_epoch'])\n",
        "\n",
        "        super().compile(*args, **kwargs)\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def _train_generator(self, batch_size):\n",
        "        with tf.GradientTape() as tape:\n",
        "            noise = generate_input_data(batch_size=batch_size)\n",
        "            fake_images, fake_images_lower_res, high_to_low_res, dev_loss = self.genBuilder.composite_predict(noise, training=True)\n",
        "            fake_output = self.des(fake_images, training=False)\n",
        "            main_loss = self.gen_loss(fake_output)\n",
        "            dev_loss = dev_loss * self.dev_loss_coeff\n",
        "            # Decay devloss with steps\n",
        "            # if self.dev_loss_decay > 0.0:\n",
        "            #     dev_decay = (1.0 - tf.minimum(1.0, tf.cast(self.train_step_counter, dtype=self.compute_dtype) / self.steps_per_epoch) * self.dev_loss_decay) * self.alpha\n",
        "            #     dev_loss = dev_loss * dev_decay\n",
        "            loss = main_loss + dev_loss\n",
        "            anchor_loss = 0.0\n",
        "            # if high_to_low_res != None:# and self.anchor_loss_coeff > 0.0:\n",
        "            #     # If we have lower resolution images, we also need to minimize the loss for them\n",
        "            #     # so the generator can continue to fool the old already trained lower res descriminator\n",
        "            #     fake_output_lower_res = self.desBuilder.old_model_cloned(high_to_low_res)\n",
        "            #     anchor_loss = self.gen_loss(fake_output_lower_res)\n",
        "            #     loss += anchor_loss * self.anchor_loss_coeff\n",
        "            #     # loss = self.g_optimizer.scale_loss(loss)\n",
        "                \n",
        "        trainable_variables = self.genBuilder.get_trainable_variables()\n",
        "        \n",
        "        gradients = tape.gradient(loss, trainable_variables)\n",
        "        self.g_optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "        return loss, main_loss, dev_loss, anchor_loss\n",
        "\n",
        "    def _train_discriminator(self, real_images, batch_size):\n",
        "        with tf.GradientTape() as total_tape, tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(real_images)\n",
        "            noise = generate_input_data(batch_size=batch_size)\n",
        "            fake_images = self.gen.predict(noise, training=False)\n",
        "            real_output = self.des(real_images, training=True)\n",
        "            fake_output = self.des(fake_images, training=True)\n",
        "            # combined_images = tf.concat([real_images, fake_images], axis=0)\n",
        "            # combined_output = self.des(combined_images, training=True)\n",
        "            # real_output, fake_output = tf.split(combined_output, num_or_size_splits=2, axis=0)\n",
        "\n",
        "            main_loss = self.des_loss(real_output=real_output, fake_output=fake_output)\n",
        "            # The above two lines can be replaced with the following line\n",
        "\n",
        "            grads = gp_tape.gradient(real_output, [real_images])[0]\n",
        "            l2norm = tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.size(tf.shape(grads))))\n",
        "            gradient_penalty = tf.reduce_mean(l2norm)\n",
        "            \n",
        "            # gradient_penalty = self.gradient_penalty_calculator(des=self.des, real_images=real_images, fake_images=fake_images)\n",
        "            gradient_penalty = self.gradient_loss_coeff * gradient_penalty\n",
        "            # gradient_penalty = 0.0\n",
        "\n",
        "            all_pred = tf.concat([fake_output, real_output], axis=0)\n",
        "            drift_loss = tf.reduce_mean(all_pred**2)\n",
        "            # drift_loss = real_output\n",
        "            drift_loss = self.drift_loss_coeff * drift_loss\n",
        "\n",
        "            loss = main_loss + gradient_penalty + drift_loss\n",
        "            # loss = self.d_optimizer.scale_loss(loss)\n",
        "        \n",
        "        trainable_variables = self.des.trainable_variables\n",
        "        gradients = total_tape.gradient(loss, trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "        return loss, main_loss, gradient_penalty, drift_loss\n",
        "\n",
        "    def alpha_update(self):\n",
        "        alphaIncr = self.alpha_incr_rate / self.steps_per_epoch\n",
        "        self.alpha.assign(tf.minimum(self.alpha + alphaIncr, 1.0))\n",
        "\n",
        "    def update_train_params_on_step(self):\n",
        "        self.train_step_counter.assign_add(1)\n",
        "        # self.train_step_counter += 1\n",
        "        # if self.can_update_alpha.read_value():\n",
        "        tf.cond(self.can_update_alpha, lambda: self.alpha_update(), lambda: None)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        return self.tf_train_step(data)\n",
        "        # return self._train_step(data)\n",
        "\n",
        "    def _train_step(self, data):\n",
        "        self.update_train_params_on_step()\n",
        "\n",
        "        real_images = data[\"image\"]\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        for _ in range(self.des_steps):\n",
        "            des_loss, des_core_loss, gradient_penalty, drift_loss = self.train_discriminator(real_images, batch_size)\n",
        "        for _ in range(self.gen_steps):\n",
        "            gen_loss, gen_core_loss, dev_loss, anchor_loss = self.train_generator(batch_size)\n",
        "\n",
        "        if tf.math.mod(self.train_step_counter, 20) == 0:\n",
        "            self.g_loss_metric.update_state(tf.abs(gen_loss))\n",
        "            self.d_loss_metric.update_state(tf.abs(des_loss))\n",
        "\n",
        "        if self.train_step_counter == self.unfreeze_lower_layer_epoch * self.steps_per_epoch:\n",
        "            self.genBuilder.set_last_output_trainable(True)\n",
        "\n",
        "        return {\n",
        "            \"des_loss\": des_loss,\n",
        "            \"des_main_loss\": des_core_loss,\n",
        "            \"des_gradient_penalty\": gradient_penalty,\n",
        "            \"des_drift_loss\": drift_loss,\n",
        "\n",
        "            'gen_deviation_loss': dev_loss,\n",
        "            'gen_anchor_loss': anchor_loss,\n",
        "            \"gen_loss\": gen_loss,\n",
        "            \"gen_main_loss\": gen_core_loss,\n",
        "        }\n",
        "\n",
        "    def get_weights(self, *args, **kwargs):\n",
        "        weights = {\n",
        "            \"des\": self.des.get_weights(*args, **kwargs),\n",
        "        }\n",
        "        if self.gen.higher_model != None:\n",
        "            weights[\"gen_lower\"] = self.gen.lower_model.get_weights(*args, **kwargs)\n",
        "            weights[\"gen_higher\"] = self.gen.higher_model.get_weights(*args, **kwargs)\n",
        "        else:\n",
        "            weights[\"gen\"] = self.gen.current_model.get_weights(*args, **kwargs)\n",
        "\n",
        "    def set_weights(self, weights, *args, **kwargs):\n",
        "        if self.gen.higher_model != None:\n",
        "            self.gen.lower_model.set_weights(weights[\"gen_lower\"], *args, **kwargs)\n",
        "            self.gen.higher_model.set_weights(weights[\"gen_higher\"], *args, **kwargs)\n",
        "        else:\n",
        "            self.gen.current_model.set_weights(weights[\"gen\"], *args, **kwargs)\n",
        "        self.des.set_weights(weights[\"des\"], *args, **kwargs)\n",
        "\n",
        "    def save_weights(self, path, *args, **kwargs):\n",
        "        # self.gen.save_weights('{path}_gen.weights.h5'.format(path=path), *args, **kwargs)\n",
        "        self.des.save_weights('{path}_des.weights.h5'.format(path=path), *args, **kwargs)\n",
        "\n",
        "    def load_weights(self, path, *args, **kwargs):\n",
        "        #self.gen.load_weights('{path}_gen.weights.h5'.format(path=path), *args, **kwargs)\n",
        "        self.des.load_weights('{path}_des.weights.h5'.format(path=path), *args, **kwargs)\n",
        "\n",
        "    def save(self, path, *args, **kwargs):\n",
        "        #self.gen.save('{path}_gen.keras'.format(path=path), *args, **kwargs)\n",
        "        self.des.save('{path}_des.keras'.format(path=path), *args, **kwargs)\n",
        "\n",
        "\n",
        "def trainProGan(config, name=MODEL_NAME, reportToWandb=False) -> ProGAN:\n",
        "    experiment_name = \"{name}_{date}\".format(\n",
        "                name=name, date=datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "            )\n",
        "    log_dir = os.path.join(TRAIN_LOGDIR, experiment_name)\n",
        "    if config['clear_previous_runs'] is True:\n",
        "        os.system(\"rm -rf ./{log_dir}\".format(log_dir=TRAIN_LOGDIR))\n",
        "\n",
        "    # Store the config object in the models directory as pickle\n",
        "    with open(os.path.join(MODEL_PATH, \"{name}.config\".format(name=experiment_name)), \"wb\") as f:\n",
        "        pickle.dump(config, f)\n",
        "\n",
        "    # Start a run, tracking hyperparameters\n",
        "    if reportToWandb:\n",
        "        wandb.init(\n",
        "            # set the wandb project where this run will be logged\n",
        "            project=\"progan-experiments\",\n",
        "            name=experiment_name,\n",
        "            # track hyperparameters and run metadata with wandb.config\n",
        "            config=config,\n",
        "        )\n",
        "        # [optional] use wandb.config as your config\n",
        "        # config = wandb.config\n",
        "\n",
        "    scaleSizes = sorted(config[\"trainingProfile\"].keys())\n",
        "    print(\"Training at scales: \", scaleSizes)\n",
        "\n",
        "    if \"start_res\" not in config:\n",
        "        config['start_res'] = scaleSizes[0]\n",
        "\n",
        "    gan:ProGAN = ProGAN(\n",
        "        name=experiment_name,\n",
        "        start_res=config['start_res'],\n",
        "        des_steps=config[\"des_steps\"],\n",
        "        gen_steps=config[\"gen_steps\"],\n",
        "        des_loss=config[\"des_loss\"],\n",
        "        gen_loss=config[\"gen_loss\"],\n",
        "        gradient_penalty_calculator=config['gradient_penalty_calculator'],\n",
        "        gen_config=config[\"gen_config\"],\n",
        "        des_config=config[\"des_config\"],\n",
        "        loss_weights=config[\"loss_weights\"],\n",
        "        initial_alpha=config[\"initial_alpha\"],\n",
        "        should_train_separately=config[\"train_separately\"],\n",
        "    )\n",
        "    tensorboard_callback = keras.callbacks.TensorBoard(\n",
        "        log_dir=log_dir,\n",
        "        update_freq=\"epoch\",\n",
        "        write_images=True,\n",
        "        write_graph=True,\n",
        "        histogram_freq=1,\n",
        "    )\n",
        "\n",
        "    # Profile from batches 10 to 15\n",
        "    # tb_callback = keras.callbacks.TensorBoard(log_dir=TRAIN_LOGDIR,\n",
        "    #                                             profile_batch='10, 15')\n",
        "    datasetName = config[\"data\"]\n",
        "    data:tf.data.Dataset = tfds.load(datasetName, split=\"train\", shuffle_files=True)\n",
        "\n",
        "    sample_noise = generate_input_data(batch_size=64)\n",
        "\n",
        "    gen_grow_config: dict={}\n",
        "    des_grow_config: dict={}\n",
        "\n",
        "    if 'gen_grow_config' in config:\n",
        "        gen_grow_config = config['gen_grow_config']\n",
        "\n",
        "    if 'des_grow_config' in config:\n",
        "        des_grow_config = config['des_grow_config']\n",
        "\n",
        "    for size in scaleSizes:\n",
        "        clear_output(wait=True)\n",
        "        conf = config[\"trainingProfile\"][size]\n",
        "        batch_size = conf[\"batch_size\"]\n",
        "        steps = len(data) // batch_size\n",
        "        print(\"Training at resolution \", size, \" with \", steps, \" steps per epoch\")\n",
        "        finalData = (\n",
        "            data\n",
        "            # .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "            # .batch(128, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .map(\n",
        "                augmenter(size, alpha=None, method=\"area\"),\n",
        "                num_parallel_calls=tf.data.AUTOTUNE\n",
        "            )\n",
        "            .cache()  # Cache after augmenting to avoid recomputation\n",
        "            # .unbatch()\n",
        "            .shuffle(4096)  # Ensure this is adequate for your dataset size\n",
        "            .batch(batch_size, drop_remainder=True)\n",
        "            .repeat()  # Repeats the dataset indefinitely\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        )\n",
        "        dataIter = iter(finalData)\n",
        "        sample_real_data = next(dataIter)[\"image\"]\n",
        "\n",
        "        print(\"Size of real data: \", sample_real_data.shape)\n",
        "\n",
        "        alphaStep = conf[\"alpha_step\"]\n",
        "        alphaDelay = conf[\"alpha_delay\"]\n",
        "        alphaMultiplier = conf[\"alpha_multiplier\"]\n",
        "\n",
        "        if 'gen_grow_config' in conf:\n",
        "            gen_grow_config = conf['gen_grow_config']\n",
        "\n",
        "        if 'des_grow_config' in conf:\n",
        "            des_grow_config = conf['des_grow_config']\n",
        "\n",
        "        gan.compile(\n",
        "            res=size,\n",
        "            steps_per_epoch=steps,\n",
        "            g_optimizer=config[\"g_optimizer_builder\"](),\n",
        "            d_optimizer=config[\"d_optimizer_builder\"](),\n",
        "            alpha_incr_rate=alphaStep,\n",
        "            alpha_delay_epochs=alphaDelay,\n",
        "            alpha_multiplier=alphaMultiplier,\n",
        "            gen_grow_config=gen_grow_config,\n",
        "            des_grow_config=des_grow_config,\n",
        "        )\n",
        "        prefix = \"res_{s}x{s}_{name}\".format(s=size, name=name)\n",
        "\n",
        "        monitor = ProGANMonitor(\n",
        "            64, sample_real_data, name=prefix, tf_log_dir=log_dir, sample_noise=sample_noise\n",
        "        )\n",
        "        ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(MODEL_PATH, \"{prefix}.keras\".format(prefix=prefix)),\n",
        "            # save_weights_only=True,\n",
        "            monitor=\"gen_loss_metric\",\n",
        "            mode='min',\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        early_stopping = keras.callbacks.EarlyStopping(\n",
        "            monitor=\"gen_loss_metric\",\n",
        "            # min_delta=0,\n",
        "            patience=5,\n",
        "            verbose=1,\n",
        "            mode=\"min\",\n",
        "            baseline=None,\n",
        "            # restore_best_weights=True,\n",
        "            start_from_epoch=7,\n",
        "        )\n",
        "\n",
        "        callbacks = [monitor, ckpt_cb, tensorboard_callback, early_stopping]\n",
        "\n",
        "        if reportToWandb == True:\n",
        "            wandb_callback = WandbMetricsLogger(initial_global_step=gan.train_step_counter.numpy(), log_freq=100)\n",
        "            callbacks.append(wandb_callback)\n",
        "\n",
        "        if conf['epochs'] > 0:\n",
        "            gan.fit(\n",
        "                finalData,\n",
        "                steps_per_epoch=steps,\n",
        "                epochs=conf[\"epochs\"],\n",
        "                callbacks=callbacks,\n",
        "            )\n",
        "\n",
        "        if config['profile']:\n",
        "            with tf.profiler.experimental.Profile(log_dir):\n",
        "                for step in range(10):  # Small number of steps for initial testing\n",
        "                    with tf.profiler.experimental.Trace('train', step_num=step):\n",
        "                        batch = next(dataIter)\n",
        "                        gan.train_step(batch)\n",
        "            print(\"Profiling completed at resolution \", size)\n",
        "\n",
        "    if reportToWandb:\n",
        "        wandb.finish()\n",
        "    return gan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8wgOTS-rxVS"
      },
      "outputs": [],
      "source": [
        "def activation_builder():\n",
        "  return layers.LeakyReLU(alpha=0.2)\n",
        "\n",
        "def kernel_initializer_builder():\n",
        "  return keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "def upsampling_builder():\n",
        "  return layers.UpSampling2D((2, 2), interpolation='nearest')\n",
        "\n",
        "def gen_conv_normalizer(x, name=None):\n",
        "  x.name = name\n",
        "  return x\n",
        "\n",
        "def des_normal_normalizer():\n",
        "  return layers.GroupNormalization(groups=-1)\n",
        "\n",
        "def des_conv_normalizer(*args, **kwargs):\n",
        "  return SpectralNormalization(power_iterations=2, *args, **kwargs)\n",
        "\n",
        "trainingConfs = {\n",
        "  'lfw':{\n",
        "    4:{\"epochs\":15, \"batch_size\":64, \"alpha_step\":0, \"alpha_delay\":0, \"alpha_multiplier\":0},\n",
        "    8:{\"epochs\":20, \"batch_size\":64, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.},\n",
        "    16:{\"epochs\":20, \"batch_size\":64, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.},\n",
        "    32:{\"epochs\":30, \"batch_size\":64, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.},\n",
        "    64:{\"epochs\":50, \"batch_size\":48, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.},\n",
        "    128:{\"epochs\":40, \"batch_size\":32, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.},\n",
        "    256:{\"epochs\":50, \"batch_size\":32, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.}\n",
        "  },\n",
        "  'celeb_a':{\n",
        "    4:{\"epochs\":10, \"batch_size\":64, \"alpha_step\":0.3, \"alpha_delay\":0, \"alpha_multiplier\":0},\n",
        "    8:{\"epochs\":50, \"batch_size\":64, \"alpha_step\":0.5, \"alpha_delay\":3, \"alpha_multiplier\":1.},\n",
        "    16:{\"epochs\":50, \"batch_size\":64, \"alpha_step\":0.5, \"alpha_delay\":3, \"alpha_multiplier\":1.},\n",
        "    32:{\"epochs\":50, \"batch_size\":64, \"alpha_step\":0.5, \"alpha_delay\":3, \"alpha_multiplier\":1.},\n",
        "    64:{\"epochs\":50, \"batch_size\":32, \"alpha_step\":0.5, \"alpha_delay\":3, \"alpha_multiplier\":1.},\n",
        "    128:{\"epochs\":50, \"batch_size\":32, \"alpha_step\":0.5, \"alpha_delay\":3, \"alpha_multiplier\":1.},\n",
        "    256:{\"epochs\":50, \"batch_size\":32, \"alpha_step\":0.5, \"alpha_delay\":3, \"alpha_multiplier\":1.}\n",
        "  }\n",
        "}\n",
        "\n",
        "def optimizer_builder_des():\n",
        "  return keras.optimizers.Adam(learning_rate=1e-4, beta_1 = 0., beta_2 = 0.999)\n",
        "  # return tf.keras.mixed_precision.LossScaleOptimizer(keras.optimizers.Adam(learning_rate=1e-4, beta_1 = 0., beta_2 = 0.999))\n",
        "\n",
        "def optimizer_builder_gen():\n",
        "  return keras.optimizers.Adam(learning_rate=5e-5, beta_1 = 0., beta_2 = 0.999)\n",
        "  # return tf.keras.mixed_precision.LossScaleOptimizer(keras.optimizers.Adam(learning_rate=5e-5, beta_1 = 0., beta_2 = 0.999))\n",
        "\n",
        "gen_config = {\n",
        "  'latent_dim':512,\n",
        "  'conv_normalizer':gen_conv_normalizer,\n",
        "  'normal_normalizer':PixelNorm,\n",
        "  'activation_builder':activation_builder,\n",
        "  'upsampling_builder':upsampling_builder,\n",
        "  'kernel_constraint':None,\n",
        "  'kernel_initializer_builder':kernel_initializer_builder,\n",
        "  'freeze_oldlayers':False,\n",
        "  'use_sepconv':True,\n",
        "\n",
        "  'gen_grow_config':{\n",
        "    'unfreeze_lower_layer_epoch':5,\n",
        "    'freeze_previous_output':False,\n",
        "  }\n",
        "}\n",
        "des_config = {\n",
        "  'conv_normalizer':des_conv_normalizer,\n",
        "  'normal_normalizer':des_normal_normalizer,\n",
        "  'activation_builder':activation_builder,\n",
        "  'kernel_constraint':None,\n",
        "  'kernel_initializer_builder':kernel_initializer_builder,\n",
        "  'freeze_oldlayers':False,\n",
        "  'use_sepconv':True,\n",
        "}\n",
        "\n",
        "config = {\n",
        "  \"clear_previous_runs\":False,\n",
        "  'profile':False,\n",
        "  'initial_alpha':0.0,\n",
        "  'des_steps':1,\n",
        "  'gen_steps':1,\n",
        "  'train_separately':True,\n",
        "  'des_loss':descriminator_hinge_loss,\n",
        "  'gen_loss':generator_hinge_loss,\n",
        "  # 'des_loss':descriminator_softplus_loss,\n",
        "  # 'gen_loss':generator_softplus_loss,\n",
        "  # 'des_loss':None,\n",
        "  # 'gen_loss':None,\n",
        "  'gradient_penalty_calculator':r1Penalty,\n",
        "  'g_optimizer_builder':optimizer_builder_gen,\n",
        "  'd_optimizer_builder':optimizer_builder_des,\n",
        "  'data':'celeb_a',\n",
        "  'gen_config':gen_config,\n",
        "  'des_config':des_config,\n",
        "  \"loss_weights\": {\"gradient_penalty\": 10, \"drift\": 0.00, \"dev_loss\": 0.0, \"anchor_loss\": 0.0},\n",
        "}\n",
        "config['trainingProfile'] = trainingConfs[config['data']]\n",
        "trainProGan(name=MODEL_NAME, config=config, reportToWandb=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-DBpo4prxVT",
        "outputId": "8c67c54b-bd3f-4b73-80b9-55a0262abcf5"
      },
      "outputs": [],
      "source": [
        "dBuilder = DescriminatorBuilder(**des_config)\n",
        "gBuilder = GeneratorBuilder(**gen_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsigVjIkrxVT",
        "outputId": "04ef45e5-3a25-4c2e-c373-b8e01e65efc4"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    gBuilder.grow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtOoVogdrxVT"
      },
      "outputs": [],
      "source": [
        "gBuilder.current_model.summary()\n",
        "# keras.utils.plot_model(\n",
        "#     d,\n",
        "#     to_file=\"descriminator.png\",\n",
        "#     expand_nested=True,\n",
        "#     show_shapes=True,\n",
        "#     show_layer_names=True,\n",
        "#     dpi=96,\n",
        "#     show_trainable=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I1Fd03IrxVT",
        "outputId": "31789d4a-da8f-454e-8208-4285377fce05"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    dBuilder.grow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVsSLeylrxVT",
        "outputId": "442a6790-225c-4695-d8e8-aa6a43bb81e7"
      },
      "outputs": [],
      "source": [
        "fake_images = gBuilder.predict(tf.random.normal([1, 512], dtype=tf.float32))\n",
        "real_images = tf.random.normal(fake_images.shape, dtype=tf.float32)\n",
        "\n",
        "print(fake_images.dtype)\n",
        "\n",
        "fake_output = dBuilder.model(fake_images)\n",
        "real_output = dBuilder.model(real_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpY5sSLIrxVU",
        "outputId": "4f008624-80a1-4aae-bdc0-6a4659fb51da"
      },
      "outputs": [],
      "source": [
        "tf.reduce_mean(fake_output), tf.reduce_mean(real_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxyGz_rorxVU",
        "outputId": "fe85efe7-3571-4e4d-d026-eae37dbcff51"
      },
      "outputs": [],
      "source": [
        "fake_output, real_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS8f7uxQDqJl",
        "outputId": "0bd5e17b-4d71-44a9-a5d9-27a9860ffbeb"
      },
      "outputs": [],
      "source": [
        "timeit.timeit(lambda: dBuilder.model( tf.random.normal(fake_images.shape)), number=100)/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-s4JYtgrxVU",
        "outputId": "6ab40fdf-6d8b-4a3d-a694-57da8a5314e8"
      },
      "outputs": [],
      "source": [
        "timeit.timeit(lambda: dBuilder.model( tf.random.normal(fake_images.shape)), number=100)/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3szmSqyArxVU",
        "outputId": "c4f70e0a-3bdb-4e17-c957-a72ebee8b694"
      },
      "outputs": [],
      "source": [
        "timeit.timeit(lambda: gBuilder.predict(tf.random.normal([1, 512])), number=100)/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c8Dj8NjrxVU"
      },
      "outputs": [],
      "source": [
        "dBuilder.model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk3pP431rxVU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
