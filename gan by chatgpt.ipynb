{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw = tfds.load('lfw', split='train', shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plotImages(imgs):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for i in range(imgs.shape[0]):\n",
    "      plt.subplot(8, 8, i+1)\n",
    "      plt.imshow(tf.cast(imgs[i, :, :, :] * 127.5 + 127.5, tf.uint8))\n",
    "      plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "KERNEL_INIT = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "def lerp(a, b, t):\n",
    "    return a + (b - a) * t\n",
    "\n",
    "def gradient_penalty(discriminator, real_images, fake_images):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    interpolated_images = lerp(real_images, fake_images, alpha)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated_images)\n",
    "        pred = discriminator(interpolated_images, training=True)\n",
    "    gradients = tape.gradient(pred, [interpolated_images])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "class PixelNormalization(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return inputs * tf.math.rsqrt(tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True) + 1e-8)\n",
    "\n",
    "class MinibatchStddev(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        group_size = tf.minimum(4, tf.shape(inputs)[0])\n",
    "        shape = tf.shape(inputs)\n",
    "        minibatch = tf.reshape(inputs, (group_size, -1, shape[1], shape[2], shape[3]))\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(minibatch - tf.reduce_mean(minibatch, axis=0)), axis=0) + 1e-8)\n",
    "        stddev = tf.reduce_mean(stddev, axis=[1, 2, 3], keepdims=True)\n",
    "        stddev = tf.tile(stddev, [group_size, shape[1], shape[2], 1])\n",
    "        return tf.concat([inputs, stddev], axis=-1)\n",
    "\n",
    "def build_generator():\n",
    "    noise = layers.Input(shape=(512,))\n",
    "    x = layers.Dense(4*4*512, use_bias=False, kernel_initializer=KERNEL_INIT)(noise)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "    x = PixelNormalization()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', kernel_initializer=KERNEL_INIT)(x)\n",
    "    x = PixelNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=(1, 1), padding='same', kernel_initializer=KERNEL_INIT)(x)\n",
    "    x = PixelNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(3, (1, 1), padding='same', activation='tanh', kernel_initializer=KERNEL_INIT)(x)\n",
    "    model = models.Model(noise, x)\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    image = layers.Input(shape=(4, 4, 3))\n",
    "    x = MinibatchStddev()(image)\n",
    "    x = layers.Conv2D(128, (1, 1), padding='same', kernel_initializer=KERNEL_INIT)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', kernel_initializer=KERNEL_INIT)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(128, (4, 4), padding='same', kernel_initializer=KERNEL_INIT)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1, kernel_initializer=KERNEL_INIT)(x)\n",
    "    model = models.Model(image, x)\n",
    "    return model\n",
    "\n",
    "def train_step(generator, discriminator, batch_size, generator_optimizer, discriminator_optimizer, data):\n",
    "    noise = tf.random.normal([batch_size, 512])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        real_images = next(data)['image']\n",
    "        real_images = (tf.cast(real_images, tf.float32) - 127.5) / 127.5\n",
    "        fake_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(real_images, training=True)\n",
    "        fake_output = discriminator(fake_images, training=True)\n",
    "\n",
    "        gp = gradient_penalty(discriminator, real_images, fake_images)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output) + gp * 10.0\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "def augmenter(size):\n",
    "  def augment(sample):\n",
    "    sample['image'] = tf.image.resize(sample['image'], [size, size], method='nearest', antialias=True)\n",
    "    return sample\n",
    "  return augment\n",
    "\n",
    "def train(generator, discriminator, epochs, batch_size=16):\n",
    "    generator_optimizer = optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
    "    discriminator_optimizer = optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
    "    currentData = lfw.map(augmenter(4)).shuffle(4096).batch(batch_size, drop_remainder=True).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    iterData = iter(currentData)\n",
    "    evalSample = next(iterData)['image']\n",
    "    evalSample = (tf.cast(evalSample, tf.float32) - 127.5) / 127.5\n",
    "    iters = len(lfw)//batch_size\n",
    "    print(f'Iterations per epoch: {iters}')\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for _ in range(iters):\n",
    "            gen_loss, disc_loss = train_step(generator, discriminator, batch_size, generator_optimizer, discriminator_optimizer, iterData)\n",
    "        print(f'Epoch {epoch+1}, Gen Loss: {gen_loss.numpy()}, Disc Loss: {disc_loss.numpy()}, Time: {time.time()-start}s')\n",
    "        \n",
    "        real = evalSample\n",
    "        noise = tf.random.normal([64, 512])\n",
    "        fake = generator(noise, training=False)\n",
    "        \n",
    "        print(\"Real: \")\n",
    "        plotImages(real)\n",
    "\n",
    "        print(\"Fake: \")\n",
    "        plotImages(fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "generator.summary(), discriminator.summary()\n",
    "train(generator, discriminator, epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
