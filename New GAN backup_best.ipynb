{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCSGKZcJDnx7"
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.utils import (\n",
    "    to_categorical,\n",
    ")  # Only for categorical one hot encoding\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorboard\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow_datasets.core.utils import gcs_utils\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "gcs_utils._is_gcs_disabled = True\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ./logs/\n",
    "RANDOM_SEED=42\n",
    "MODEL_NAME = 'NewGAN'\n",
    "MODEL_PATH = os.path.join('models', MODEL_NAME)\n",
    "TRAIN_LOGDIR = os.path.join(\"logs\", \"tensorflow\", MODEL_NAME) # Sets up a log directory.\n",
    "# Start a profiler server before your model runs.\n",
    "tf.profiler.experimental.server.start(6009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Optimizations\n",
    "import ctypes\n",
    "\n",
    "_libcudart = ctypes.CDLL('libcudart.so')\n",
    "# Set device limit on the current device\n",
    "# cudaLimitMaxL2FetchGranularity = 0x05\n",
    "pValue = ctypes.cast((ctypes.c_int*1)(), ctypes.POINTER(ctypes.c_int))\n",
    "_libcudart.cudaDeviceSetLimit(ctypes.c_int(0x05), ctypes.c_int(128))\n",
    "_libcudart.cudaDeviceGetLimit(pValue, ctypes.c_int(0x05))\n",
    "assert pValue.contents.value == 128\n",
    "tf.compat.v1.ConfigProto.force_gpu_compatible=True\n",
    "\n",
    "keras.config.set_image_data_format('channels_last')\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "os.environ['TF_GPU_THREAD_COUNT']='1'\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "keras.config.enable_unsafe_deserialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eQiTgWLrkAT"
   },
   "outputs": [],
   "source": [
    "normalizeImage = lambda x: tf.divide(tf.subtract(tf.cast(x, dtype=tf.float32), 127.5), 127.5) \n",
    "denormalizeImage = lambda x: tf.cast(tf.add(tf.multiply(x, 127.5), 127.5), dtype=tf.uint8)\n",
    "\n",
    "def plotImages(imgs):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for i in range(imgs.shape[0]):\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(tf.cast(denormalizeImage(imgs[i, :, :, :]), tf.uint8))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EqualizeLearningRate(layers.Wrapper):\n",
    "    \"\"\"\n",
    "    Reference from WeightNormalization implementation of TF Addons\n",
    "    EqualizeLearningRate wrapper works for keras CNN and Dense (RNN not tested).\n",
    "    ```python\n",
    "      net = EqualizeLearningRate(\n",
    "          layers.Conv2D(2, 2, activation='relu'),\n",
    "          input_shape=(32, 32, 3),\n",
    "          data_init=True)(x)\n",
    "      net = EqualizeLearningRate(\n",
    "          layers.Conv2D(16, 5, activation='relu'),\n",
    "          data_init=True)(net)\n",
    "      net = EqualizeLearningRate(\n",
    "          layers.Dense(120, activation='relu'),\n",
    "          data_init=True)(net)\n",
    "      net = EqualizeLearningRate(\n",
    "          layers.Dense(n_classes),\n",
    "          data_init=True)(net)\n",
    "    ```\n",
    "    Arguments:\n",
    "      layer: a layer instance.\n",
    "    Raises:\n",
    "      ValueError: If `Layer` does not contain a `kernel` of weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        super(EqualizeLearningRate, self).__init__(layer, **kwargs)\n",
    "        self._track_trackable(layer, name=\"layer\")\n",
    "        self.is_rnn = isinstance(self.layer, layers.RNN)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build `Layer`\"\"\"\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        self.input_spec = layers.InputSpec(shape=[None] + input_shape[1:])\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "\n",
    "        kernel_layer = self.layer.cell if self.is_rnn else self.layer\n",
    "\n",
    "        if not hasattr(kernel_layer, \"kernel\"):\n",
    "            raise ValueError(\n",
    "                \"`EqualizeLearningRate` must wrap a layer that\"\n",
    "                \" contains a `kernel` for weights\"\n",
    "            )\n",
    "\n",
    "        if self.is_rnn:\n",
    "            kernel = kernel_layer.recurrent_kernel\n",
    "        else:\n",
    "            kernel = kernel_layer.kernel\n",
    "\n",
    "        # He constant\n",
    "        self.fan_in, self.fan_out = self._compute_fans(kernel.shape)\n",
    "        self.he_constant = tf.Variable(\n",
    "            1.0 / np.sqrt(self.fan_in), dtype=tf.float32, trainable=False\n",
    "        )\n",
    "\n",
    "        self.v = kernel\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"Call `Layer`\"\"\"\n",
    "        # Multiply the kernel with the he constant.\n",
    "        kernel = self.v  # * self.he_constant\n",
    "\n",
    "        if self.is_rnn:\n",
    "            print(self.is_rnn)\n",
    "            self.layer.cell.recurrent_kernel = kernel\n",
    "            update_kernel = tf.identity(self.layer.cell.recurrent_kernel)\n",
    "        else:\n",
    "            self.layer.kernel = kernel\n",
    "            # update_kernel = tf.identity(self.layer.kernel)\n",
    "\n",
    "        # Ensure we calculate result after updating kernel.\n",
    "        # with tf.control_dependencies([update_kernel]):\n",
    "        outputs = self.layer(inputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(self.layer.compute_output_shape(input_shape).as_list())\n",
    "\n",
    "    def _compute_fans(self, shape, data_format=\"channels_last\"):\n",
    "        \"\"\"\n",
    "        From Official Keras implementation\n",
    "        Computes the number of input and output units for a weight shape.\n",
    "        # Arguments\n",
    "            shape: Integer shape tuple.\n",
    "            data_format: Image data format to use for convolution kernels.\n",
    "                Note that all kernels in Keras are standardized on the\n",
    "                `channels_last` ordering (even when inputs are set\n",
    "                to `channels_first`).\n",
    "        # Returns\n",
    "            A tuple of scalars, `(fan_in, fan_out)`.\n",
    "        # Raises\n",
    "            ValueError: in case of invalid `data_format` argument.\n",
    "        \"\"\"\n",
    "        if len(shape) == 2:\n",
    "            fan_in = shape[0]\n",
    "            fan_out = shape[1]\n",
    "        elif len(shape) in {3, 4, 5}:\n",
    "            # Assuming convolution kernels (1D, 2D or 3D).\n",
    "            # TH kernel shape: (depth, input_depth, ...)\n",
    "            # TF kernel shape: (..., input_depth, depth)\n",
    "            if data_format == \"channels_first\":\n",
    "                receptive_field_size = np.prod(shape[2:])\n",
    "                fan_in = shape[1] * receptive_field_size\n",
    "                fan_out = shape[0] * receptive_field_size\n",
    "            elif data_format == \"channels_last\":\n",
    "                receptive_field_size = np.prod(shape[:-2])\n",
    "                fan_in = shape[-2] * receptive_field_size\n",
    "                fan_out = shape[-1] * receptive_field_size\n",
    "            else:\n",
    "                raise ValueError(\"Invalid data_format: \" + data_format)\n",
    "        else:\n",
    "            # No specific assumptions.\n",
    "            fan_in = np.sqrt(np.prod(shape))\n",
    "            fan_out = np.sqrt(np.prod(shape))\n",
    "        return fan_in, fan_out\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, channelReduce=1, name=None, initializers=\"glorot_uniform\", **kwargs):\n",
    "        super(SelfAttention, self).__init__(name=name, **kwargs)\n",
    "        self.channelReduce = channelReduce\n",
    "        self.initializers = initializers\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.name}\n",
    "        base_config = super(SelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return [self.gamma, self.kernel_f, self.kernel_g, self.kernel_h]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.filters_f_g = self.channels // self.channelReduce\n",
    "        self.filters_h = self.channels\n",
    "\n",
    "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
    "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
    "\n",
    "        # Create a trainable weight variable for this layer:\n",
    "        self.gamma = self.add_weight(\n",
    "            name=\"gamma\", shape=[1], initializer=\"zeros\", trainable=True\n",
    "        )\n",
    "        self.kernel_f = self.add_weight(\n",
    "            shape=kernel_shape_f_g,\n",
    "            initializer=self.initializers,\n",
    "            name=\"kernel_f\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.kernel_g = self.add_weight(\n",
    "            shape=kernel_shape_f_g,\n",
    "            initializer=self.initializers,\n",
    "            name=\"kernel_g\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.kernel_h = self.add_weight(\n",
    "            shape=kernel_shape_h,\n",
    "            initializer=self.initializers,\n",
    "            name=\"kernel_h\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, input):\n",
    "        # input = [NHWC]\n",
    "        f_x = tf.nn.conv2d(input, filters=self.kernel_f, strides=(1, 1), padding=\"SAME\")\n",
    "        g_x = tf.nn.conv2d(input, filters=self.kernel_g, strides=(1, 1), padding=\"SAME\")\n",
    "        h_x = tf.nn.conv2d(input, filters=self.kernel_h, strides=(1, 1), padding=\"SAME\")\n",
    "\n",
    "        f_x_flat = hw_flatten(f_x)  # [N(HW)C]\n",
    "        g_x_flat = hw_flatten(g_x)  # [N(HW)C]\n",
    "\n",
    "        s = keras.backend.batch_dot(g_x_flat, keras.backend.permute_dimensions(f_x_flat, (0, 2, 1)))\n",
    "\n",
    "        beta = tf.nn.softmax(s, axis=-1)\n",
    "        o = keras.backend.batch_dot(beta, hw_flatten(h_x))\n",
    "\n",
    "        o = tf.reshape(o, shape=tf.shape(input))  # [bs, h, w, C]\n",
    "        x = self.gamma * o + input\n",
    "        return x\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class PixelNorm(keras.Layer):\n",
    "    def __init__(self, epsilon=1e-8, **kwargs):\n",
    "        super(PixelNorm, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Calculate the square of each element\n",
    "        squared_values = tf.square(inputs)\n",
    "        # Calculate the mean of the squared values across the channel dimension\n",
    "        mean_squared_values = tf.reduce_mean(squared_values, axis=-1, keepdims=True)\n",
    "        # Add epsilon for numerical stability before taking the square root\n",
    "        scaling_factor = tf.math.rsqrt(mean_squared_values + self.epsilon)\n",
    "        # Normalize the inputs by the scaling factor\n",
    "        normalized_inputs = inputs * scaling_factor\n",
    "        return normalized_inputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PixelNorm, self).get_config()\n",
    "        config.update({'epsilon': self.epsilon})\n",
    "        return config\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class FadeAdd(layers.Layer):\n",
    "    def __init__(self, alpha = None, **kwargs):\n",
    "        super(FadeAdd, self).__init__(**kwargs)\n",
    "        if alpha is None:\n",
    "            self.alpha = tf.Variable(initial_value=0.0, trainable=False)\n",
    "        elif not isinstance(alpha, tf.Variable):\n",
    "            self.alpha = tf.Variable(initial_value=alpha, trainable=False)\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "\n",
    "    def incrementAlpha(self, step=0.1):\n",
    "        self.alpha.assign(tf.minimum(self.alpha + step, 1.0))\n",
    "        # Debug print to check the value of alpha if needed\n",
    "        # print(\"New Alpha: \", self.alpha)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        new, old = inputs\n",
    "        return (new * self.alpha) + (old * (1 - self.alpha))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"alpha\": self.alpha.numpy()})\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Assuming the input is a list of two elements, both having the same shape\n",
    "        if isinstance(input_shape, list) and len(input_shape) == 2:\n",
    "            return input_shape[0]  # both 'new' and 'old' inputs should have the same shape\n",
    "        else:\n",
    "            raise ValueError(\"FadeAdd layer should be called on a list of two inputs.\")\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class MinibatchStddev(layers.Layer):\n",
    "    def __init__(self, group_size=4, name=None, **kwargs):\n",
    "        super(MinibatchStddev, self).__init__(name=name, **kwargs)\n",
    "        self.group_size = group_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        group_size = tf.minimum(self.group_size, tf.shape(inputs)[0])\n",
    "        shape = tf.shape(inputs)\n",
    "        minibatch = tf.reshape(inputs, (group_size, -1, shape[1], shape[2], shape[3]))\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(minibatch - tf.reduce_mean(minibatch, axis=0)), axis=0) + 1e-8)\n",
    "        stddev = tf.reduce_mean(stddev, axis=[1, 2, 3], keepdims=True)\n",
    "        stddev = tf.tile(stddev, [group_size, shape[1], shape[2], 1])\n",
    "        return tf.concat([inputs, stddev], axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MinibatchStddev, self).get_config()\n",
    "        config.update({'group_size': self.group_size})\n",
    "        return config\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def hw_flatten(x):\n",
    "    inp_shape = tf.shape(x)\n",
    "    # inp_shape = x.shape\n",
    "    batch_size, height, width, channels = inp_shape[0], inp_shape[1], inp_shape[2], inp_shape[3]\n",
    "    shape = [batch_size, height * width, channels]\n",
    "    return tf.reshape(x, shape=shape)\n",
    "        \n",
    "def layer_init_stddev(shape, gain=np.sqrt(2)):\n",
    "    \"\"\"Get the He initialization scaling term.\"\"\"\n",
    "    fan_in = np.prod(shape[:-1])\n",
    "    return gain / np.sqrt(fan_in)\n",
    "\n",
    "# @keras.saving.register_keras_serializable()\n",
    "class FakeLayer(layers.Layer):\n",
    "    def __init__(self, layer, name=None, **kwargs):\n",
    "        super(FakeLayer, self).__init__(name=name, **kwargs)\n",
    "        self.layer = layer\n",
    "        # self.trainable = True\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.layer(input)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.name, \"layer\": self.layer}\n",
    "        base_config = super(FakeLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def buildModel(layers, inpTensor):\n",
    "    layer = inpTensor\n",
    "    # print(\"Rebasing\")\n",
    "    for i in range(len(layers)):\n",
    "        print(\"Adding layer: \", layer, \" -> \", layers[i])\n",
    "        layer = layers[i](layer)\n",
    "    # print(\"Done\")\n",
    "    return layer\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class PixelReshuffle(keras.Layer):\n",
    "    def __init__(self, upscale_factor, **kwargs):\n",
    "        super(PixelReshuffle, self).__init__(**kwargs)\n",
    "        self.upscale_factor = upscale_factor\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.depth_to_space(inputs, self.upscale_factor)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        height, width, channels = input_shape[1], input_shape[2], input_shape[3]\n",
    "        new_height = height * self.upscale_factor\n",
    "        new_width = width * self.upscale_factor\n",
    "        new_channels = channels // (self.upscale_factor ** 2)\n",
    "        return (input_shape[0], new_height, new_width, new_channels)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PixelReshuffle, self).get_config()\n",
    "        config.update({\"upscale_factor\": self.upscale_factor})\n",
    "        return config\n",
    "    \n",
    "class DepthToSpace(layers.Layer):\n",
    "    def __init__(self, block_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def call(self, input):\n",
    "        batch, height, width, depth = keras.shape(input)\n",
    "        depth = depth // (self.block_size**2)\n",
    "\n",
    "        x = keras.ops.reshape(\n",
    "            input, [batch, height, width, self.block_size, self.block_size, depth]\n",
    "        )\n",
    "        x = keras.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "        x = keras.reshape(\n",
    "            x, [batch, height * self.block_size, width * self.block_size, depth]\n",
    "        )\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.src.utils.numerical_utils import normalize\n",
    "\n",
    "# @tf.function\n",
    "# @tf.function(jit_compile=True)\n",
    "def normalized_weights(vector_u: tf.Variable, kernel: tf.Variable, kernel_shape: tf.shape, power_iterations: int):\n",
    "    \"\"\"Generate spectral normalized weights.\n",
    "    This method returns the updated value for `self.kernel` with the\n",
    "    spectral normalized value, so that the layer is ready for `call()`.\n",
    "    \"\"\"\n",
    "    weights = tf.reshape(kernel, [-1, kernel_shape[-1]])\n",
    "    vec_u_val = vector_u.value\n",
    "\n",
    "    for _ in range(power_iterations):\n",
    "        vector_v = tf.nn.l2_normalize(\n",
    "            tf.matmul(vec_u_val, weights, transpose_b=True), axis=None\n",
    "        )\n",
    "        vec_u_val = tf.nn.l2_normalize(tf.matmul(vector_v, weights), axis=None)\n",
    "    # vector_u = tf.stop_gradient(vector_u)\n",
    "    # vector_v = tf.stop_gradient(vector_v)\n",
    "    sigma = tf.matmul(\n",
    "        tf.matmul(vector_v, weights), vec_u_val, transpose_b=True\n",
    "    )\n",
    "    kernel_val = tf.reshape(tf.divide(kernel, sigma), kernel_shape)\n",
    "    return vec_u_val, kernel_val\n",
    "\n",
    "# @tf.function\n",
    "def spectral_normalize_weights(vector_u: tf.Variable, kernel: tf.Variable, kernel_shape: tf.shape, power_iterations: int):\n",
    "    # new_vector_u, new_kernel = tf.cond(\n",
    "    #     tf.reduce_sum(kernel.value) == 0.0,\n",
    "    #     lambda: (vector_u.value, kernel.value),\n",
    "    #     lambda: normalized_weights(vector_u, kernel, kernel_shape, power_iterations)\n",
    "    # )\n",
    "    new_vector_u, new_kernel = normalized_weights(vector_u, kernel, kernel_shape, power_iterations)\n",
    "    vector_u.assign(new_vector_u)\n",
    "    kernel.assign(new_kernel)\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class SpectralNormalization(layers.Wrapper):\n",
    "    def __init__(self, layer, power_iterations=1, **kwargs):\n",
    "        super().__init__(layer, **kwargs)\n",
    "        if power_iterations <= 0:\n",
    "            raise ValueError(\n",
    "                \"`power_iterations` should be greater than zero. Received: \"\n",
    "                f\"`power_iterations={power_iterations}`\"\n",
    "            )\n",
    "        self.power_iterations = power_iterations\n",
    "        self.is_separable_conv = isinstance(layer, keras.layers.SeparableConv2D)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.input_spec = layers.InputSpec(shape=[None] + list(input_shape[1:]))\n",
    "\n",
    "        if self.is_separable_conv:\n",
    "            self.kernel_depthwise = self.layer.depthwise_kernel\n",
    "            self.kernel_pointwise = self.layer.pointwise_kernel\n",
    "            self.kernel_depthwise_shape = self.kernel_depthwise.shape\n",
    "            self.kernel_pointwise_shape = self.kernel_pointwise.shape\n",
    "            self.vector_u_depthwise = self.add_weight(\n",
    "                shape=(1, self.kernel_depthwise_shape[-1]),\n",
    "                initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                trainable=False,\n",
    "                name=\"vector_u_depthwise\",\n",
    "                dtype=self.kernel_depthwise.dtype,\n",
    "            )\n",
    "            self.vector_u_pointwise = self.add_weight(\n",
    "                shape=(1, self.kernel_pointwise_shape[-1]),\n",
    "                initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                trainable=False,\n",
    "                name=\"vector_u_pointwise\",\n",
    "                dtype=self.kernel_pointwise.dtype,\n",
    "            )\n",
    "        else:\n",
    "            if hasattr(self.layer, \"kernel\"):\n",
    "                self.kernel = self.layer.kernel\n",
    "            elif hasattr(self.layer, \"embeddings\"):\n",
    "                self.kernel = self.layer.embeddings\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"{type(self.layer).__name__} object has no attribute 'kernel' \"\n",
    "                    \"nor 'embeddings'\"\n",
    "                )\n",
    "            self.kernel_shape = self.kernel.shape\n",
    "            self.vector_u = self.add_weight(\n",
    "                shape=(1, self.kernel_shape[-1]),\n",
    "                initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                trainable=False,\n",
    "                name=\"vector_u\",\n",
    "                dtype=self.kernel.dtype,\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if training:\n",
    "            if self.is_separable_conv:\n",
    "                spectral_normalize_weights(\n",
    "                    self.vector_u_depthwise,\n",
    "                    self.kernel_depthwise,\n",
    "                    self.kernel_depthwise_shape,\n",
    "                    self.power_iterations,\n",
    "                )\n",
    "                spectral_normalize_weights(\n",
    "                    self.vector_u_pointwise,\n",
    "                    self.kernel_pointwise,\n",
    "                    self.kernel_pointwise_shape,\n",
    "                    self.power_iterations,\n",
    "                )\n",
    "            else:\n",
    "                spectral_normalize_weights(\n",
    "                    self.vector_u, self.kernel, self.kernel_shape, self.power_iterations\n",
    "                )\n",
    "\n",
    "        output = self.layer(inputs)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.layer.compute_output_shape(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"power_iterations\": self.power_iterations}\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fmap_base = 8192\n",
    "fmap_max = 512\n",
    "fmap_decay = 1.0\n",
    "\n",
    "def nf(stage):\n",
    "    return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
    "\n",
    "class GeneratorBuilder():\n",
    "    def __init__(self, \n",
    "                latent_dim=512, start_res=4, \n",
    "                conv_normalizer: layers.Layer=FakeLayer, \n",
    "                normal_normalizer=PixelNorm,\n",
    "                activation_builder: lambda : layers.Layer=lambda : layers.LeakyReLU(alpha=0.2),\n",
    "                upsampling_builder: lambda : layers.Layer=lambda : layers.UpSampling2D((2, 2), interpolation='bilinear'),\n",
    "                alpha:tf.Variable=None,\n",
    "                kernel_constraint=None,\n",
    "                kernel_initializer_builder=lambda:keras.initializers.RandomNormal(mean=0.0, stddev=0.02)(),\n",
    "                freeze_oldlayers=False,\n",
    "                use_sepconv=False,\n",
    "                deviation_loss=keras.losses.mean_absolute_error\n",
    "            ):\n",
    "        self.conv_normalizer = conv_normalizer\n",
    "        self.activation_builder = activation_builder\n",
    "        self.normal_normalizer = normal_normalizer\n",
    "        self.general_layer_configs = {\n",
    "            \"kernel_constraint\": kernel_constraint,\n",
    "            \"kernel_initializer\": kernel_initializer_builder(),\n",
    "            \"use_bias\": False,\n",
    "        }\n",
    "        self.conv_layer_configs = {\n",
    "            **self.general_layer_configs,\n",
    "            \"padding\": \"same\",\n",
    "        }\n",
    "        self.separableconv_layer_configs = {\n",
    "            \"use_bias\": False,\n",
    "            \"padding\": \"same\",\n",
    "            \"depthwise_constraint\": kernel_constraint,\n",
    "            \"depthwise_initializer\": kernel_initializer_builder(),\n",
    "            \"pointwise_constraint\": kernel_constraint,\n",
    "            \"pointwise_initializer\": kernel_initializer_builder(),\n",
    "        }\n",
    "        self.current_res = None\n",
    "        self.start_res = start_res\n",
    "        self.latent_dim = latent_dim\n",
    "        self.upsampling_builder = upsampling_builder\n",
    "        self.alpha = alpha\n",
    "        self.freeze_oldlayers = freeze_oldlayers\n",
    "        self.use_sepconv = use_sepconv\n",
    "        self.downscaler = layers.AveragePooling2D((2, 2))\n",
    "        self.global_block_id = 0\n",
    "        self.deviation_loss = deviation_loss\n",
    "        \n",
    "    def gen_block_name(self, res, blocktype, suffix='', use_blockid=True):\n",
    "        if use_blockid == True:\n",
    "            blockid = self.global_block_id\n",
    "            self.global_block_id += 1\n",
    "            if suffix == '':\n",
    "                return f\"{res}x{res}_{blocktype}-{blockid}\"\n",
    "            return f\"{res}x{res}_{blocktype}-{blockid}_{suffix}\"\n",
    "        else:\n",
    "            if suffix == '':\n",
    "                return f\"{res}x{res}_{blocktype}\"\n",
    "            return f\"{res}x{res}_{blocktype}_{suffix}\"\n",
    "\n",
    "    def freeze_layers(self):\n",
    "        for layer in self.current_model.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "    def unfreeze_layers(self):\n",
    "        for layer in self.current_model.layers:\n",
    "            layer.trainable = True\n",
    "            \n",
    "    def make_conv(self, res, features, kernel_size=(3, 3), blockid=0, suffix=\"\"):\n",
    "        if self.use_sepconv == True:\n",
    "            return self.conv_normalizer(layers.SeparableConv2D(features, kernel_size=kernel_size, **self.separableconv_layer_configs), name=self.gen_block_name(res, \"sepconv\", suffix))\n",
    "        else:\n",
    "            return self.conv_normalizer(layers.Conv2D(features, kernel_size=kernel_size, **self.conv_layer_configs), name=self.gen_block_name(res, \"conv\", suffix))\n",
    "        \n",
    "    def make_output_block(self, res, input_tensor):\n",
    "        output_tensor = self.conv_normalizer(\n",
    "            layers.Conv2D(3, kernel_size=(1, 1), **self.conv_layer_configs), \n",
    "            name=self.gen_block_name(res, 'conv', 'output', use_blockid=False)\n",
    "        )(input_tensor)\n",
    "        output_tensor = layers.Activation('tanh', name=self.gen_block_name(res, \"output\", use_blockid=False))(output_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "    def build_base(self, latent_dim, start_res):\n",
    "        log_res = int(np.log2(start_res))\n",
    "        features = nf(log_res - 1)\n",
    "        \n",
    "        input_tensor = layers.Input(shape=(latent_dim,))\n",
    "    \n",
    "        x = layers.Reshape((start_res, start_res, latent_dim // ((start_res)**2)))(input_tensor)\n",
    "        x = PixelNorm()(x)\n",
    "        # PixelReshuffle(2)\n",
    "        x = self.conv_normalizer(layers.Conv2D(features, kernel_size=(1, 1),**self.conv_layer_configs), name=self.gen_block_name(start_res, \"conv\", \"base_first\"))(x)\n",
    "        x = self.activation_builder()(x)\n",
    "        x = self.normal_normalizer()(x)\n",
    "\n",
    "        x = self.make_conv(start_res, features, kernel_size=(3, 3), suffix=\"base\")(x)\n",
    "        x = self.activation_builder()(x)\n",
    "        x = self.normal_normalizer()(x)\n",
    "        \n",
    "        x = self.make_conv(start_res, features, kernel_size=(3, 3), suffix=\"base\")(x)\n",
    "        x = self.activation_builder()(x)\n",
    "        x = self.normal_normalizer(name=self.gen_block_name(start_res, \"final\", use_blockid=False))(x)\n",
    "        \n",
    "        output_tensor = self.make_output_block(start_res, x)\n",
    "        \n",
    "        print(\"Generator base built with shape \", output_tensor.shape)\n",
    "        self.current_model = keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "        # self.lower_model = keras.Model(inputs=input_tensor, outputs=x)\n",
    "        # self.higher_model = keras.Model(inputs=x, outputs=[output_tensor])\n",
    "        self.higher_model = None\n",
    "        self.current_res = start_res\n",
    "    \n",
    "    def add_stage(self, res=0):\n",
    "        if res == 0:\n",
    "            res = self.current_res * 2\n",
    "            assert res == self.current_model.output.shape[1] * 2\n",
    "        log_res = int(np.log2(res))\n",
    "        features = nf(log_res - 1)\n",
    "        # We need to add stage after the last layer\n",
    "        \n",
    "        if self.freeze_oldlayers:\n",
    "            self.freeze_layers()\n",
    "    \n",
    "        last_final_tensor = self.current_model.get_layer(self.gen_block_name(res // 2, \"final\", use_blockid=False)).output\n",
    "        print(\"Adding stage \", res)\n",
    "        x = PixelReshuffle(2)(last_final_tensor)    # Upscale\n",
    "        \n",
    "        x = self.make_conv(res, features, kernel_size=(3, 3))(x)\n",
    "        x = self.activation_builder()(x)\n",
    "        x = self.normal_normalizer()(x)\n",
    "        \n",
    "        x = self.make_conv(res, features, kernel_size=(3, 3), blockid=1)(x)\n",
    "        x = self.activation_builder()(x)\n",
    "        x = self.normal_normalizer(name=self.gen_block_name(res, \"final\", use_blockid=False))(x)\n",
    "        \n",
    "        output_tensor = self.make_output_block(res, x)      # 16x16 RGB Image Output\n",
    "        input_tensor = self.current_model.input\n",
    "        last_output_tensor = self.current_model.get_layer(self.gen_block_name(res // 2, \"output\", use_blockid=False)).output\n",
    "        \n",
    "        self.current_model = keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "        self.lower_model = keras.Model(inputs=input_tensor, outputs=last_final_tensor)\n",
    "        self.higher_model = keras.Model(inputs=last_final_tensor, outputs=[last_output_tensor, output_tensor])\n",
    "        \n",
    "        last_conv_output_layer = self.higher_model.get_layer(self.gen_block_name(res // 2, \"conv\", \"output\", use_blockid=False))\n",
    "        last_conv_output_layer.trainable = False\n",
    "        self.current_res = res\n",
    "    \n",
    "    def grow(self, res=0):\n",
    "        if self.current_res is None:\n",
    "            self.build_base(self.latent_dim, self.start_res)\n",
    "        else:\n",
    "            self.add_stage(res)\n",
    "        # return self.model\n",
    "        \n",
    "    def get_layers(self):\n",
    "        if self.higher_model == None:\n",
    "            return self.current_model.layers\n",
    "        return self.lower_model.layers + self.higher_model.layers\n",
    "        \n",
    "    def predict(self, input, training=False) -> tf.Tensor:\n",
    "        return self.current_model(input, training=training)\n",
    "\n",
    "    def composite_predict(self, input, training=True):\n",
    "        if self.higher_model == None:\n",
    "            preds = self.predict(input, training=training)\n",
    "            return preds, None, 0\n",
    "        preds = self.lower_model(input, training=training)\n",
    "        lower_out, higher_out = self.higher_model(preds, training=True)\n",
    "        high_to_low = self.downscaler(higher_out)\n",
    "        # print(lower_out.shape, higher_out.shape, high_to_low.shape)\n",
    "        dev_loss = self.deviation_loss(lower_out, high_to_low)\n",
    "        return higher_out, lower_out, tf.reduce_mean(dev_loss)\n",
    "    \n",
    "    def get_trainable_variables(self):\n",
    "        if self.higher_model == None:\n",
    "            return self.current_model.trainable_variables\n",
    "        else:\n",
    "            return self.higher_model.trainable_variables + self.lower_model.trainable_variables\n",
    "    \n",
    "class DescriminatorBuilder():\n",
    "    def __init__(self,\n",
    "                start_res=4, \n",
    "                conv_normalizer: layers.Layer=SpectralNormalization,\n",
    "                normal_normalizer=lambda: FakeLayer(lambda x: x),\n",
    "                activation_builder: lambda : layers.Layer=lambda : layers.LeakyReLU(alpha=0.2),\n",
    "                alpha:tf.Variable=tf.Variable(0.0, dtype=tf.float32),\n",
    "                kernel_constraint=None,\n",
    "                kernel_initializer_builder=lambda:keras.initializers.RandomNormal(mean=0.0, stddev=0.02)(),\n",
    "                freeze_oldlayers=False,\n",
    "                use_sepconv=False,\n",
    "            ):\n",
    "        self.conv_normalizer = conv_normalizer\n",
    "        self.activation_builder = activation_builder\n",
    "        self.normal_normalizer = normal_normalizer\n",
    "        self.general_layer_configs = {\n",
    "            \"kernel_constraint\": kernel_constraint,\n",
    "            \"kernel_initializer\": kernel_initializer_builder(),\n",
    "            \"use_bias\": False,\n",
    "        }\n",
    "        self.conv_layer_configs = {\n",
    "            **self.general_layer_configs,\n",
    "            \"padding\": \"same\",\n",
    "        }\n",
    "        self.separableconv_layer_configs = {\n",
    "            \"use_bias\": False,\n",
    "            \"padding\": \"same\",\n",
    "            \"depthwise_constraint\": kernel_constraint,\n",
    "            \"depthwise_initializer\": kernel_initializer_builder(),\n",
    "            \"pointwise_constraint\": kernel_constraint,\n",
    "            \"pointwise_initializer\": kernel_initializer_builder(),\n",
    "        }\n",
    "        self.current_res = None\n",
    "        self.start_res = start_res\n",
    "        self.alpha = alpha\n",
    "        self.freeze_oldlayers = freeze_oldlayers\n",
    "        self.use_sepconv = use_sepconv\n",
    "        self.global_block_id = 0\n",
    "        print(\"Use SepConv: \", self.use_sepconv)\n",
    "        \n",
    "    def gen_block_name(self, res, blocktype, suffix='', use_blockid=True):\n",
    "        if use_blockid == True:\n",
    "            blockid = self.global_block_id\n",
    "            self.global_block_id += 1\n",
    "            if suffix == '':\n",
    "                return f\"{res}x{res}_{blocktype}-{blockid}\"\n",
    "            return f\"{res}x{res}_{blocktype}-{blockid}_{suffix}\"\n",
    "        else:\n",
    "            if suffix == '':\n",
    "                return f\"{res}x{res}_{blocktype}\"\n",
    "            return f\"{res}x{res}_{blocktype}_{suffix}\"\n",
    "\n",
    "    def freeze_layers(self):\n",
    "        for layer in self.model.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "    def unfreeze_layers(self):\n",
    "        for layer in self.model.layers:\n",
    "            layer.trainable = True\n",
    "            \n",
    "    def make_conv(self, res, features, kernel_size=(3, 3), blockid=0, suffix=\"\"):\n",
    "        if self.use_sepconv == True:\n",
    "            return self.conv_normalizer(layers.SeparableConv2D(features, kernel_size=kernel_size, **self.separableconv_layer_configs), name=self.gen_block_name(res, \"sepconv\", suffix))\n",
    "        else:\n",
    "            return self.conv_normalizer(layers.Conv2D(features, kernel_size=kernel_size, **self.conv_layer_configs), name=self.gen_block_name(res, \"conv\", suffix))\n",
    "        \n",
    "    def build_base(self, start_res):\n",
    "        log_res = int(np.log2(start_res))\n",
    "        features = nf(log_res - 1)\n",
    "        \n",
    "        input_tensor = layers.Input(shape=(start_res, start_res, 3))\n",
    "        \n",
    "        initial_layers = []\n",
    "        # initial_layers.append(MinibatchStddev())\n",
    "        initial_layers.append(self.make_conv(start_res, features, kernel_size=(1, 1), suffix=\"base_extractor\"))\n",
    "        initial_layers.append(self.normal_normalizer())\n",
    "        initial_layers.append(self.activation_builder())\n",
    "        \n",
    "        main_layers = []\n",
    "        main_layers.append(MinibatchStddev())\n",
    "        main_layers.append(self.make_conv(start_res, features, kernel_size=(3, 3), suffix=\"base\"))\n",
    "        main_layers.append(self.normal_normalizer())\n",
    "        main_layers.append(self.activation_builder())\n",
    "        \n",
    "        main_layers.append(layers.Flatten())\n",
    "        # layers.GlobalAveragePooling2D())\n",
    "        main_layers.append(layers.Dense(1, name=\"output_logit\", **self.general_layer_configs))\n",
    "        \n",
    "        output_tensor = buildModel(initial_layers + main_layers, input_tensor)\n",
    "        \n",
    "        self.main_layers = main_layers\n",
    "        self.initial_layers = initial_layers\n",
    "        \n",
    "        self.model = keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "        self.current_res = start_res\n",
    "        self.old_model_cloned = self.model\n",
    "        \n",
    "    def add_stage(self, res=0):\n",
    "        if res == 0:\n",
    "            res = self.current_res * 2\n",
    "            assert res == self.model.input.shape[1] * 2\n",
    "        log_res = int(np.log2(res))\n",
    "        features = nf(log_res - 1)\n",
    "        lowerFeatures = nf(log_res - 2)\n",
    "        \n",
    "        if self.freeze_oldlayers:\n",
    "            for layer in self.initial_layers:\n",
    "                layer.trainable = False\n",
    "                \n",
    "            for layer in self.main_layers:\n",
    "                layer.trainable = False\n",
    "        \n",
    "        # We need to add stage before all the main layers\n",
    "        \n",
    "        input_layer = layers.Input(shape=(res, res, 3))\n",
    "        print(\"Adding stage \", res)\n",
    "        downscaled = layers.AveragePooling2D((2, 2))(input_layer)\n",
    "        old_downscaled_tensor = buildModel(self.initial_layers, downscaled)\n",
    "        \n",
    "        new_initial_layers = []\n",
    "        # new_initial_layers.append(MinibatchStddev())\n",
    "        new_initial_layers.append(self.make_conv(res, features, kernel_size=(1, 1), suffix=\"extractor\"))\n",
    "        new_initial_layers.append(self.normal_normalizer())\n",
    "        new_initial_layers.append(self.activation_builder())\n",
    "        \n",
    "        new_main_layers = []\n",
    "        # Add self-attention layer conditionally based on resolution\n",
    "        # if res >= 16:\n",
    "        #     new_SelfAttention(name=\"attention_{res}\".format(res=res)))\n",
    "            \n",
    "        # new_MinibatchStddev())\n",
    "        new_main_layers.append(self.make_conv(res, features, kernel_size=(3, 3), suffix=\"\"))\n",
    "        new_main_layers.append(self.normal_normalizer())\n",
    "        new_main_layers.append(self.activation_builder())\n",
    "        \n",
    "        # Add self-attention layer conditionally based on resolution\n",
    "        # if res >= 16:\n",
    "        #     new_SelfAttention(name=\"attention2_{res}\".format(res=res)))  \n",
    "\n",
    "        new_main_layers.append(self.make_conv(res, lowerFeatures, kernel_size=(3, 3), suffix=\"\"))\n",
    "        new_main_layers.append(self.normal_normalizer())\n",
    "        new_main_layers.append(self.activation_builder())\n",
    "        \n",
    "        new_main_layers.append(layers.AveragePooling2D((2, 2)))\n",
    "        \n",
    "        new_downscaled_tensor = buildModel(new_initial_layers + new_main_layers, input_layer)\n",
    "        \n",
    "        alpha = FadeAdd(self.alpha)\n",
    "        new_downscaled_tensor = alpha([new_downscaled_tensor, old_downscaled_tensor])\n",
    "        \n",
    "        print(\"New downscaled tensor: \", new_downscaled_tensor)\n",
    "        \n",
    "        output_tensor = buildModel(self.main_layers, new_downscaled_tensor)\n",
    "        \n",
    "        self.main_layers = new_main_layers + self.main_layers\n",
    "        self.initial_layers = new_initial_layers\n",
    "        # self.old_model_cloned = keras.models.clone_model(self.model)\n",
    "        self.model.save('tmp.keras')\n",
    "        self.old_model_cloned = keras.models.load_model('tmp.keras', custom_objects={\n",
    "            \"FadeAdd\": FadeAdd,\n",
    "            \"MinibatchStddev\": MinibatchStddev,\n",
    "            \"SelfAttention\": SelfAttention,\n",
    "            \"SpectralNormalization\": SpectralNormalization,\n",
    "            \"FakeLayer\": FakeLayer,\n",
    "        })\n",
    "        self.old_model_cloned.trainable = False\n",
    "        self.old_model_original = self.model\n",
    "        self.model = keras.Model(inputs=input_layer, outputs=output_tensor)\n",
    "        self.current_res = res\n",
    "        \n",
    "    def grow(self, res=0) -> keras.Model:\n",
    "        if self.current_res is None:\n",
    "            self.build_base(self.start_res)\n",
    "        else:\n",
    "            self.add_stage(res)\n",
    "        return self.model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gBuilder = GeneratorBuilder()\n",
    "gBuilder.grow()\n",
    "gBuilder.grow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gBuilder = GeneratorBuilder(freeze_oldlayers=True)\n",
    "for i in range(6):\n",
    "    g = gBuilder.grow()\n",
    "\n",
    "# keras.utils.plot_model(\n",
    "#     g,\n",
    "#     to_file=\"generator.png\",\n",
    "#     expand_nested=True,\n",
    "#     show_shapes=True,\n",
    "#     show_layer_names=True,\n",
    "#     dpi=96,\n",
    "#     show_trainable=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.summary(), d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    d,\n",
    "    to_file=\"descriminator.png\",\n",
    "    expand_nested=True,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    dpi=96,\n",
    "    show_trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(a, b, t):\n",
    "    return a + (b - a) * t\n",
    "\n",
    "def generate_input_data(batch_size=64):\n",
    "    return tf.random.normal([batch_size, 512])\n",
    "\n",
    "def discriminator_wgan_loss(real_output, fake_output):\n",
    "    # wgan_loss = fake_output - real_output\n",
    "    # return wgan_loss\n",
    "    real_loss = tf.reduce_mean(real_output)\n",
    "    fake_loss = tf.reduce_mean(fake_output)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "def generator_wgan_loss(fake_output):\n",
    "    total_loss = -tf.reduce_mean(fake_output)\n",
    "    return total_loss\n",
    "\n",
    "# @tf.function\n",
    "def descriminator_hinge_loss(real_output, fake_output):    \n",
    "    real_loss = tf.reduce_mean(tf.nn.relu(1.0 - real_output))\n",
    "    fake_loss = tf.reduce_mean(tf.nn.relu(1.0 + fake_output))\n",
    "    des_loss = real_loss + fake_loss\n",
    "    return des_loss\n",
    "\n",
    "# @tf.function\n",
    "def generator_hinge_loss(fake_output):\n",
    "    total_loss = -tf.reduce_mean(fake_output)\n",
    "    return total_loss\n",
    "\n",
    "# @tf.function\n",
    "def descriminator_softplus_loss(real_output, fake_output):\n",
    "    des_loss = tf.nn.softplus(fake_output) + tf.nn.softplus(-real_output)\n",
    "    return tf.reduce_mean(des_loss)\n",
    "\n",
    "# @tf.function\n",
    "def generator_softplus_loss(fake_output):\n",
    "    return tf.reduce_mean(tf.nn.softplus(-fake_output))\n",
    "\n",
    "def generator_enc_loss(real, fake):\n",
    "    # return tf.reduce_mean(tf.abs(real - fake))\n",
    "    return tf.abs(real - fake)\n",
    "\n",
    "# @tf.function\n",
    "def gradientPenalty(des, real_images, fake_images):\n",
    "    with tf.GradientTape() as tape:\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        epsilon = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0, dtype=tf.float32)\n",
    "        interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "        tape.watch(interpolates)\n",
    "        pred = des(interpolates, training=True)\n",
    "    grads = tape.gradient(pred, [interpolates])[0]\n",
    "    l2norm = tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.size(tf.shape(grads))))\n",
    "    norm = tf.sqrt(l2norm)\n",
    "    loss = tf.square(norm - 1)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "# @tf.function\n",
    "def gradientPenaltyAlt(des, real_images, fake_images):\n",
    "    with tf.GradientTape() as tape:\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        epsilon = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0, dtype=tf.float32)\n",
    "        interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "        tape.watch(interpolates)\n",
    "        pred = des(interpolates, training=True)\n",
    "    grads = tape.gradient(pred, [interpolates])[0]\n",
    "    l2norm = tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.size(tf.shape(grads))))\n",
    "    # loss = l2norm\n",
    "    loss = tf.reduce_mean(l2norm)\n",
    "    return loss\n",
    "\n",
    "# @tf.function\n",
    "def r1Penalty(des, real_images, fake_images):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(real_images)\n",
    "        pred = des(real_images, training=True)\n",
    "        # pred = tf.reduce_sum(pred)\n",
    "    grads = tape.gradient(pred, [real_images])[0]\n",
    "    l2norm = tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.size(tf.shape(grads))))\n",
    "    loss = l2norm\n",
    "    loss = tf.reduce_mean(l2norm)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augmenter(size, alpha=None, method='area'):\n",
    "  if alpha is not None:\n",
    "    @tf.function()\n",
    "    def augment(sample):\n",
    "      image = (tf.cast(sample['image'], tf.float32) - 127.5) / 127.5\n",
    "      big = tf.image.resize(image, [size, size], method=method, antialias=True)\n",
    "      small =  tf.image.resize(image, [size//2, size//2], method=method, antialias=True)\n",
    "      small = tf.image.resize(small, [size, size], method='area')\n",
    "      image = (big*alpha.alpha) + (small*(1-alpha.alpha))\n",
    "      image = tf.image.random_flip_left_right(image)\n",
    "      return {'image':image}\n",
    "    return augment\n",
    "  else:\n",
    "    @tf.function()\n",
    "    def augment(sample):\n",
    "      image = (tf.cast(sample['image'], tf.float32) - 127.5) / 127.5\n",
    "      image = tf.image.resize(image, [size, size], method=method, antialias=True)\n",
    "      image = tf.image.random_flip_left_right(image)\n",
    "      return {'image':image}\n",
    "    return augment\n",
    "    \n",
    "\n",
    "class ProGANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, batch_size, sample_real_data, name=None, tf_log_dir=None):\n",
    "        super().__init__()\n",
    "        self.sample_real_data = sample_real_data\n",
    "        self.sample_noise = generate_input_data(batch_size=batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.results = []\n",
    "        self.tf_writer = tf.summary.create_file_writer(os.path.join(tf_log_dir, 'train'))\n",
    "    \n",
    "    \n",
    "    def evaluate(self, batches=10):\n",
    "        desAcc = 0\n",
    "        genLoss = 0\n",
    "        gen = self.model.gen\n",
    "        des = self.model.des\n",
    "        for _ in range(batches):\n",
    "            real = self.sample_real_data\n",
    "            fake = gen.predict(generate_input_data(batch_size=self.batch_size), training=False)\n",
    "\n",
    "            real_output = des(real, training=False)\n",
    "            fake_output = des(fake, training=False)\n",
    "\n",
    "            output = tf.concat((fake_output, real_output), axis=0)\n",
    "\n",
    "            labels = tf.reshape(\n",
    "                tf.concat(\n",
    "                    (tf.zeros_like(fake_output), tf.ones_like(real_output)), axis=0\n",
    "                ),\n",
    "                [-1],\n",
    "            )\n",
    "            output = tf.reshape(output, [-1])\n",
    "\n",
    "            acc = keras.metrics.binary_accuracy(labels, output, threshold=0.5)\n",
    "            desAcc += acc.numpy()\n",
    "            genLoss += (\n",
    "                tf.reduce_sum(generator_wgan_loss(fake_output)).numpy() / self.batch_size\n",
    "            )\n",
    "        return desAcc / batches, genLoss / batches\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"Original Samples\")\n",
    "        plotImages(self.sample_real_data)\n",
    "        fake = self.model.gen.predict(self.sample_noise, training=False)\n",
    "        print(\"Generated Samples\")\n",
    "        plotImages(fake)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "        print(\"Starting epoch \", epoch)\n",
    "        \n",
    "        if epoch >= self.model.alpha_delay_epochs:\n",
    "            self.model.can_update_alpha.assign(True)\n",
    "            # we increment the alpha by the multiplier\n",
    "            self.model.alpha_incr_rate.assign(\n",
    "                self.model.alpha_incr_rate * self.model.alpha_multiplier\n",
    "            )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 == 0:\n",
    "            clear_output()\n",
    "\n",
    "        print(\"\\nEpoch complete. Epoch Time: \", time.time() - self.epoch_start_time)\n",
    "        desAcc, genLoss = self.evaluate()\n",
    "        self.results.append({\"desAcc\": desAcc, \"genLoss\": genLoss})\n",
    "        fake = self.model.gen.predict(self.sample_noise, training=False)\n",
    "        print(\"Epoch \", epoch, \"Descriminator Accuracy \", desAcc, \"Generator Loss \", genLoss)\n",
    "        with self.tf_writer.as_default():\n",
    "            tf.summary.scalar(\"Descriminator Accuracy at Epoch\", desAcc, step=epoch)\n",
    "            tf.summary.scalar(\"Generator Loss at Epoch\", genLoss, step=epoch)\n",
    "            # Share images of fake data generated\n",
    "            tf.summary.image(\n",
    "                \"Fake Images\", tf.cast(denormalizeImage(fake), tf.uint8), step=epoch, max_outputs=10\n",
    "            )\n",
    "            # Share the layers of the model as histograms\n",
    "            for layer in self.model.des.layers:\n",
    "                # If the layer has a 'kernel' or a 'embeddings' attribute\n",
    "                weights = layer.get_weights()\n",
    "                for i, weight in enumerate(weights):\n",
    "                    # print(\"Des Layer: \", layer.name, \"Weight: \", i, \"Shape: \", weight.shape)\n",
    "                    tf.summary.histogram(f\"Weights/Descriminator/{layer.name}/{i}\", weight, step=epoch)\n",
    "                    # tf.summary.image(f\"Weights/Descriminator/{layer.name}{i}/image\", tf.expand_dims(weight, axis=1), step=epoch)\n",
    "            for layer in self.model.gen.get_layers():\n",
    "                weights = layer.get_weights()\n",
    "                for i, weight in enumerate(weights):\n",
    "                    # print(\"Gen Layer: \", layer.name, \"Weight: \", i, \"Shape: \", weight.shape)\n",
    "                    tf.summary.histogram(f\"Weights/Generator/{layer.name}/{i}\", weight, step=epoch)\n",
    "                    # tf.summary.image(f\"Weights/Generator/{layer.name}/{i}/image\", tf.expand_dims(weight, axis=1), step=epoch)\n",
    "        print(\"Real: \")\n",
    "        plotImages(self.sample_real_data)\n",
    "        print(\"Fake: \")\n",
    "        plotImages(fake)\n",
    "        print(\"Final Alpha: \", self.model.alpha.value())\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if tf.math.mod(self.model.train_step_counter.value(), 20) == 0:\n",
    "            with self.tf_writer.as_default():\n",
    "                tf.summary.scalar(\"Batch\", batch, step=self.model.train_step_counter)\n",
    "                tf.summary.scalar(\n",
    "                    \"Alpha\",\n",
    "                    self.model.alpha.value(),\n",
    "                    step=self.model.train_step_counter,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"Generator Loss\",\n",
    "                    # logs[\"gen_loss\"],\n",
    "                    self.model.g_loss_metric.result(),\n",
    "                    step=self.model.train_step_counter,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"Descriminator Loss\",\n",
    "                    # logs[\"des_loss\"],\n",
    "                    self.model.d_loss_metric.result(),\n",
    "                    step=self.model.train_step_counter,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"Descriminator core loss\",\n",
    "                    logs['des_main_loss'],\n",
    "                    step=self.model.train_step_counter,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"Descriminator gradient penalty\",\n",
    "                    logs['des_gradient_penalty'],\n",
    "                    step=self.model.train_step_counter,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"Descriminator drift loss\",\n",
    "                    logs['des_drift_loss'],\n",
    "                    step=self.model.train_step_counter,\n",
    "                )                   \n",
    "        \n",
    "class ProGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_res=4,\n",
    "        name=None,\n",
    "        gen_loss=generator_wgan_loss,\n",
    "        des_loss=discriminator_wgan_loss,\n",
    "        gradient_penalty_calculator=gradientPenalty,\n",
    "        des_steps=1,\n",
    "        gen_steps=1,\n",
    "        gen_config:dict={},\n",
    "        des_config:dict={},\n",
    "        loss_weights={\"gradient_penalty\": 10, \"drift\": 0.001},\n",
    "        initial_alpha=0.0,\n",
    "        should_train_separately=False,\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        # gen, des, enc, baseLayers = generateBaseModels()\n",
    "        self.alpha = tf.Variable(initial_alpha, trainable=False, dtype=tf.float32)\n",
    "        self.initial_alpha = initial_alpha\n",
    "        self.desBuilder = DescriminatorBuilder(alpha=self.alpha, start_res=start_res, **des_config)\n",
    "        self.genBuilder = GeneratorBuilder(alpha=self.alpha, start_res=start_res, **gen_config)\n",
    "        self.genBuilder.grow()\n",
    "        gen = self.genBuilder\n",
    "        des = self.desBuilder.grow()\n",
    "        self.gen = gen\n",
    "        self.des = des\n",
    "        self.gen_loss = gen_loss\n",
    "        self.des_loss = des_loss\n",
    "        self.des_steps = des_steps\n",
    "        self.gen_steps = gen_steps\n",
    "        self.current_res = start_res\n",
    "        self.train_step_counter = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "        self.loss_weights = loss_weights\n",
    "        self.g_optimizer = None\n",
    "        self.d_optimizer = None\n",
    "        self.can_update_alpha = tf.Variable(False, dtype=tf.bool, trainable=False)\n",
    "        self.gradient_penalty_calculator = gradient_penalty_calculator\n",
    "        self.should_train_separately = should_train_separately\n",
    "\n",
    "    \"\"\"\n",
    "    Grows the model by doubling the resolution\n",
    "    \"\"\"\n",
    "\n",
    "    def grow_model(self):\n",
    "        self.current_res *= 2\n",
    "        newRes = self.current_res\n",
    "        # Reset the alpha\n",
    "        self.alpha.assign(self.initial_alpha)\n",
    "        # Add a new stage to the Des and Gen\n",
    "        des = self.desBuilder.grow()\n",
    "        self.genBuilder.grow()\n",
    "        self.des = des\n",
    "        pass\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        res: int,\n",
    "        steps_per_epoch: int,\n",
    "        g_optimizer: keras.Optimizer,\n",
    "        d_optimizer: keras.Optimizer,\n",
    "        alpha_incr_rate: float = 0.1,\n",
    "        alpha_delay_epochs: int = 0,\n",
    "        alpha_multiplier: float = 1.1,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.loss_weights = kwargs.pop(\"loss_weights\", self.loss_weights)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.alpha_incr_rate = tf.Variable(alpha_incr_rate, dtype=tf.float32, trainable=False)\n",
    "        self.alpha_delay_epochs = alpha_delay_epochs\n",
    "        self.alpha_multiplier = alpha_multiplier\n",
    "\n",
    "        self.can_update_alpha.assign(False)\n",
    "\n",
    "        if g_optimizer is None:\n",
    "            g_optimizer = keras.optimizers.Adam(\n",
    "                learning_rate=1e-4, beta_1=0, beta_2=0.999\n",
    "            )\n",
    "        if d_optimizer is None:\n",
    "            d_optimizer = keras.optimizers.Adam(\n",
    "                learning_rate=1e-4, beta_1=0, beta_2=0.999\n",
    "            )\n",
    "\n",
    "        # self.train_step_counter.assign(0)\n",
    "        # self.train_step_counter = 0\n",
    "\n",
    "        if res != self.current_res:\n",
    "            self.g_optimizer = g_optimizer\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.grow_model()\n",
    "        elif self.g_optimizer is None:\n",
    "            self.g_optimizer = g_optimizer\n",
    "            self.d_optimizer = d_optimizer\n",
    "\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"des_loss_metric\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"gen_loss_metric\")\n",
    "        self.tf_train_step = tf.function(self._train_step, jit_compile=True)\n",
    "        self.train_discriminator = tf.function(self._train_discriminator, jit_compile=True)\n",
    "        self.train_generator = tf.function(self._train_generator, jit_compile=True)\n",
    "        super().compile(*args, **kwargs)\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def _train_generator(self, batch_size):\n",
    "        with tf.GradientTape() as tape:\n",
    "            noise = generate_input_data(batch_size=batch_size)\n",
    "            fake_images, fake_images_lower_res, dev_loss = self.genBuilder.composite_predict(noise, training=True)\n",
    "            fake_output = self.des(fake_images, training=False)\n",
    "            # Decay devloss with steps\n",
    "            # dev_decay = (1.0 - tf.minimum(1.0, tf.cast(self.train_step_counter, tf.float32) / self.steps_per_epoch) * 0.1) * self.alpha\n",
    "            loss = self.gen_loss(fake_output) + dev_loss# * dev_decay\n",
    "            if fake_images_lower_res != None:\n",
    "                # If we have lower resolution images, we also need to minimize the loss for them\n",
    "                # so the generator can continue to fool the old already trained lower res descriminator\n",
    "                fake_output_lower_res = self.desBuilder.old_model_cloned(fake_images_lower_res)\n",
    "                loss += self.gen_loss(fake_output_lower_res)\n",
    "        gradients = tape.gradient(loss, self.genBuilder.get_trainable_variables())\n",
    "        self.g_optimizer.apply_gradients(zip(gradients, self.genBuilder.get_trainable_variables()))\n",
    "        # gradients = tape.gradient(loss, )\n",
    "        # self.g_optimizer.apply_gradients(zip(gradients, self.genBuilder.lower_model.trainable_variables))\n",
    "        return loss, fake_images\n",
    "\n",
    "    def _train_discriminator(self, real_images, batch_size):\n",
    "        with tf.GradientTape() as total_tape:\n",
    "            noise = generate_input_data(batch_size=batch_size)\n",
    "            fake_images = self.gen.predict(noise, training=False)\n",
    "            real_output = self.des(real_images, training=True)\n",
    "            fake_output = self.des(fake_images, training=True)\n",
    "            # combined_images = tf.concat([real_images, fake_images], axis=0)\n",
    "            # combined_output = self.des(combined_images, training=True)\n",
    "            # real_output, fake_output = tf.split(combined_output, num_or_size_splits=2, axis=0)\n",
    "\n",
    "            main_loss = self.des_loss(real_output=real_output, fake_output=fake_output)\n",
    "            # The above two lines can be replaced with the following line\n",
    "            \n",
    "            gradient_penalty = self.gradient_penalty_calculator(des=self.des, real_images=real_images, fake_images=fake_images)\n",
    "            gradient_penalty = self.loss_weights[\"gradient_penalty\"] * gradient_penalty\n",
    "            \n",
    "            all_pred = tf.concat([fake_output, real_output], axis=0)\n",
    "            drift_loss = tf.reduce_mean(all_pred**2)\n",
    "            # drift_loss = real_output\n",
    "            drift_loss = self.loss_weights[\"drift\"] * drift_loss\n",
    "\n",
    "            loss = main_loss + gradient_penalty + drift_loss\n",
    "            # loss = tf.reduce_mean(loss)\n",
    "\n",
    "        gradients = total_tape.gradient(loss, self.des.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(gradients, self.des.trainable_variables))\n",
    "        return loss, main_loss, gradient_penalty, drift_loss\n",
    "            \n",
    "    def alpha_update(self):\n",
    "        alphaIncr = self.alpha_incr_rate / self.steps_per_epoch\n",
    "        self.alpha.assign(tf.minimum(self.alpha + alphaIncr, 1.0))\n",
    "\n",
    "    def update_train_params_on_step(self):\n",
    "        self.train_step_counter.assign_add(1)\n",
    "        # self.train_step_counter += 1\n",
    "        # if self.can_update_alpha.read_value():\n",
    "        tf.cond(self.can_update_alpha, lambda: self.alpha_update(), lambda: None)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        return self.tf_train_step(data)\n",
    "        # return self._train_step(data)\n",
    "\n",
    "    def _train_step(self, data):\n",
    "        self.update_train_params_on_step()\n",
    "\n",
    "        real_images = data[\"image\"]\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        for _ in range(self.des_steps):\n",
    "            des_loss, des_core_loss, gradient_penalty, drift_loss = self.train_discriminator(real_images, batch_size)\n",
    "        for _ in range(self.gen_steps):\n",
    "            gen_loss, _ = self.train_generator(batch_size)\n",
    "        \n",
    "        if tf.math.mod(self.train_step_counter, 20) == 0:\n",
    "            self.g_loss_metric.update_state(tf.abs(gen_loss))\n",
    "            self.d_loss_metric.update_state(tf.abs(des_loss))\n",
    "        \n",
    "        return {\n",
    "            \"des_loss\": des_loss,\n",
    "            \"gen_loss\": gen_loss,\n",
    "            \"des_main_loss\": des_core_loss,\n",
    "            \"des_gradient_penalty\": gradient_penalty,\n",
    "            \"des_drift_loss\": drift_loss,\n",
    "        }\n",
    "        \n",
    "    def get_weights(self, *args, **kwargs):\n",
    "        weights = {\n",
    "            \"des\": self.des.get_weights(*args, **kwargs),\n",
    "        }\n",
    "        if self.gen.higher_model != None:\n",
    "            weights[\"gen_lower\"] = self.gen.lower_model.get_weights(*args, **kwargs)\n",
    "            weights[\"gen_higher\"] = self.gen.higher_model.get_weights(*args, **kwargs)\n",
    "        else:\n",
    "            weights[\"gen\"] = self.gen.current_model.get_weights(*args, **kwargs)\n",
    "        \n",
    "    def set_weights(self, weights, *args, **kwargs):\n",
    "        if self.gen.higher_model != None:\n",
    "            self.gen.lower_model.set_weights(weights[\"gen_lower\"], *args, **kwargs)\n",
    "            self.gen.higher_model.set_weights(weights[\"gen_higher\"], *args, **kwargs)\n",
    "        else:\n",
    "            self.gen.current_model.set_weights(weights[\"gen\"], *args, **kwargs)\n",
    "        self.des.set_weights(weights[\"des\"], *args, **kwargs)\n",
    "        \n",
    "    def save_weights(self, path, *args, **kwargs):\n",
    "        # self.gen.save_weights('{path}_gen.weights.h5'.format(path=path), *args, **kwargs)\n",
    "        self.des.save_weights('{path}_des.weights.h5'.format(path=path), *args, **kwargs)\n",
    "    \n",
    "    def load_weights(self, path, *args, **kwargs):\n",
    "        #self.gen.load_weights('{path}_gen.weights.h5'.format(path=path), *args, **kwargs)\n",
    "        self.des.load_weights('{path}_des.weights.h5'.format(path=path), *args, **kwargs)\n",
    "    \n",
    "    def save(self, path, *args, **kwargs):\n",
    "        #self.gen.save('{path}_gen.keras'.format(path=path), *args, **kwargs)\n",
    "        self.des.save('{path}_des.keras'.format(path=path), *args, **kwargs)\n",
    "\n",
    "\n",
    "def trainProGan(config, name=MODEL_NAME, reportToWandb=False) -> ProGAN:\n",
    "    experiment_name = \"{name}_{date}\".format(\n",
    "                name=name, date=datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "            )\n",
    "    log_dir = os.path.join(TRAIN_LOGDIR, experiment_name)\n",
    "    if config['clear_previous_runs'] is True:\n",
    "        os.system(\"rm -rf ./{log_dir}\".format(log_dir=TRAIN_LOGDIR))\n",
    "    # Start a run, tracking hyperparameters\n",
    "    if reportToWandb:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"progan-experiments\",\n",
    "            name=experiment_name,\n",
    "            # track hyperparameters and run metadata with wandb.config\n",
    "            config=config,\n",
    "        )\n",
    "        # [optional] use wandb.config as your config\n",
    "        config = wandb.config\n",
    "        \n",
    "    scaleSizes = sorted(config[\"trainingProfile\"].keys())\n",
    "    print(\"Training at scales: \", scaleSizes)\n",
    "    \n",
    "    if \"start_res\" not in config:\n",
    "        config['start_res'] = scaleSizes[0]\n",
    "    \n",
    "    gan:ProGAN = ProGAN(\n",
    "        name=name,\n",
    "        start_res=config['start_res'],\n",
    "        des_steps=config[\"des_steps\"],\n",
    "        gen_steps=config[\"gen_steps\"],\n",
    "        des_loss=config[\"des_loss\"],\n",
    "        gen_loss=config[\"gen_loss\"],\n",
    "        gradient_penalty_calculator=config['gradient_penalty_calculator'],\n",
    "        gen_config=config[\"gen_config\"],\n",
    "        des_config=config[\"des_config\"],\n",
    "        loss_weights=config[\"loss_weights\"],\n",
    "        initial_alpha=config[\"initial_alpha\"],\n",
    "        should_train_separately=config[\"train_separately\"],\n",
    "    )\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        write_graph=True,\n",
    "        histogram_freq=1,\n",
    "    )\n",
    "    \n",
    "        # Profile from batches 10 to 15\n",
    "    # tb_callback = keras.callbacks.TensorBoard(log_dir=TRAIN_LOGDIR,\n",
    "    #                                             profile_batch='10, 15')\n",
    "    datasetName = config[\"data\"]\n",
    "    data:tf.data.Dataset = tfds.load(datasetName, split=\"train\", shuffle_files=True)\n",
    "\n",
    "    for size in scaleSizes:\n",
    "        clear_output(wait=True)\n",
    "        conf = config[\"trainingProfile\"][size]\n",
    "        batch_size = conf[\"batch_size\"]\n",
    "        steps = len(data) // batch_size\n",
    "        print(\"Training at resolution \", size, \" with \", steps, \" steps per epoch\")\n",
    "        finalData = (\n",
    "            data\n",
    "            # .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "            .batch(128, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .map(\n",
    "                augmenter(size, alpha=None, method=\"area\"), \n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            )\n",
    "            .cache()  # Cache after augmenting to avoid recomputation\n",
    "            .unbatch()\n",
    "            .shuffle(8192)  # Ensure this is adequate for your dataset size\n",
    "            .batch(batch_size, drop_remainder=True)\n",
    "            .repeat()  # Repeats the dataset indefinitely\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "        dataIter = iter(finalData)\n",
    "        sample_real_data = next(dataIter)[\"image\"]\n",
    "        \n",
    "        print(\"Size of real data: \", sample_real_data.shape)\n",
    "\n",
    "        alphaStep = conf[\"alpha_step\"]\n",
    "        alphaDelay = conf[\"alpha_delay\"]\n",
    "        alphaMultiplier = conf[\"alpha_multiplier\"]\n",
    "\n",
    "        gan.compile(\n",
    "            res=size,\n",
    "            steps_per_epoch=steps,\n",
    "            g_optimizer=config[\"g_optimizer_builder\"](),\n",
    "            d_optimizer=config[\"d_optimizer_builder\"](),\n",
    "            alpha_incr_rate=alphaStep,\n",
    "            alpha_delay_epochs=alphaDelay,\n",
    "            alpha_multiplier=alphaMultiplier,\n",
    "        )\n",
    "        prefix = \"res_{s}x{s}_{name}\".format(s=size, name=name)\n",
    "\n",
    "        monitor = ProGANMonitor(\n",
    "            batch_size, sample_real_data, name=prefix, tf_log_dir=log_dir\n",
    "        )\n",
    "        ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODEL_PATH, \"{prefix}.keras\".format(prefix=prefix)),\n",
    "            # save_weights_only=True,\n",
    "            mode='min',\n",
    "            verbose=0,\n",
    "        )\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor=\"gen_loss_metric\",\n",
    "            # min_delta=0,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode=\"min\",\n",
    "            baseline=None,\n",
    "            restore_best_weights=True,\n",
    "            start_from_epoch=6,\n",
    "        )\n",
    "\n",
    "        if conf['epochs'] > 0:\n",
    "            gan.fit(\n",
    "                finalData,\n",
    "                steps_per_epoch=steps,\n",
    "                epochs=conf[\"epochs\"],\n",
    "                callbacks=[monitor, ckpt_cb, tensorboard_callback, early_stopping],\n",
    "            )\n",
    "            \n",
    "        if config['profile']:\n",
    "            with tf.profiler.experimental.Profile(log_dir):\n",
    "                for step in range(10):  # Small number of steps for initial testing\n",
    "                    with tf.profiler.experimental.Trace('train', step_num=step):\n",
    "                        batch = next(dataIter)\n",
    "                        gan.train_step(batch)\n",
    "            print(\"Profiling completed at resolution \", size)\n",
    "\n",
    "    if reportToWandb:\n",
    "        wandb.finish()\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = {\n",
    "  'latent_dim':512,\n",
    "  'conv_normalizer':keras.layers.Lambda,\n",
    "  'normal_normalizer':PixelNorm,\n",
    "  'activation_builder':lambda : layers.LeakyReLU(alpha=0.2),\n",
    "  'upsampling_builder':lambda : layers.UpSampling2D((2, 2), interpolation='nearest'),\n",
    "  'kernel_constraint':None,\n",
    "  'kernel_initializer_builder':lambda : keras.initializers.RandomNormal(mean=0.0, stddev=0.02),\n",
    "  'freeze_oldlayers':False,\n",
    "  'use_sepconv':True,\n",
    "}\n",
    "des_config = {\n",
    "  'conv_normalizer':lambda *args, **kwargs : SpectralNormalization(power_iterations=2, *args, **kwargs),\n",
    "  'normal_normalizer':lambda : layers.GroupNormalization(groups=-1),#lambda: FakeLayer(lambda x: x),\n",
    "  'activation_builder':lambda : layers.LeakyReLU(alpha=0.2),\n",
    "  'kernel_constraint':None, #lambda : keras.constraints.UnitNorm(),\n",
    "  'kernel_initializer_builder':lambda : keras.initializers.RandomNormal(mean=0.0, stddev=0.02),\n",
    "  'freeze_oldlayers':False,\n",
    "  'use_sepconv':True,\n",
    "}\n",
    "\n",
    "trainingConfs = {\n",
    "  'lfw':{\n",
    "    # 4:{\"epochs\":0, \"batch_size\":64, \"alpha_step\":0, \"alpha_delay\":0, \"alpha_multiplier\":1}, \n",
    "    # 8:{\"epochs\":0, \"batch_size\":64, \"alpha_step\":0, \"alpha_delay\":0, \"alpha_multiplier\":1}, \n",
    "    # 16:{\"epochs\":0, \"batch_size\":64, \"alpha_step\":0, \"alpha_delay\":0, \"alpha_multiplier\":0}, \n",
    "    # 32:{\"epochs\":50, \"batch_size\":64, \"alpha_step\":0, \"alpha_delay\":0, \"alpha_multiplier\":0}, \n",
    "    \n",
    "    4:{\"epochs\":15, \"batch_size\":64, \"alpha_step\":0, \"alpha_delay\":0, \"alpha_multiplier\":0}, \n",
    "    8:{\"epochs\":20, \"batch_size\":64, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    16:{\"epochs\":20, \"batch_size\":64, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    32:{\"epochs\":30, \"batch_size\":64, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    64:{\"epochs\":50, \"batch_size\":48, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    128:{\"epochs\":40, \"batch_size\":32, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    256:{\"epochs\":50, \"batch_size\":32, \"alpha_step\":1, \"alpha_delay\":2, \"alpha_multiplier\":1.}\n",
    "  },\n",
    "  'celeb_a':{\n",
    "    # 4:{\"epochs\":15, \"batch_size\":64, \"alpha_step\":0, \"alpha_delay\":0, \"alpha_multiplier\":0}, \n",
    "    # 8:{\"epochs\":35, \"batch_size\":64, \"alpha_step\":2e-4, \"alpha_delay\":2, \"alpha_multiplier\":1.8}, \n",
    "    # 16:{\"epochs\":40, \"batch_size\":64, \"alpha_step\":1e-4, \"alpha_delay\":2, \"alpha_multiplier\":1.7}, \n",
    "    # 32:{\"epochs\":45, \"batch_size\":64, \"alpha_step\":1e-4, \"alpha_delay\":2, \"alpha_multiplier\":1.5}, \n",
    "    # 64:{\"epochs\":50, \"batch_size\":48, \"alpha_step\":1e-4, \"alpha_delay\":2, \"alpha_multiplier\":1.5}, \n",
    "    # 128:{\"epochs\":60, \"batch_size\":32, \"alpha_step\":1e-4, \"alpha_delay\":2, \"alpha_multiplier\":1.3}, \n",
    "    # 256:{\"epochs\":70, \"batch_size\":32, \"alpha_step\":1e-4, \"alpha_delay\":2, \"alpha_multiplier\":1.2}\n",
    "    \n",
    "    4:{\"epochs\":10, \"batch_size\":64, \"alpha_step\":0.3, \"alpha_delay\":0, \"alpha_multiplier\":0}, \n",
    "    8:{\"epochs\":20, \"batch_size\":64, \"alpha_step\":0.5, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    16:{\"epochs\":20, \"batch_size\":64, \"alpha_step\":0.5, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    32:{\"epochs\":20, \"batch_size\":64, \"alpha_step\":0.5, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    64:{\"epochs\":20, \"batch_size\":32, \"alpha_step\":0.5, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    128:{\"epochs\":20, \"batch_size\":32, \"alpha_step\":0.5, \"alpha_delay\":2, \"alpha_multiplier\":1.}, \n",
    "    256:{\"epochs\":20, \"batch_size\":32, \"alpha_step\":0.5, \"alpha_delay\":2, \"alpha_multiplier\":1.}\n",
    "  }\n",
    "}\n",
    "optimizer_builder_des = lambda : keras.optimizers.Adam(learning_rate=1e-4, beta_1 = 0., beta_2 = 0.999)\n",
    "optimizer_builder_gen = lambda : keras.optimizers.Adam(learning_rate=5e-5, beta_1 = 0., beta_2 = 0.999)\n",
    "config = {\n",
    "  \"clear_previous_runs\":False,\n",
    "  'profile':False,\n",
    "  'initial_alpha':0.0,\n",
    "  'des_steps':1, \n",
    "  'gen_steps':1,\n",
    "  'train_separately':True,\n",
    "  'des_loss':descriminator_hinge_loss,\n",
    "  'gen_loss':generator_hinge_loss,\n",
    "  # 'des_loss':descriminator_softplus_loss,\n",
    "  # 'gen_loss':generator_softplus_loss,\n",
    "  # 'des_loss':None,\n",
    "  # 'gen_loss':None,\n",
    "  'gradient_penalty_calculator':r1Penalty,\n",
    "  'g_optimizer_builder':optimizer_builder_gen,\n",
    "  'd_optimizer_builder':optimizer_builder_des,\n",
    "  'data':'celeb_a',\n",
    "  'gen_config':gen_config,\n",
    "  'des_config':des_config,\n",
    "  \"loss_weights\": {\"gradient_penalty\": 10, \"drift\": 0.00},\n",
    "}\n",
    "config['trainingProfile'] = trainingConfs[config['data']]\n",
    "trainProGan(name=MODEL_NAME, config=config, reportToWandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = {\n",
    "  'latent_dim':512,\n",
    "  'conv_normalizer':keras.layers.Lambda,\n",
    "  'normal_normalizer':PixelNorm,\n",
    "  'activation_builder':lambda : layers.LeakyReLU(alpha=0.2),\n",
    "  'upsampling_builder':lambda : layers.UpSampling2D((2, 2), interpolation='nearest'),\n",
    "  'kernel_constraint':None,\n",
    "  'kernel_initializer_builder':lambda : keras.initializers.RandomNormal(mean=0.0, stddev=0.02),\n",
    "  'freeze_oldlayers':False,\n",
    "  'use_sepconv':True,\n",
    "}\n",
    "des_config = {\n",
    "  'conv_normalizer':lambda *args, **kwargs : SpectralNormalization(power_iterations=2, *args, **kwargs),\n",
    "  # 'normal_normalizer':lambda : layers.GroupNormalization(groups=-1),#lambda: FakeLayer(lambda x: x),\n",
    "  'normal_normalizer': lambda: keras.layers.Lambda(lambda x: x),\n",
    "  'activation_builder':lambda : layers.LeakyReLU(alpha=0.2),\n",
    "  'kernel_constraint':None, #lambda : keras.constraints.UnitNorm(),\n",
    "  'kernel_initializer_builder':lambda : keras.initializers.RandomNormal(mean=0.0, stddev=0.02),\n",
    "  'freeze_oldlayers':False,\n",
    "  'use_sepconv':True,\n",
    "}\n",
    "\n",
    "dBuilder = DescriminatorBuilder(**des_config)\n",
    "dBuilder.grow()\n",
    "gBuilder = GeneratorBuilder(**gen_config)\n",
    "gBuilder.grow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dBuilder.grow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = keras.models.clone_model(dBuilder.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dBuilder.alpha.assign_add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dBuilder.model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_images = gBuilder.predict(tf.random.normal([1, 512]))\n",
    "real_images = tf.random.normal([1, 4, 4, 3])\n",
    "\n",
    "fake_output = dBuilder.model(fake_images)\n",
    "real_output = dBuilder.model(real_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(fake_output), tf.reduce_mean(real_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_output, real_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0320d31ea717465588c394eb4d51e7ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f334dc705d443d5b99541e467294220",
      "placeholder": "​",
      "style": "IPY_MODEL_3907d6b9d0944d6aba5bf5d39dd210b1",
      "value": "Dl Completed...:   0%"
     }
    },
    "0587d65448f64931a8b2a7567571a501": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8f22d4ccb3b4296970ecc92dd63d8f1",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8da5b95687334274bf8b419512f733b8",
      "value": 0
     }
    },
    "09991e0fe8234e4e9904ba454690ab6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "0eebfcc871b2429fbb7e2be0c30e5dac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a969db852d99484e9f71f284e357e3a5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_41c40155b5644a12a43797b6150b5938",
      "value": 1
     }
    },
    "1f96e1060b7f4ad1819d9b1ee5832e76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "305738872c794051adbb8f574dda9fae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8bf85e3104c45a68c803f5bb943d767",
       "IPY_MODEL_6e06576433db421ba87acd5a96cc058b",
       "IPY_MODEL_a7d7738f3a69421eb0850dad03c62d62"
      ],
      "layout": "IPY_MODEL_921c132c04a243e9bf3a8ab4aaa0f032"
     }
    },
    "3907d6b9d0944d6aba5bf5d39dd210b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "41c40155b5644a12a43797b6150b5938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4d0955cedfda4f528508280d55f43b00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56ac596d39214e26976bebd9feb73972": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc18c429df5446f2928e301253af66df",
      "placeholder": "​",
      "style": "IPY_MODEL_7c369757827c43c19b2a5e63672a634e",
      "value": " 0/1 [03:20&lt;?, ? url/s]"
     }
    },
    "6be34b510cba4fbfad8512bb1f82ac8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c2bc8cd41a3471ba669e1909e137c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bad4648f3d844c08be69d44f1bd2064e",
      "placeholder": "​",
      "style": "IPY_MODEL_8406068892494f1485665dca6b884c40",
      "value": "Dl Size...:  54%"
     }
    },
    "6e06576433db421ba87acd5a96cc058b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09991e0fe8234e4e9904ba454690ab6b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7d3aef57552c47f4978bc0abf0bef07c",
      "value": 0
     }
    },
    "6fa6fcdd41e14baf8e5cea8617d0a103": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7884836108d74585b2d6c43499be4b00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0320d31ea717465588c394eb4d51e7ef",
       "IPY_MODEL_0587d65448f64931a8b2a7567571a501",
       "IPY_MODEL_56ac596d39214e26976bebd9feb73972"
      ],
      "layout": "IPY_MODEL_c2771b2e01cc42ae9c72691e40261429"
     }
    },
    "7c369757827c43c19b2a5e63672a634e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d3aef57552c47f4978bc0abf0bef07c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8406068892494f1485665dca6b884c40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8428dd5a5032425489e6181f9849eec2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d0955cedfda4f528508280d55f43b00",
      "placeholder": "​",
      "style": "IPY_MODEL_f4c44c94d37147478e5e059a0f3efb35",
      "value": " 92/172 [03:20&lt;02:36,  1.96s/ MiB]"
     }
    },
    "8b7461e8c5c84bd9b3b0530436cc30b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8da5b95687334274bf8b419512f733b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "921c132c04a243e9bf3a8ab4aaa0f032": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f334dc705d443d5b99541e467294220": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7d7738f3a69421eb0850dad03c62d62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fa6fcdd41e14baf8e5cea8617d0a103",
      "placeholder": "​",
      "style": "IPY_MODEL_6be34b510cba4fbfad8512bb1f82ac8c",
      "value": " 0/0 [03:20&lt;?, ? file/s]"
     }
    },
    "a8f22d4ccb3b4296970ecc92dd63d8f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a969db852d99484e9f71f284e357e3a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "b0600d22ed5948a28bcb5b7ce2c26dcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6c2bc8cd41a3471ba669e1909e137c33",
       "IPY_MODEL_0eebfcc871b2429fbb7e2be0c30e5dac",
       "IPY_MODEL_8428dd5a5032425489e6181f9849eec2"
      ],
      "layout": "IPY_MODEL_b22cb14f030f4991816426452c582cd6"
     }
    },
    "b22cb14f030f4991816426452c582cd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bad4648f3d844c08be69d44f1bd2064e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2771b2e01cc42ae9c72691e40261429": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8bf85e3104c45a68c803f5bb943d767": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f96e1060b7f4ad1819d9b1ee5832e76",
      "placeholder": "​",
      "style": "IPY_MODEL_8b7461e8c5c84bd9b3b0530436cc30b5",
      "value": "Extraction completed...: "
     }
    },
    "f4c44c94d37147478e5e059a0f3efb35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc18c429df5446f2928e301253af66df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
