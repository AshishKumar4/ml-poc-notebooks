{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJVKL-Y3a-Ul",
        "outputId": "68801559-67d2-4f95-8e94-ae4c3ae94a2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "2024-05-21 17:59:57.180185: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-05-21 17:59:57.204945: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-21 17:59:57.780588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "import cupy as np\n",
        "import math\n",
        "import sklearn # Only for downloading MNIST Dataset and Accuracy Metrics\n",
        "import sklearn.metrics\n",
        "from keras.utils import to_categorical  # Only for categorical one hot encoding\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo5ppWNtb1lI",
        "outputId": "576c34d1-07e0-428d-b456-09ad5e603f05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((7, 2), (7, 1))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xor_x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 0], [0, 0], [1, 1]])\n",
        "xor_y_train = np.array([[0], [1], [1], [0], [1], [0], [0]])\n",
        "xor_x_train.shape, xor_y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ0xUhqjby0F",
        "outputId": "7c67e5d3-d008-46e3-adb6-e1e25b0734ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "cy_train = np.array(to_categorical(y_train))\n",
        "cy_test = np.array(to_categorical(y_test))\n",
        "\n",
        "cx_train, cx_test = np.array(x_train.reshape(-1, 784)/255.), np.array(x_test.reshape(-1, 784)/255.)\n",
        "cx_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import NeuroLab as nl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmj7tTpeSINf"
      },
      "source": [
        "# Neural Network with SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MYhVRsYqW-mi"
      },
      "outputs": [],
      "source": [
        "class SGD_Optimizer(nl.Optimizer):\n",
        "    def __init__(self, model, learning_rate, loss, gamma=1, delta=2):\n",
        "        super().__init__(model, loss)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.delta = delta\n",
        "\n",
        "    def train_step(self, x_batch, y_batch):\n",
        "        predictions = self.model.forward(x_batch)\n",
        "        loss_value = self.loss.forward(predictions, y_batch)\n",
        "        grad_loss = self.loss.backward(predictions, y_batch)\n",
        "\n",
        "        for layer in reversed(self.model.get_layers()):\n",
        "            # Back propogate the loss to the layer\n",
        "            grad_loss, deltas = layer.backward(grad_loss)\n",
        "            if deltas != None:\n",
        "                # Simple Stochastic Gradient Decent\n",
        "                delta_weights, delta_biases = deltas\n",
        "                deltas = (delta_weights * self.learning_rate, delta_biases * self.learning_rate)\n",
        "            # Update the weights\n",
        "            layer.update_parameters(deltas)\n",
        "\n",
        "        return loss_value\n",
        "\n",
        "    def on_epoch_end(self, epoch):\n",
        "        # Decay Weight\n",
        "        self.learning_rate *= self.gamma\n",
        "        self.delta *= self.gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lUaEoghOuoRE",
        "outputId": "03f961ca-ebbc-4866-a29f-c20f2ee3e553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.002965000749972247, test_acc: 0.9343\n",
            "Epoch 1, Loss: 0.00454360453606925, test_acc: 0.9519\n",
            "Epoch 2, Loss: 0.005682691445204189, test_acc: 0.9587\n",
            "Epoch 3, Loss: 0.0020864411427078417, test_acc: 0.9635\n",
            "Epoch 4, Loss: 0.0030289556967028116, test_acc: 0.9679\n",
            "Epoch 5, Loss: 0.0026872328693654055, test_acc: 0.9692\n",
            "Epoch 6, Loss: 0.0034305042118758034, test_acc: 0.9699\n",
            "Epoch 7, Loss: 0.001227873028913653, test_acc: 0.9723\n",
            "Epoch 8, Loss: 0.0022649712001616197, test_acc: 0.9725\n",
            "Epoch 9, Loss: 0.0010420624870716318, test_acc: 0.973\n",
            "Accuracy: 0.973\n",
            "Confusion Matrix: [[ 967    0    2    1    0    1    6    1    1    1]\n",
            " [   0 1122    4    1    0    1    3    1    3    0]\n",
            " [   3    1 1011    2    2    0    1    7    5    0]\n",
            " [   0    0    9  980    0    8    0    4    4    5]\n",
            " [   1    0    5    0  948    0    5    1    2   20]\n",
            " [   4    1    1    6    1  864    7    0    5    3]\n",
            " [   5    3    0    1    2    5  940    0    2    0]\n",
            " [   2    9   16    2    1    0    0  991    1    6]\n",
            " [   4    1    5    3    3    3    4    6  941    4]\n",
            " [   4    5    1   11   13    0    1    5    3  966]]\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       980\n",
            "           1       0.98      0.99      0.99      1135\n",
            "           2       0.96      0.98      0.97      1032\n",
            "           3       0.97      0.97      0.97      1010\n",
            "           4       0.98      0.97      0.97       982\n",
            "           5       0.98      0.97      0.97       892\n",
            "           6       0.97      0.98      0.98       958\n",
            "           7       0.98      0.96      0.97      1028\n",
            "           8       0.97      0.97      0.97       974\n",
            "           9       0.96      0.96      0.96      1009\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(4)\n",
        "# MNIST\n",
        "layers = [\n",
        "    nl.Dense(784, 128),\n",
        "    nl.ReLU(),\n",
        "    nl.Dense(128, 10),\n",
        "    # Softmax(),\n",
        "    nl.Sigmoid(),\n",
        "]\n",
        "\n",
        "net = nl.NeuralNet(layers)\n",
        "\n",
        "optimizer = SGD_Optimizer(net, learning_rate = 0.01, loss=nl.MeanSquaredError(), delta=0.95, gamma=1)\n",
        "optimizer.fit((cx_train, cy_train), (cx_test, cy_test), epochs=10, batch_size=100, verbose=True)\n",
        "# Infer on cx_test, cy_test\n",
        "predictions = net.predict(cx_test)\n",
        "preds = np.array(nl.antiCategorical(predictions)).get()\n",
        "expected = np.array(nl.antiCategorical(cy_test)).get()\n",
        "\n",
        "print(f'Accuracy: {sklearn.metrics.accuracy_score(expected, preds)}')\n",
        "print(f'Confusion Matrix: {sklearn.metrics.confusion_matrix(expected, preds)}')\n",
        "print(f'Classification Report: {sklearn.metrics.classification_report(expected, preds)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pARZuRAtDgY0"
      },
      "source": [
        "# Bayesian Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1WhGL1JTfs8"
      },
      "outputs": [],
      "source": [
        "class BayesianDense(Layer):\n",
        "    def __init__(self, n_input, n_output, prior_std=1.0, posterior_std=0.1):\n",
        "        self.n_input = n_input\n",
        "        self.n_output = n_output\n",
        "        self.prior_std = prior_std\n",
        "        self.posterior_std = posterior_std\n",
        "\n",
        "        self.W_mu = np.random.normal(0, scale=(1/float(math.sqrt(n_input))), size=(n_input, n_output)).astype(np.float64)\n",
        "        self.b_mu = np.zeros((1, n_output)).astype(np.float64)\n",
        "\n",
        "        self.W_sigma = np.full((n_input, n_output), np.log(self.posterior_std)).astype(np.float64)\n",
        "        self.b_sigma = np.full((1, n_output), np.log(self.posterior_std)).astype(np.float64)\n",
        "        print(\"W_mu\", np.mean(self.W_mu),\n",
        "              \"b_mu\", np.mean(self.b_mu),\n",
        "              \"W_sigma\", np.mean(self.W_sigma), self.W_sigma.shape,\n",
        "              \"b_sigma\", np.mean(self.b_sigma), self.b_sigma.shape)\n",
        "\n",
        "    def sample_weights(self):\n",
        "        weights_std = np.exp(0.5 * self.W_sigma)\n",
        "        self.W = self.W_mu + weights_std * np.random.normal(size=(self.n_input, self.n_output))\n",
        "        biases_std = np.exp(0.5 * self.b_sigma)\n",
        "        self.b = self.b_mu + biases_std * np.random.normal(size=(1, self.n_output))\n",
        "        # print(\"w\", np.mean(self.W), np.mean(weights_std), \"b\", np.mean(self.b), np.mean(biases_std))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.sample_weights()\n",
        "        self.output = np.dot(inputs, self.W) + self.b\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_output, learning_rate):\n",
        "        grad_input = np.dot(grad_output, self.W.T)\n",
        "\n",
        "        grad_W = np.dot(self.inputs.T, grad_output)\n",
        "        grad_b = np.mean(grad_output, axis=0, keepdims=True)\n",
        "\n",
        "        grad_W_mu = grad_W / grad_output.shape[0]\n",
        "        grad_b_mu = grad_b / grad_output.shape[0]\n",
        "\n",
        "        grad_W_sigma = ((grad_W ** 2) - 1) / (2 * grad_output.shape[0])\n",
        "        grad_b_sigma = ((grad_b_mu ** 2) - 1) / (2 * grad_output.shape[0])\n",
        "\n",
        "        self.W_mu -= learning_rate * grad_W_mu\n",
        "        self.b_mu -= learning_rate * grad_b_mu\n",
        "\n",
        "        self.W_sigma -= learning_rate * grad_W_sigma\n",
        "        self.b_sigma -= learning_rate * grad_b_sigma\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjRFdyUOThmr",
        "outputId": "2295f52d-1425-46a2-f374-fa04fe11c4e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W_mu -0.0001789835031751473 b_mu 0.0 W_sigma -2.3025850929940455 (784, 100) b_sigma -2.3025850929940455 (1, 100)\n",
            "W_mu 0.0011180262676438772 b_mu 0.0 W_sigma -2.3025850929940455 (100, 50) b_sigma -2.3025850929940455 (1, 50)\n",
            "W_mu 0.0023444765023452477 b_mu 0.0 W_sigma -2.302585092994045 (50, 10) b_sigma -2.3025850929940455 (1, 10)\n",
            "Epoch 0, Batch 0, Loss: 0.07363589103305021\n",
            "Epoch 1, Batch 0, Loss: 0.03985098104954909\n",
            "Epoch 2, Batch 0, Loss: 0.02593534829055494\n",
            "Epoch 3, Batch 0, Loss: 0.02348652026844813\n",
            "Epoch 4, Batch 0, Loss: 0.019979116766026435\n",
            "Epoch 5, Batch 0, Loss: 0.01774217933964233\n",
            "Epoch 6, Batch 0, Loss: 0.017500500468994637\n",
            "Epoch 7, Batch 0, Loss: 0.01771578663478224\n",
            "Epoch 8, Batch 0, Loss: 0.01647555074216599\n",
            "Epoch 9, Batch 0, Loss: 0.013828134710165842\n",
            "Accuracy: 0.7548\n",
            "Confusion Matrix: [[ 866    0   10    1    1   11    3    2   13   73]\n",
            " [   0 1078    2    4    0    0    4    0   44    3]\n",
            " [  25   15  810   20    9    5   20   13   76   39]\n",
            " [  19    3   36  587    0  197    5   19  126   18]\n",
            " [   0    0    3    0  330    1   18    0   42  588]\n",
            " [  42    2   25   35   11  540   15    7  152   63]\n",
            " [  16    2   33    0   27   26  818    0   15   21]\n",
            " [   8   26   16    0    3    1    0  786   20  168]\n",
            " [   2   11   13   19   13   37   13    5  811   50]\n",
            " [   6    1    3    3   10   15    0   18   31  922]]\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88       980\n",
            "           1       0.95      0.95      0.95      1135\n",
            "           2       0.85      0.78      0.82      1032\n",
            "           3       0.88      0.58      0.70      1010\n",
            "           4       0.82      0.34      0.48       982\n",
            "           5       0.65      0.61      0.63       892\n",
            "           6       0.91      0.85      0.88       958\n",
            "           7       0.92      0.76      0.84      1028\n",
            "           8       0.61      0.83      0.70       974\n",
            "           9       0.47      0.91      0.62      1009\n",
            "\n",
            "    accuracy                           0.75     10000\n",
            "   macro avg       0.79      0.75      0.75     10000\n",
            "weighted avg       0.80      0.75      0.75     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(4)\n",
        "# MNIST\n",
        "layers = [\n",
        "    BayesianDense(784, 100),\n",
        "    LeakyReLU(),\n",
        "    BayesianDense(100, 50),\n",
        "    LeakyReLU(),\n",
        "    BayesianDense(50, 10),\n",
        "    Softmax(),\n",
        "]\n",
        "\n",
        "net = NeuralNet(layers, learning_rate = 0.01, loss=MeanSquaredError(), delta=0.95, gamma=1)\n",
        "\n",
        "net.fit(cx_train, cy_train, epochs=10, batch_size=100, verbose=True)\n",
        "# Infer on cx_test, cy_test\n",
        "predictions = net.predict(cx_test, 20)\n",
        "preds = np.array(antiCategorical(predictions)).get()\n",
        "expected = np.array(antiCategorical(cy_test)).get()\n",
        "\n",
        "\n",
        "print(f'Accuracy: {sklearn.metrics.accuracy_score(expected, preds)}')\n",
        "print(f'Confusion Matrix: {sklearn.metrics.confusion_matrix(expected, preds)}')\n",
        "print(f'Classification Report: {sklearn.metrics.classification_report(expected, preds)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53HGMJspYc4G",
        "outputId": "df2bf8b0-8523-4122-842f-39d7912adca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6333\n",
            "Confusion Matrix: [[747   0  19   1   0  16  84   1 111   1]\n",
            " [  0 982  22   7   0   0   3  27  94   0]\n",
            " [  4  54 706   5   6   8  41   1 207   0]\n",
            " [  9  37  40 685   2   9   8   2 217   1]\n",
            " [  4  17   6 122 518  34  17   4 243  17]\n",
            " [ 23  31  32  46   6 263  20   4 461   6]\n",
            " [  9   2  15   8   3   6 814   0 101   0]\n",
            " [  2   8  62  19   7  25   5 672 203  25]\n",
            " [  4  11  32   0   2   1  13   1 910   0]\n",
            " [  2   4  21 361   5   5   3 185 387  36]]\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.76      0.84       980\n",
            "           1       0.86      0.87      0.86      1135\n",
            "           2       0.74      0.68      0.71      1032\n",
            "           3       0.55      0.68      0.61      1010\n",
            "           4       0.94      0.53      0.68       982\n",
            "           5       0.72      0.29      0.42       892\n",
            "           6       0.81      0.85      0.83       958\n",
            "           7       0.75      0.65      0.70      1028\n",
            "           8       0.31      0.93      0.47       974\n",
            "           9       0.42      0.04      0.07      1009\n",
            "\n",
            "    accuracy                           0.63     10000\n",
            "   macro avg       0.70      0.63      0.62     10000\n",
            "weighted avg       0.70      0.63      0.62     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions = net.predict(cx_test, 100)\n",
        "preds = np.array(antiCategorical(predictions)).get()\n",
        "expected = np.array(antiCategorical(cy_test)).get()\n",
        "\n",
        "print(f'Accuracy: {sklearn.metrics.accuracy_score(expected, preds)}')\n",
        "print(f'Confusion Matrix: {sklearn.metrics.confusion_matrix(expected, preds)}')\n",
        "print(f'Classification Report: {sklearn.metrics.classification_report(expected, preds)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRbfd8TrTdOK"
      },
      "source": [
        "# Simulated Anealing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OFwMWXcJy7a-"
      },
      "outputs": [],
      "source": [
        "class SimulatedAnnealingOptimizer(nl.Optimizer):\n",
        "    def __init__(self, model, learning_rate, loss):\n",
        "        super().__init__(model, loss)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.best_loss = float(\"inf\")\n",
        "\n",
        "    def forward_with_perturbations(self, x_train, stddev):\n",
        "        perturbations = []\n",
        "        inputs = x_train\n",
        "        for layer in self.model.get_layers():\n",
        "            if isinstance(layer, nl.SimulatedAnnealingLayer):\n",
        "                perturbation = layer.perturb_parameters(stddev)\n",
        "                perturbations.append(perturbation)\n",
        "                inputs = layer.forward_with_perturbations(inputs, perturbation)\n",
        "            else:\n",
        "                inputs = layer.forward(inputs)\n",
        "        return inputs, perturbations\n",
        "\n",
        "    def update_parameters(self, perturbations):\n",
        "      i = 0\n",
        "      for layer in self.model.get_layers():\n",
        "            if isinstance(layer, nl.SimulatedAnnealingLayer):\n",
        "                layer.update_parameters(perturbations[i])\n",
        "                i += 1\n",
        "\n",
        "    def cooling_schedule(self, old_loss, new_loss, current_temp, current_step_size, cooling_rate, step_decay_rate=0.9999):\n",
        "        deltaLoss = new_loss - old_loss\n",
        "        deltaLossScale = deltaLoss / new_loss\n",
        "\n",
        "        if abs(deltaLossScale) > min(1e-4, current_temp):\n",
        "            scalefactor = min(1, current_temp/new_loss)\n",
        "            elastic_rate = 1.0 - scalefactor\n",
        "            elastic_rate = (elastic_rate + scalefactor) / 2\n",
        "            rate = max(cooling_rate, elastic_rate)\n",
        "            new_temp = current_temp * rate\n",
        "            # new_temp = current_temp * cooling_rate\n",
        "            new_step_size = current_step_size * step_decay_rate\n",
        "        else:\n",
        "            new_temp = current_temp / cooling_rate\n",
        "            new_step_size = self.learning_rate\n",
        "            self.learning_rate *= step_decay_rate\n",
        "\n",
        "        return new_temp, new_step_size\n",
        "\n",
        "    def train_step(self, x_batch, y_batch):\n",
        "        for j in range(self.sample_per_batch):\n",
        "            predictions, perturbations = self.forward_with_perturbations(x_batch, self.step_size)\n",
        "            loss = self.loss.forward(predictions, y_batch)\n",
        "            delta_E = (loss - self.best_loss)\n",
        "            energy_cost = np.exp(-delta_E / self.current_temp)\n",
        "            acceptance_rate = energy_cost if delta_E > 0 else  1.0\n",
        "            if delta_E < 0 or np.random.rand() <= acceptance_rate:\n",
        "                # Accept proposal\n",
        "                self.update_parameters(perturbations)\n",
        "                self.best_loss = loss\n",
        "                # print(f'Accepted proposal due to temperature {loss} exp: {acceptance_rate}, deltaE {delta_E}')\n",
        "        return self.best_loss\n",
        "\n",
        "    def on_epoch_start(self, epoch):\n",
        "        self.old_loss = self.best_loss\n",
        "\n",
        "    def on_epoch_end(self, epoch):\n",
        "        self.current_temp, self.step_size = self.cooling_schedule(self.old_loss, self.best_loss, self.current_temp, self.step_size, self.cooling_rate, 0.9999)\n",
        "\n",
        "    def on_reporting(self, epoch, loss_value, acc):\n",
        "        print(f'Epoch {epoch}, BestLoss: {loss_value}, Temperature {self.current_temp}, step_size {self.step_size}, test_acc: {acc}')\n",
        "\n",
        "    def on_train_start(self, sample_per_batch=1, initial_temp=1.0, cooling_rate=0.99):\n",
        "        self.current_temp = initial_temp\n",
        "        # self.best_loss = float(\"inf\")\n",
        "        self.step_size = self.learning_rate\n",
        "        self.sample_per_batch = sample_per_batch\n",
        "        self.cooling_rate = cooling_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mv5qG3HYd7vO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, BestLoss: 0.12919434167596391, Temperature 1.0, step_size 0.1, test_acc: 1.0\n",
            "Epoch 1, BestLoss: 0.12596324649147103, Temperature 0.95, step_size 0.09999000000000001, test_acc: 1.0\n",
            "Epoch 2, BestLoss: 0.13656031225867155, Temperature 0.9025, step_size 0.09998000100000001, test_acc: 1.0\n",
            "Epoch 3, BestLoss: 0.13864004638512728, Temperature 0.8573749999999999, step_size 0.09997000299990001, test_acc: 1.0\n",
            "Epoch 4, BestLoss: 0.1501615658207422, Temperature 0.8145062499999999, step_size 0.09996000599960002, test_acc: 1.0\n",
            "Epoch 5, BestLoss: 0.13138389377193116, Temperature 0.7737809374999999, step_size 0.09995000999900006, test_acc: 1.0\n",
            "Epoch 6, BestLoss: 0.12838641533258108, Temperature 0.7350918906249998, step_size 0.09994001499800016, test_acc: 1.0\n",
            "Epoch 7, BestLoss: 0.13132383173879686, Temperature 0.6983372960937497, step_size 0.09993002099650036, test_acc: 1.0\n",
            "Epoch 8, BestLoss: 0.16953789963699095, Temperature 0.6634204312890623, step_size 0.09992002799440071, test_acc: 1.0\n",
            "Epoch 9, BestLoss: 0.1624200804191472, Temperature 0.6302494097246091, step_size 0.09991003599160127, test_acc: 1.0\n",
            "Epoch 10, BestLoss: 0.15317042796389152, Temperature 0.5987369392383786, step_size 0.0999000449880021, test_acc: 1.0\n",
            "Epoch 11, BestLoss: 0.15854572984477824, Temperature 0.5688000922764596, step_size 0.0998900549835033, test_acc: 1.0\n",
            "Epoch 12, BestLoss: 0.17674267400688226, Temperature 0.5403600876626365, step_size 0.09988006597800494, test_acc: 1.0\n",
            "Epoch 13, BestLoss: 0.17973997190966925, Temperature 0.5133420832795047, step_size 0.09987007797140715, test_acc: 1.0\n",
            "Epoch 14, BestLoss: 0.19112489273809782, Temperature 0.48767497911552943, step_size 0.09986009096361001, test_acc: 1.0\n",
            "Epoch 15, BestLoss: 0.19426764350633477, Temperature 0.46329123015975293, step_size 0.09985010495451364, test_acc: 1.0\n",
            "Epoch 16, BestLoss: 0.2010285155540594, Temperature 0.44012666865176525, step_size 0.09984011994401819, test_acc: 1.0\n",
            "Epoch 17, BestLoss: 0.20685430999580734, Temperature 0.41812033521917696, step_size 0.09983013593202379, test_acc: 1.0\n",
            "Epoch 18, BestLoss: 0.19374007419555203, Temperature 0.3972143184582181, step_size 0.09982015291843059, test_acc: 1.0\n",
            "Epoch 19, BestLoss: 0.18517413917705006, Temperature 0.37735360253530714, step_size 0.09981017090313875, test_acc: 1.0\n",
            "Epoch 20, BestLoss: 0.17697576602397738, Temperature 0.35848592240854177, step_size 0.09980018988604844, test_acc: 1.0\n",
            "Epoch 21, BestLoss: 0.14128138035733454, Temperature 0.34056162628811465, step_size 0.09979020986705983, test_acc: 1.0\n",
            "Epoch 22, BestLoss: 0.17973424399166144, Temperature 0.3235335449737089, step_size 0.09978023084607313, test_acc: 1.0\n",
            "Epoch 23, BestLoss: 0.19665959904855415, Temperature 0.30735686772502346, step_size 0.09977025282298851, test_acc: 1.0\n",
            "Epoch 24, BestLoss: 0.15949386643749763, Temperature 0.2919890243387723, step_size 0.09976027579770622, test_acc: 1.0\n",
            "Epoch 25, BestLoss: 0.121602401566218, Temperature 0.27738957312183365, step_size 0.09975029977012645, test_acc: 1.0\n",
            "Epoch 26, BestLoss: 0.15280041560830826, Temperature 0.263520094465742, step_size 0.09974032474014943, test_acc: 1.0\n",
            "Epoch 27, BestLoss: 0.1578691495121737, Temperature 0.25034408974245487, step_size 0.09973035070767541, test_acc: 1.0\n",
            "Epoch 28, BestLoss: 0.16975408320309907, Temperature 0.2378268852553321, step_size 0.09972037767260465, test_acc: 1.0\n",
            "Epoch 29, BestLoss: 0.1985364277982817, Temperature 0.2259355409925655, step_size 0.09971040563483739, test_acc: 1.0\n",
            "Epoch 30, BestLoss: 0.19895470635377407, Temperature 0.2146387639429372, step_size 0.09970043459427391, test_acc: 1.0\n",
            "Epoch 31, BestLoss: 0.20029746638929927, Temperature 0.20390682574579033, step_size 0.09969046455081448, test_acc: 1.0\n",
            "Epoch 32, BestLoss: 0.18710486239654628, Temperature 0.1937114844585008, step_size 0.09968049550435941, test_acc: 1.0\n",
            "Epoch 33, BestLoss: 0.1766554289628064, Temperature 0.18402591023557577, step_size 0.09967052745480898, test_acc: 1.0\n",
            "Epoch 34, BestLoss: 0.18057543914378477, Temperature 0.17482461472379698, step_size 0.0996605604020635, test_acc: 1.0\n",
            "Epoch 35, BestLoss: 0.18182321572864144, Temperature 0.16608338398760714, step_size 0.0996505943460233, test_acc: 1.0\n",
            "Epoch 36, BestLoss: 0.18696130073287268, Temperature 0.15777921478822676, step_size 0.0996406292865887, test_acc: 1.0\n",
            "Epoch 37, BestLoss: 0.20869209094403418, Temperature 0.14989025404881542, step_size 0.09963066522366004, test_acc: 1.0\n",
            "Epoch 38, BestLoss: 0.21222470634609764, Temperature 0.14239574134637464, step_size 0.09962070215713768, test_acc: 1.0\n",
            "Epoch 39, BestLoss: 0.23403721927017834, Temperature 0.1352759542790559, step_size 0.09961074008692197, test_acc: 1.0\n",
            "Epoch 40, BestLoss: 0.21650515743164447, Temperature 0.1285121565651031, step_size 0.09960077901291328, test_acc: 1.0\n",
            "Epoch 41, BestLoss: 0.17617589278181997, Temperature 0.12208654873684793, step_size 0.099590818935012, test_acc: 1.0\n",
            "Epoch 42, BestLoss: 0.14634187176406857, Temperature 0.11598222130000553, step_size 0.0995808598531185, test_acc: 1.0\n",
            "Epoch 43, BestLoss: 0.1511938156577635, Temperature 0.11018311023500525, step_size 0.0995709017671332, test_acc: 1.0\n",
            "Epoch 44, BestLoss: 0.14575274215910666, Temperature 0.10467395472325498, step_size 0.09956094467695648, test_acc: 1.0\n",
            "Epoch 45, BestLoss: 0.12607256570147343, Temperature 0.09944025698709223, step_size 0.09955098858248879, test_acc: 1.0\n",
            "Epoch 46, BestLoss: 0.12606042284687305, Temperature 0.09446824413773762, step_size 0.09954103348363054, test_acc: 1.0\n",
            "Epoch 47, BestLoss: 0.12748591306951237, Temperature 0.09944025698709223, step_size 0.1, test_acc: 1.0\n",
            "Epoch 48, BestLoss: 0.14319579603020394, Temperature 0.09446824413773762, step_size 0.09999000000000001, test_acc: 1.0\n",
            "Epoch 49, BestLoss: 0.13806350213928154, Temperature 0.08974483193085074, step_size 0.09998000100000001, test_acc: 1.0\n",
            "Epoch 50, BestLoss: 0.15270271153574339, Temperature 0.0852575903343082, step_size 0.09997000299990001, test_acc: 1.0\n",
            "Epoch 51, BestLoss: 0.1276061931797294, Temperature 0.08099471081759278, step_size 0.09996000599960002, test_acc: 1.0\n",
            "Epoch 52, BestLoss: 0.13688822111036325, Temperature 0.07694497527671314, step_size 0.09995000999900006, test_acc: 1.0\n",
            "Epoch 53, BestLoss: 0.12690349844890259, Temperature 0.07309772651287748, step_size 0.09994001499800016, test_acc: 1.0\n",
            "Epoch 54, BestLoss: 0.12547704450946573, Temperature 0.0694428401872336, step_size 0.09993002099650036, test_acc: 1.0\n",
            "Epoch 55, BestLoss: 0.142584852971966, Temperature 0.0659706981778719, step_size 0.09992002799440071, test_acc: 1.0\n",
            "Epoch 56, BestLoss: 0.1446102286804481, Temperature 0.0626721632689783, step_size 0.09991003599160127, test_acc: 1.0\n",
            "Epoch 57, BestLoss: 0.17449158432702935, Temperature 0.059538555105529384, step_size 0.0999000449880021, test_acc: 1.0\n",
            "Epoch 58, BestLoss: 0.172080691686074, Temperature 0.05656162735025291, step_size 0.0998900549835033, test_acc: 1.0\n",
            "Epoch 59, BestLoss: 0.14767796566397834, Temperature 0.053733545982740265, step_size 0.09988006597800494, test_acc: 1.0\n",
            "Epoch 60, BestLoss: 0.1623800641622469, Temperature 0.05104686868360325, step_size 0.09987007797140715, test_acc: 1.0\n",
            "Epoch 61, BestLoss: 0.16847035639044325, Temperature 0.04849452524942309, step_size 0.09986009096361001, test_acc: 1.0\n",
            "Epoch 62, BestLoss: 0.15840585393216516, Temperature 0.04606979898695193, step_size 0.09985010495451364, test_acc: 1.0\n",
            "Epoch 63, BestLoss: 0.1857194537514457, Temperature 0.04376630903760433, step_size 0.09984011994401819, test_acc: 1.0\n",
            "Epoch 64, BestLoss: 0.19943778036626578, Temperature 0.041577993585724116, step_size 0.09983013593202379, test_acc: 1.0\n",
            "Epoch 65, BestLoss: 0.1792276382248646, Temperature 0.03949909390643791, step_size 0.09982015291843059, test_acc: 1.0\n",
            "Epoch 66, BestLoss: 0.18147132352540088, Temperature 0.03752413921111601, step_size 0.09981017090313875, test_acc: 1.0\n",
            "Epoch 67, BestLoss: 0.17299808449091958, Temperature 0.03564793225056021, step_size 0.09980018988604844, test_acc: 1.0\n",
            "Epoch 68, BestLoss: 0.1578837150156033, Temperature 0.0338655356380322, step_size 0.09979020986705983, test_acc: 1.0\n",
            "Epoch 69, BestLoss: 0.1592564375440812, Temperature 0.032172258856130585, step_size 0.09978023084607313, test_acc: 1.0\n",
            "Epoch 70, BestLoss: 0.1438633909898432, Temperature 0.030563645913324056, step_size 0.09977025282298851, test_acc: 1.0\n",
            "Epoch 71, BestLoss: 0.17427456639574185, Temperature 0.029035463617657853, step_size 0.09976027579770622, test_acc: 1.0\n",
            "Epoch 72, BestLoss: 0.18522707312250128, Temperature 0.027583690436774957, step_size 0.09975029977012645, test_acc: 1.0\n",
            "Epoch 73, BestLoss: 0.1874642277786393, Temperature 0.02620450591493621, step_size 0.09974032474014943, test_acc: 1.0\n",
            "Epoch 74, BestLoss: 0.19799482043599406, Temperature 0.0248942806191894, step_size 0.09973035070767541, test_acc: 1.0\n",
            "Epoch 75, BestLoss: 0.19553001000699263, Temperature 0.023649566588229927, step_size 0.09972037767260465, test_acc: 1.0\n",
            "Epoch 76, BestLoss: 0.20129055734898138, Temperature 0.022467088258818428, step_size 0.09971040563483739, test_acc: 1.0\n",
            "Epoch 77, BestLoss: 0.18821242365372212, Temperature 0.021343733845877507, step_size 0.09970043459427391, test_acc: 1.0\n",
            "Epoch 78, BestLoss: 0.19050580181554416, Temperature 0.02027654715358363, step_size 0.09969046455081448, test_acc: 1.0\n",
            "Epoch 79, BestLoss: 0.19930602297924566, Temperature 0.019262719795904448, step_size 0.09968049550435941, test_acc: 1.0\n",
            "Epoch 80, BestLoss: 0.20414415092733967, Temperature 0.018299583806109226, step_size 0.09967052745480898, test_acc: 1.0\n",
            "Epoch 81, BestLoss: 0.20573047616968473, Temperature 0.017384604615803764, step_size 0.0996605604020635, test_acc: 1.0\n",
            "Epoch 82, BestLoss: 0.20021339086636478, Temperature 0.016515374385013576, step_size 0.0996505943460233, test_acc: 1.0\n",
            "Epoch 83, BestLoss: 0.20265154391358425, Temperature 0.015689605665762895, step_size 0.0996406292865887, test_acc: 1.0\n",
            "Epoch 84, BestLoss: 0.20285463784554558, Temperature 0.01490512538247475, step_size 0.09963066522366004, test_acc: 1.0\n",
            "Epoch 85, BestLoss: 0.19720293712971085, Temperature 0.014159869113351011, step_size 0.09962070215713768, test_acc: 1.0\n",
            "Epoch 86, BestLoss: 0.19659896624057088, Temperature 0.01345187565768346, step_size 0.09961074008692197, test_acc: 1.0\n",
            "Epoch 87, BestLoss: 0.19823632077140627, Temperature 0.012779281874799287, step_size 0.09960077901291328, test_acc: 1.0\n",
            "Epoch 88, BestLoss: 0.19085301128389354, Temperature 0.012140317781059323, step_size 0.099590818935012, test_acc: 1.0\n",
            "Epoch 89, BestLoss: 0.18561150104019952, Temperature 0.011533301892006355, step_size 0.0995808598531185, test_acc: 1.0\n",
            "Epoch 90, BestLoss: 0.1960267349345811, Temperature 0.010956636797406038, step_size 0.0995709017671332, test_acc: 1.0\n",
            "Epoch 91, BestLoss: 0.1912265412421052, Temperature 0.010408804957535735, step_size 0.09956094467695648, test_acc: 1.0\n",
            "Epoch 92, BestLoss: 0.1888547292368473, Temperature 0.009888364709658948, step_size 0.09955098858248879, test_acc: 1.0\n",
            "Epoch 93, BestLoss: 0.1817187482119738, Temperature 0.009393946474176, step_size 0.09954103348363054, test_acc: 1.0\n",
            "Epoch 94, BestLoss: 0.1927053779388886, Temperature 0.0089242491504672, step_size 0.09953107938028218, test_acc: 1.0\n",
            "Epoch 95, BestLoss: 0.18733944129654573, Temperature 0.008478036692943839, step_size 0.09952112627234415, test_acc: 1.0\n",
            "Epoch 96, BestLoss: 0.18451189411961152, Temperature 0.008054134858296647, step_size 0.09951117415971691, test_acc: 1.0\n",
            "Epoch 97, BestLoss: 0.16380610624215725, Temperature 0.0076514281153818135, step_size 0.09950122304230094, test_acc: 1.0\n",
            "Epoch 98, BestLoss: 0.16698317577986707, Temperature 0.0072688567096127225, step_size 0.09949127291999671, test_acc: 1.0\n",
            "Epoch 99, BestLoss: 0.16660321233080477, Temperature 0.006905413874132086, step_size 0.0994813237927047, test_acc: 1.0\n",
            "Epoch 100, BestLoss: 0.17106843051225853, Temperature 0.006560143180425482, step_size 0.09947137566032543, test_acc: 1.0\n",
            "Epoch 101, BestLoss: 0.170864592127908, Temperature 0.0062321360214042075, step_size 0.0994614285227594, test_acc: 1.0\n",
            "Epoch 102, BestLoss: 0.16870872372263143, Temperature 0.005920529220333997, step_size 0.09945148237990713, test_acc: 1.0\n",
            "Epoch 103, BestLoss: 0.16217858916852254, Temperature 0.0056245027593172965, step_size 0.09944153723166914, test_acc: 1.0\n",
            "Epoch 104, BestLoss: 0.17201920655074404, Temperature 0.005343277621351432, step_size 0.09943159307794597, test_acc: 1.0\n",
            "Epoch 105, BestLoss: 0.138851606023288, Temperature 0.0050761137402838595, step_size 0.09942164991863818, test_acc: 1.0\n",
            "Epoch 106, BestLoss: 0.11502783921540846, Temperature 0.004822308053269666, step_size 0.09941170775364631, test_acc: 1.0\n",
            "Epoch 107, BestLoss: 0.10746700243700429, Temperature 0.004581192650606183, step_size 0.09940176658287095, test_acc: 1.0\n",
            "Epoch 108, BestLoss: 0.11151696723169524, Temperature 0.0043521330180758735, step_size 0.09939182640621266, test_acc: 1.0\n",
            "Epoch 109, BestLoss: 0.09758200542494973, Temperature 0.0041345263671720795, step_size 0.09938188722357204, test_acc: 1.0\n",
            "Epoch 110, BestLoss: 0.08890105345968455, Temperature 0.003927800048813475, step_size 0.09937194903484968, test_acc: 1.0\n",
            "Epoch 111, BestLoss: 0.08842521813505419, Temperature 0.0037314100463728015, step_size 0.0993620118399462, test_acc: 1.0\n",
            "Epoch 112, BestLoss: 0.075370218821498, Temperature 0.0035448395440541612, step_size 0.09935207563876221, test_acc: 1.0\n",
            "Epoch 113, BestLoss: 0.07204737965053212, Temperature 0.003367597566851453, step_size 0.09934214043119834, test_acc: 1.0\n",
            "Epoch 114, BestLoss: 0.06782921465567635, Temperature 0.00319921768850888, step_size 0.09933220621715522, test_acc: 1.0\n",
            "Epoch 115, BestLoss: 0.05514325312961932, Temperature 0.003039256804083436, step_size 0.0993222729965335, test_acc: 1.0\n",
            "Epoch 116, BestLoss: 0.05179787614290094, Temperature 0.0028872939638792637, step_size 0.09931234076923384, test_acc: 1.0\n",
            "Epoch 117, BestLoss: 0.054036262161491745, Temperature 0.0027429292656853004, step_size 0.09930240953515691, test_acc: 1.0\n",
            "Epoch 118, BestLoss: 0.04914427933508029, Temperature 0.0026057828024010354, step_size 0.0992924792942034, test_acc: 1.0\n",
            "Epoch 119, BestLoss: 0.0424065863361783, Temperature 0.0024754936622809836, step_size 0.09928255004627398, test_acc: 1.0\n",
            "Epoch 120, BestLoss: 0.03470910766748916, Temperature 0.002351718979166934, step_size 0.09927262179126936, test_acc: 1.0\n",
            "Epoch 121, BestLoss: 0.040563570893098554, Temperature 0.0022341330302085875, step_size 0.09926269452909023, test_acc: 1.0\n",
            "Epoch 122, BestLoss: 0.02220490625683604, Temperature 0.002122426378698158, step_size 0.09925276825963732, test_acc: 1.0\n",
            "Epoch 123, BestLoss: 0.02722168915618216, Temperature 0.0020163050597632503, step_size 0.09924284298281136, test_acc: 1.0\n",
            "Epoch 124, BestLoss: 0.025760414155418545, Temperature 0.0019154898067750877, step_size 0.09923291869851308, test_acc: 1.0\n",
            "Epoch 125, BestLoss: 0.02101079402650261, Temperature 0.0018197153164363333, step_size 0.09922299540664323, test_acc: 1.0\n",
            "Epoch 126, BestLoss: 0.017422598652064702, Temperature 0.0017287295506145165, step_size 0.09921307310710256, test_acc: 1.0\n",
            "Epoch 127, BestLoss: 0.012280586062735906, Temperature 0.0016422930730837905, step_size 0.09920315179979185, test_acc: 1.0\n",
            "Epoch 128, BestLoss: 0.013821156253768245, Temperature 0.0015601784194296008, step_size 0.09919323148461187, test_acc: 1.0\n",
            "Epoch 129, BestLoss: 0.011051531935098668, Temperature 0.0014821694984581207, step_size 0.09918331216146341, test_acc: 1.0\n",
            "Epoch 130, BestLoss: 0.008395284581493597, Temperature 0.0014080610235352145, step_size 0.09917339383024726, test_acc: 1.0\n",
            "Epoch 131, BestLoss: 0.005898209354742228, Temperature 0.0013376579723584536, step_size 0.09916347649086424, test_acc: 1.0\n",
            "Epoch 132, BestLoss: 0.004145160510422428, Temperature 0.0012707750737405309, step_size 0.09915356014321515, test_acc: 1.0\n",
            "Epoch 133, BestLoss: 0.0032535792043393856, Temperature 0.0012072363200535043, step_size 0.09914364478720084, test_acc: 1.0\n",
            "Epoch 134, BestLoss: 0.00460241820151725, Temperature 0.001146874504050829, step_size 0.09913373042272211, test_acc: 1.0\n",
            "Epoch 135, BestLoss: 0.002627649273628058, Temperature 0.0010895307788482875, step_size 0.09912381704967983, test_acc: 1.0\n",
            "Epoch 136, BestLoss: 0.0027287191344002528, Temperature 0.001035054239905873, step_size 0.09911390466797487, test_acc: 1.0\n",
            "Epoch 137, BestLoss: 0.0037143324927664868, Temperature 0.0009833015279105794, step_size 0.09910399327750807, test_acc: 1.0\n",
            "Epoch 138, BestLoss: 0.004235174279769117, Temperature 0.0009341364515150504, step_size 0.09909408287818032, test_acc: 1.0\n",
            "Epoch 139, BestLoss: 0.003342975289683331, Temperature 0.0008874296289392978, step_size 0.0990841734698925, test_acc: 1.0\n",
            "Epoch 140, BestLoss: 0.0031346596651923328, Temperature 0.0008430581474923328, step_size 0.0990742650525455, test_acc: 1.0\n",
            "Epoch 141, BestLoss: 0.004119279667812892, Temperature 0.0008009052401177162, step_size 0.09906435762604025, test_acc: 1.0\n",
            "Epoch 142, BestLoss: 0.0037037818569755415, Temperature 0.0007608599781118304, step_size 0.09905445119027766, test_acc: 1.0\n",
            "Epoch 143, BestLoss: 0.007888249596422533, Temperature 0.0007228169792062388, step_size 0.09904454574515863, test_acc: 1.0\n",
            "Epoch 144, BestLoss: 0.004454573009332445, Temperature 0.0006866761302459269, step_size 0.09903464129058412, test_acc: 1.0\n",
            "Epoch 145, BestLoss: 0.005212197100939022, Temperature 0.0006523423237336305, step_size 0.09902473782645506, test_acc: 1.0\n",
            "Epoch 146, BestLoss: 0.004466980015412992, Temperature 0.0006197252075469489, step_size 0.09901483535267241, test_acc: 1.0\n",
            "Epoch 147, BestLoss: 0.00462990311556709, Temperature 0.0005887389471696014, step_size 0.09900493386913715, test_acc: 1.0\n",
            "Epoch 148, BestLoss: 0.004586703197494058, Temperature 0.0005593019998111214, step_size 0.09899503337575025, test_acc: 1.0\n",
            "Epoch 149, BestLoss: 0.004776070526084713, Temperature 0.0005313368998205653, step_size 0.09898513387241267, test_acc: 1.0\n",
            "Epoch 150, BestLoss: 0.003224167302479111, Temperature 0.0005047700548295369, step_size 0.09897523535902543, test_acc: 1.0\n",
            "Epoch 151, BestLoss: 0.0025784272003571396, Temperature 0.00047953155208806006, step_size 0.09896533783548953, test_acc: 1.0\n",
            "Epoch 152, BestLoss: 0.002004515895500553, Temperature 0.000455554974483657, step_size 0.09895544130170597, test_acc: 1.0\n",
            "Epoch 153, BestLoss: 0.0021032417241014837, Temperature 0.0004327772257594741, step_size 0.0989455457575758, test_acc: 1.0\n",
            "Epoch 154, BestLoss: 0.0013848598537213228, Temperature 0.0004111383644715004, step_size 0.09893565120300005, test_acc: 1.0\n",
            "Epoch 155, BestLoss: 0.0021673699153415735, Temperature 0.00039058144624792536, step_size 0.09892575763787975, test_acc: 1.0\n",
            "Epoch 156, BestLoss: 0.002382363848370053, Temperature 0.0003710523739355291, step_size 0.09891586506211597, test_acc: 1.0\n",
            "Epoch 157, BestLoss: 0.0015441896074558786, Temperature 0.00035249975523875265, step_size 0.09890597347560975, test_acc: 1.0\n",
            "Epoch 158, BestLoss: 0.0014538765131569035, Temperature 0.000334874767476815, step_size 0.0988960828782622, test_acc: 1.0\n",
            "Epoch 159, BestLoss: 0.0017023527607402012, Temperature 0.00031813102910297424, step_size 0.09888619326997437, test_acc: 1.0\n",
            "Epoch 160, BestLoss: 0.0012422468096032508, Temperature 0.0003022244776478255, step_size 0.09887630465064738, test_acc: 1.0\n",
            "Epoch 161, BestLoss: 0.0011258641459679604, Temperature 0.0002871132537654342, step_size 0.09886641702018231, test_acc: 1.0\n",
            "Epoch 162, BestLoss: 0.001979377720178825, Temperature 0.00027275759107716247, step_size 0.0988565303784803, test_acc: 1.0\n",
            "Epoch 163, BestLoss: 0.0010923262415726092, Temperature 0.00025911971152330434, step_size 0.09884664472544245, test_acc: 1.0\n",
            "Epoch 164, BestLoss: 0.0015227362214412356, Temperature 0.00024616372594713913, step_size 0.0988367600609699, test_acc: 1.0\n",
            "Epoch 165, BestLoss: 0.001114905297233201, Temperature 0.00023385553964978215, step_size 0.0988268763849638, test_acc: 1.0\n",
            "Epoch 166, BestLoss: 0.0008121222371610999, Temperature 0.00022216276266729304, step_size 0.09881699369732531, test_acc: 1.0\n",
            "Epoch 167, BestLoss: 0.0007246871686256674, Temperature 0.00021105462453392839, step_size 0.09880711199795558, test_acc: 1.0\n",
            "Epoch 168, BestLoss: 0.0009795958845435452, Temperature 0.00020050189330723196, step_size 0.09879723128675579, test_acc: 1.0\n",
            "Epoch 169, BestLoss: 0.0004820901871295606, Temperature 0.00019047679864187035, step_size 0.09878735156362711, test_acc: 1.0\n",
            "Epoch 170, BestLoss: 0.00047881747532037207, Temperature 0.00018095295870977683, step_size 0.09877747282847076, test_acc: 1.0\n",
            "Epoch 171, BestLoss: 0.0003476953076020622, Temperature 0.00017190531077428798, step_size 0.0987675950811879, test_acc: 1.0\n",
            "Epoch 172, BestLoss: 0.00027231294354527867, Temperature 0.00016331004523557357, step_size 0.09875771832167979, test_acc: 1.0\n",
            "Epoch 173, BestLoss: 0.00037280739477399047, Temperature 0.00015514454297379488, step_size 0.09874784254984763, test_acc: 1.0\n",
            "Epoch 174, BestLoss: 0.0003901309809963648, Temperature 0.00014738731582510513, step_size 0.09873796776559264, test_acc: 1.0\n",
            "Epoch 175, BestLoss: 0.0005572998132393621, Temperature 0.00014001795003384986, step_size 0.09872809396881609, test_acc: 1.0\n",
            "Epoch 176, BestLoss: 0.0007049571060083974, Temperature 0.00013301705253215737, step_size 0.09871822115941921, test_acc: 1.0\n",
            "Epoch 177, BestLoss: 0.0007368748747486133, Temperature 0.0001263661999055495, step_size 0.09870834933730327, test_acc: 1.0\n",
            "Epoch 178, BestLoss: 0.0004013423178891383, Temperature 0.000120047889910272, step_size 0.09869847850236954, test_acc: 1.0\n",
            "Epoch 179, BestLoss: 0.00047813839933037527, Temperature 0.0001140454954147584, step_size 0.09868860865451931, test_acc: 1.0\n",
            "Epoch 180, BestLoss: 0.00040182685728399676, Temperature 0.00010834322064402047, step_size 0.09867873979365387, test_acc: 1.0\n",
            "Epoch 181, BestLoss: 0.00038388238696669913, Temperature 0.00010292605961181944, step_size 0.09866887191967451, test_acc: 1.0\n",
            "Epoch 182, BestLoss: 0.0003866855817028517, Temperature 9.777975663122846e-05, step_size 0.09865900503248254, test_acc: 1.0\n",
            "Epoch 183, BestLoss: 0.00030385396440767333, Temperature 9.289076879966704e-05, step_size 0.09864913913197929, test_acc: 1.0\n",
            "Epoch 184, BestLoss: 0.0003560448242771555, Temperature 8.824623035968368e-05, step_size 0.09863927421806609, test_acc: 1.0\n",
            "Epoch 185, BestLoss: 0.00034139876130293446, Temperature 8.383391884169949e-05, step_size 0.09862941029064429, test_acc: 1.0\n",
            "Epoch 186, BestLoss: 0.00032011696045624124, Temperature 7.96422228996145e-05, step_size 0.09861954734961523, test_acc: 1.0\n",
            "Epoch 187, BestLoss: 0.0002480687652554573, Temperature 7.566011175463378e-05, step_size 0.09860968539488027, test_acc: 1.0\n",
            "Epoch 188, BestLoss: 0.0002572587420605033, Temperature 7.187710616690208e-05, step_size 0.09859982442634078, test_acc: 1.0\n",
            "Epoch 189, BestLoss: 0.00020614219580177768, Temperature 6.828325085855697e-05, step_size 0.09858996444389816, test_acc: 1.0\n",
            "Epoch 190, BestLoss: 0.00013936013735496117, Temperature 6.486908831562912e-05, step_size 0.09858010544745377, test_acc: 1.0\n",
            "Epoch 191, BestLoss: 0.00015015447456282157, Temperature 6.162563389984766e-05, step_size 0.09857024743690902, test_acc: 1.0\n",
            "Epoch 192, BestLoss: 0.00019911062137277039, Temperature 5.8544352204855274e-05, step_size 0.09856039041216533, test_acc: 1.0\n",
            "Epoch 193, BestLoss: 0.00026603209168521305, Temperature 5.561713459461251e-05, step_size 0.09855053437312411, test_acc: 1.0\n",
            "Epoch 194, BestLoss: 0.0001440897055636394, Temperature 5.283627786488188e-05, step_size 0.0985406793196868, test_acc: 1.0\n",
            "Epoch 195, BestLoss: 0.00013923335515679216, Temperature 5.019446397163778e-05, step_size 0.09853082525175483, test_acc: 1.0\n",
            "Epoch 196, BestLoss: 0.00020414689853574867, Temperature 4.768474077305589e-05, step_size 0.09852097216922966, test_acc: 1.0\n",
            "Epoch 197, BestLoss: 0.0001534700247576843, Temperature 4.53005037344031e-05, step_size 0.09851112007201274, test_acc: 1.0\n",
            "Epoch 198, BestLoss: 0.00014652676654033567, Temperature 4.303547854768294e-05, step_size 0.09850126896000554, test_acc: 1.0\n",
            "Epoch 199, BestLoss: 0.00015221841425714952, Temperature 4.0883704620298796e-05, step_size 0.09849141883310954, test_acc: 1.0\n",
            "Epoch 200, BestLoss: 8.898200550807742e-05, Temperature 3.883951938928385e-05, step_size 0.09848156969122623, test_acc: 1.0\n",
            "Epoch 201, BestLoss: 7.266221870451242e-05, Temperature 3.689754341981966e-05, step_size 0.09847172153425711, test_acc: 1.0\n",
            "Epoch 202, BestLoss: 6.939256085965486e-05, Temperature 3.505266624882867e-05, step_size 0.09846187436210369, test_acc: 1.0\n",
            "Epoch 203, BestLoss: 5.862517335135843e-05, Temperature 3.3300032936387236e-05, step_size 0.09845202817466747, test_acc: 1.0\n",
            "Epoch 204, BestLoss: 4.815615488085764e-05, Temperature 3.163503128956787e-05, step_size 0.09844218297185, test_acc: 1.0\n",
            "Epoch 205, BestLoss: 5.3758115545519485e-05, Temperature 3.0053279725089478e-05, step_size 0.09843233875355283, test_acc: 1.0\n",
            "Epoch 206, BestLoss: 4.9885297123713695e-05, Temperature 2.8550615738835003e-05, step_size 0.09842249551967747, test_acc: 1.0\n",
            "Epoch 207, BestLoss: 7.711596377823671e-05, Temperature 2.7123084951893252e-05, step_size 0.0984126532701255, test_acc: 1.0\n",
            "Epoch 208, BestLoss: 3.810596557604026e-05, Temperature 2.576693070429859e-05, step_size 0.09840281200479849, test_acc: 1.0\n",
            "Epoch 209, BestLoss: 5.741094276881545e-05, Temperature 2.447858416908366e-05, step_size 0.09839297172359801, test_acc: 1.0\n",
            "Epoch 210, BestLoss: 5.483068493135694e-05, Temperature 2.3254654960629475e-05, step_size 0.09838313242642566, test_acc: 1.0\n",
            "Epoch 211, BestLoss: 4.873435996732421e-05, Temperature 2.2091922212598e-05, step_size 0.09837329411318302, test_acc: 1.0\n",
            "Epoch 212, BestLoss: 4.235642614485271e-05, Temperature 2.09873261019681e-05, step_size 0.0983634567837717, test_acc: 1.0\n",
            "Epoch 213, BestLoss: 9.302717782882997e-05, Temperature 1.9937959796869693e-05, step_size 0.09835362043809333, test_acc: 1.0\n",
            "Epoch 214, BestLoss: 8.577234269093565e-05, Temperature 1.8941061807026206e-05, step_size 0.09834378507604952, test_acc: 1.0\n",
            "Epoch 215, BestLoss: 6.751878445897454e-05, Temperature 1.7994008716674895e-05, step_size 0.09833395069754192, test_acc: 1.0\n",
            "Epoch 216, BestLoss: 9.029187635701638e-05, Temperature 1.7094308280841148e-05, step_size 0.09832411730247216, test_acc: 1.0\n",
            "Epoch 217, BestLoss: 7.677755131483558e-05, Temperature 1.623959286679909e-05, step_size 0.09831428489074191, test_acc: 1.0\n",
            "Epoch 218, BestLoss: 7.291108576050138e-05, Temperature 1.5427613223459133e-05, step_size 0.09830445346225283, test_acc: 1.0\n",
            "Epoch 219, BestLoss: 6.6000913751649e-05, Temperature 1.4656232562286176e-05, step_size 0.09829462301690661, test_acc: 1.0\n",
            "Epoch 220, BestLoss: 7.514648549820131e-05, Temperature 1.3923420934171866e-05, step_size 0.09828479355460493, test_acc: 1.0\n",
            "Epoch 221, BestLoss: 1.8266028202476332e-05, Temperature 1.3227249887463272e-05, step_size 0.09827496507524947, test_acc: 1.0\n",
            "Epoch 222, BestLoss: 2.235448976166387e-05, Temperature 1.2565887393090107e-05, step_size 0.09826513757874195, test_acc: 1.0\n",
            "Epoch 223, BestLoss: 1.352536820444233e-05, Temperature 1.19375930234356e-05, step_size 0.09825531106498407, test_acc: 1.0\n",
            "Epoch 224, BestLoss: 1.665979580827972e-05, Temperature 1.134071337226382e-05, step_size 0.09824548553387757, test_acc: 1.0\n",
            "Epoch 225, BestLoss: 9.680800540676267e-06, Temperature 1.0773677703650629e-05, step_size 0.09823566098532419, test_acc: 1.0\n",
            "Epoch 226, BestLoss: 1.0363876394523788e-05, Temperature 1.0234993818468096e-05, step_size 0.09822583741922565, test_acc: 1.0\n",
            "Epoch 227, BestLoss: 7.241235190996045e-06, Temperature 9.72324412754469e-06, step_size 0.09821601483548373, test_acc: 1.0\n",
            "Epoch 228, BestLoss: 1.1413509686363214e-05, Temperature 9.237081921167456e-06, step_size 0.09820619323400018, test_acc: 1.0\n",
            "Epoch 229, BestLoss: 9.526577985506656e-06, Temperature 8.775227825109082e-06, step_size 0.09819637261467677, test_acc: 1.0\n",
            "Epoch 230, BestLoss: 4.852831300314657e-06, Temperature 8.336466433853627e-06, step_size 0.09818655297741531, test_acc: 1.0\n",
            "Epoch 231, BestLoss: 5.24294824822328e-06, Temperature 7.919643112160946e-06, step_size 0.09817673432211757, test_acc: 1.0\n",
            "Epoch 232, BestLoss: 5.492902871871772e-06, Temperature 7.523660956552898e-06, step_size 0.09816691664868536, test_acc: 1.0\n",
            "Epoch 233, BestLoss: 1.769015012834793e-05, Temperature 7.147477908725253e-06, step_size 0.09815709995702049, test_acc: 1.0\n",
            "Epoch 234, BestLoss: 9.237041467270086e-06, Temperature 6.79010401328899e-06, step_size 0.09814728424702479, test_acc: 1.0\n",
            "Epoch 235, BestLoss: 8.561951374985062e-06, Temperature 6.45059881262454e-06, step_size 0.09813746951860008, test_acc: 1.0\n",
            "Epoch 236, BestLoss: 7.2498815114093975e-06, Temperature 6.128068871993313e-06, step_size 0.09812765577164823, test_acc: 1.0\n",
            "Epoch 237, BestLoss: 1.4018737174132103e-05, Temperature 5.821665428393647e-06, step_size 0.09811784300607106, test_acc: 1.0\n",
            "Epoch 238, BestLoss: 1.3834078003655115e-05, Temperature 5.530582156973964e-06, step_size 0.09810803122177045, test_acc: 1.0\n",
            "Epoch 239, BestLoss: 1.8318583451543792e-05, Temperature 5.254053049125266e-06, step_size 0.09809822041864827, test_acc: 1.0\n",
            "Epoch 240, BestLoss: 2.2812728448661232e-05, Temperature 4.991350396669002e-06, step_size 0.0980884105966064, test_acc: 1.0\n",
            "Epoch 241, BestLoss: 1.8886748575395276e-05, Temperature 4.741782876835552e-06, step_size 0.09807860175554674, test_acc: 1.0\n",
            "Epoch 242, BestLoss: 1.1319979007533397e-05, Temperature 4.504693732993774e-06, step_size 0.09806879389537118, test_acc: 1.0\n",
            "Epoch 243, BestLoss: 2.1830472039762773e-05, Temperature 4.279459046344085e-06, step_size 0.09805898701598165, test_acc: 1.0\n",
            "Epoch 244, BestLoss: 8.307383772163706e-06, Temperature 4.065486094026881e-06, step_size 0.09804918111728005, test_acc: 1.0\n",
            "Epoch 245, BestLoss: 7.236942859497619e-06, Temperature 3.862211789325536e-06, step_size 0.09803937619916832, test_acc: 1.0\n",
            "Epoch 246, BestLoss: 7.678717613701994e-06, Temperature 3.6691011998592594e-06, step_size 0.0980295722615484, test_acc: 1.0\n",
            "Epoch 247, BestLoss: 6.65588806097114e-06, Temperature 3.485646139866296e-06, step_size 0.09801976930432225, test_acc: 1.0\n",
            "Epoch 248, BestLoss: 9.974425867738828e-06, Temperature 3.3113638328729812e-06, step_size 0.09800996732739183, test_acc: 1.0\n",
            "Epoch 249, BestLoss: 1.1331084467791884e-05, Temperature 3.145795641229332e-06, step_size 0.09800016633065908, test_acc: 1.0\n",
            "Epoch 250, BestLoss: 1.4389574332423783e-05, Temperature 2.988505859167865e-06, step_size 0.09799036631402602, test_acc: 1.0\n",
            "Epoch 251, BestLoss: 1.8148243070604422e-05, Temperature 2.8390805662094717e-06, step_size 0.09798056727739463, test_acc: 1.0\n",
            "Epoch 252, BestLoss: 1.185350096802388e-05, Temperature 2.697126537898998e-06, step_size 0.09797076922066689, test_acc: 1.0\n",
            "Epoch 253, BestLoss: 1.1916890130486102e-05, Temperature 2.562270211004048e-06, step_size 0.09796097214374481, test_acc: 1.0\n",
            "Epoch 254, BestLoss: 9.819208897659962e-06, Temperature 2.4341567004538453e-06, step_size 0.09795117604653045, test_acc: 1.0\n",
            "Epoch 255, BestLoss: 1.1920330502727285e-05, Temperature 2.312448865431153e-06, step_size 0.09794138092892579, test_acc: 1.0\n",
            "Epoch 256, BestLoss: 1.1472758238496961e-05, Temperature 2.196826422159595e-06, step_size 0.0979315867908329, test_acc: 1.0\n",
            "Epoch 257, BestLoss: 7.502951372310499e-06, Temperature 2.0869851010516154e-06, step_size 0.09792179363215382, test_acc: 1.0\n",
            "Epoch 258, BestLoss: 9.731767268970745e-06, Temperature 1.9826358459990345e-06, step_size 0.09791200145279061, test_acc: 1.0\n",
            "Epoch 259, BestLoss: 1.4884011787128274e-05, Temperature 1.8835040536990826e-06, step_size 0.09790221025264532, test_acc: 1.0\n",
            "Epoch 260, BestLoss: 8.370538620275759e-06, Temperature 1.7893288510141284e-06, step_size 0.09789242003162006, test_acc: 1.0\n",
            "Epoch 261, BestLoss: 7.812453411744505e-06, Temperature 1.699862408463422e-06, step_size 0.0978826307896169, test_acc: 1.0\n",
            "Epoch 262, BestLoss: 9.583160337372758e-06, Temperature 1.6148692880402508e-06, step_size 0.09787284252653794, test_acc: 1.0\n",
            "Epoch 263, BestLoss: 8.151902743765824e-06, Temperature 1.534125823638238e-06, step_size 0.0978630552422853, test_acc: 1.0\n",
            "Epoch 264, BestLoss: 9.688087068293862e-06, Temperature 1.457419532456326e-06, step_size 0.09785326893676106, test_acc: 1.0\n",
            "Epoch 265, BestLoss: 6.957525683437938e-06, Temperature 1.3845485558335097e-06, step_size 0.09784348360986739, test_acc: 1.0\n",
            "Epoch 266, BestLoss: 4.286399003194622e-06, Temperature 1.3153211280418342e-06, step_size 0.0978336992615064, test_acc: 1.0\n",
            "Epoch 267, BestLoss: 4.041042054081414e-06, Temperature 1.2495550716397424e-06, step_size 0.09782391589158025, test_acc: 1.0\n",
            "Epoch 268, BestLoss: 4.088134249109606e-06, Temperature 1.1870773180577553e-06, step_size 0.09781413349999109, test_acc: 1.0\n",
            "Epoch 269, BestLoss: 4.396265237531921e-06, Temperature 1.1277234521548674e-06, step_size 0.0978043520866411, test_acc: 1.0\n",
            "Epoch 270, BestLoss: 3.639126166315744e-06, Temperature 1.071337279547124e-06, step_size 0.09779457165143243, test_acc: 1.0\n",
            "Epoch 271, BestLoss: 1.6659752222855824e-06, Temperature 1.0177704155697677e-06, step_size 0.09778479219426729, test_acc: 1.0\n",
            "Epoch 272, BestLoss: 2.8164163031120157e-06, Temperature 9.668818947912793e-07, step_size 0.09777501371504786, test_acc: 1.0\n",
            "Epoch 273, BestLoss: 1.9802756809650346e-06, Temperature 9.185378000517153e-07, step_size 0.09776523621367636, test_acc: 1.0\n",
            "Epoch 274, BestLoss: 1.6201280814240417e-06, Temperature 8.726109100491295e-07, step_size 0.09775545969005499, test_acc: 1.0\n",
            "Epoch 275, BestLoss: 1.1868709418974845e-06, Temperature 8.28980364546673e-07, step_size 0.09774568414408598, test_acc: 1.0\n",
            "Epoch 276, BestLoss: 8.432268960734832e-07, Temperature 7.875313463193393e-07, step_size 0.09773590957567158, test_acc: 1.0\n",
            "Epoch 277, BestLoss: 5.027123700348321e-07, Temperature 7.481547790033723e-07, step_size 0.09772613598471401, test_acc: 1.0\n",
            "Epoch 278, BestLoss: 5.456920489947514e-07, Temperature 7.107470400532036e-07, step_size 0.09771636337111554, test_acc: 1.0\n",
            "Epoch 279, BestLoss: 1.1235166173959118e-06, Temperature 6.752096880505434e-07, step_size 0.09770659173477843, test_acc: 1.0\n",
            "Epoch 280, BestLoss: 7.781830187499552e-07, Temperature 6.414492036480162e-07, step_size 0.09769682107560496, test_acc: 1.0\n",
            "Epoch 281, BestLoss: 9.769287166568748e-07, Temperature 6.093767434656153e-07, step_size 0.0976870513934974, test_acc: 1.0\n",
            "Epoch 282, BestLoss: 1.00148050157459e-06, Temperature 5.789079062923345e-07, step_size 0.09767728268835806, test_acc: 1.0\n",
            "Epoch 283, BestLoss: 6.560800320300304e-07, Temperature 5.499625109777178e-07, step_size 0.09766751496008923, test_acc: 1.0\n",
            "Epoch 284, BestLoss: 6.996028062172025e-07, Temperature 5.224643854288319e-07, step_size 0.09765774820859321, test_acc: 1.0\n",
            "Epoch 285, BestLoss: 6.490454608542225e-07, Temperature 4.963411661573902e-07, step_size 0.09764798243377236, test_acc: 1.0\n",
            "Epoch 286, BestLoss: 5.255367337196194e-07, Temperature 4.715241078495207e-07, step_size 0.09763821763552899, test_acc: 1.0\n",
            "Epoch 287, BestLoss: 4.979087280302212e-07, Temperature 4.4794790245704463e-07, step_size 0.09762845381376543, test_acc: 1.0\n",
            "Epoch 288, BestLoss: 4.96490051946985e-07, Temperature 4.255505073341924e-07, step_size 0.09761869096838406, test_acc: 1.0\n",
            "Epoch 289, BestLoss: 4.836038731713935e-07, Temperature 4.0427298196748275e-07, step_size 0.09760892909928723, test_acc: 1.0\n",
            "Epoch 290, BestLoss: 4.92086249737923e-07, Temperature 3.840593328691086e-07, step_size 0.0975991682063773, test_acc: 1.0\n",
            "Epoch 291, BestLoss: 6.423078024613748e-07, Temperature 3.648563662256532e-07, step_size 0.09758940828955667, test_acc: 1.0\n",
            "Epoch 292, BestLoss: 8.031998404782909e-07, Temperature 3.466135479143705e-07, step_size 0.09757964934872772, test_acc: 1.0\n",
            "Epoch 293, BestLoss: 1.3847909964701793e-06, Temperature 3.2928287051865196e-07, step_size 0.09756989138379285, test_acc: 1.0\n",
            "Epoch 294, BestLoss: 2.1819921067560802e-06, Temperature 3.1281872699271933e-07, step_size 0.09756013439465447, test_acc: 1.0\n",
            "Epoch 295, BestLoss: 1.8544205764581162e-06, Temperature 2.9717779064308334e-07, step_size 0.09755037838121501, test_acc: 1.0\n",
            "Epoch 296, BestLoss: 1.8208993697646132e-06, Temperature 2.8231890111092917e-07, step_size 0.09754062334337689, test_acc: 1.0\n",
            "Epoch 297, BestLoss: 1.7725743843079048e-06, Temperature 2.682029560553827e-07, step_size 0.09753086928104256, test_acc: 1.0\n",
            "Epoch 298, BestLoss: 9.67884396136249e-07, Temperature 2.5479280825261354e-07, step_size 0.09752111619411445, test_acc: 1.0\n",
            "Epoch 299, BestLoss: 8.421407629930624e-07, Temperature 2.4205316783998285e-07, step_size 0.09751136408249504, test_acc: 1.0\n",
            "Epoch 300, BestLoss: 9.321671847896201e-07, Temperature 2.299505094479837e-07, step_size 0.0975016129460868, test_acc: 1.0\n",
            "Epoch 301, BestLoss: 9.904252530033607e-07, Temperature 2.1845298397558449e-07, step_size 0.09749186278479219, test_acc: 1.0\n",
            "Epoch 302, BestLoss: 7.814377141032051e-07, Temperature 2.0753033477680526e-07, step_size 0.0974821135985137, test_acc: 1.0\n",
            "Epoch 303, BestLoss: 6.08711938683012e-07, Temperature 1.97153818037965e-07, step_size 0.09747236538715386, test_acc: 1.0\n",
            "Epoch 304, BestLoss: 5.808383257121063e-07, Temperature 1.8729612713606673e-07, step_size 0.09746261815061515, test_acc: 1.0\n",
            "Epoch 305, BestLoss: 8.569487923094018e-07, Temperature 1.7793132077926338e-07, step_size 0.09745287188880009, test_acc: 1.0\n",
            "Epoch 306, BestLoss: 8.666647952189484e-07, Temperature 1.690347547403002e-07, step_size 0.09744312660161121, test_acc: 1.0\n",
            "Epoch 307, BestLoss: 7.179208149733573e-07, Temperature 1.6058301700328518e-07, step_size 0.09743338228895106, test_acc: 1.0\n",
            "Epoch 308, BestLoss: 6.36027820498017e-07, Temperature 1.5255386615312092e-07, step_size 0.09742363895072216, test_acc: 1.0\n",
            "Epoch 309, BestLoss: 7.548706974125599e-07, Temperature 1.4492617284546488e-07, step_size 0.09741389658682709, test_acc: 1.0\n",
            "Epoch 310, BestLoss: 4.0239313836333223e-07, Temperature 1.3767986420319162e-07, step_size 0.0974041551971684, test_acc: 1.0\n",
            "Epoch 311, BestLoss: 5.552784468264066e-07, Temperature 1.3079587099303203e-07, step_size 0.09739441478164869, test_acc: 1.0\n",
            "Epoch 312, BestLoss: 5.267606760184912e-07, Temperature 1.2425607744338041e-07, step_size 0.09738467534017052, test_acc: 1.0\n",
            "Epoch 313, BestLoss: 6.831268659852125e-07, Temperature 1.1804327357121139e-07, step_size 0.0973749368726365, test_acc: 1.0\n",
            "Epoch 314, BestLoss: 6.152680200164067e-07, Temperature 1.1214110989265082e-07, step_size 0.09736519937894923, test_acc: 1.0\n",
            "Epoch 315, BestLoss: 4.097044483243038e-07, Temperature 1.0653405439801827e-07, step_size 0.09735546285901134, test_acc: 1.0\n",
            "Epoch 316, BestLoss: 2.9698512135447905e-07, Temperature 1.0120735167811735e-07, step_size 0.09734572731272544, test_acc: 1.0\n",
            "Epoch 317, BestLoss: 3.7422634085776617e-07, Temperature 9.614698409421148e-08, step_size 0.09733599273999416, test_acc: 1.0\n",
            "Epoch 318, BestLoss: 3.96419270330576e-07, Temperature 9.133963488950091e-08, step_size 0.09732625914072016, test_acc: 1.0\n",
            "Epoch 319, BestLoss: 2.969982556351849e-07, Temperature 8.677265314502586e-08, step_size 0.09731652651480609, test_acc: 1.0\n",
            "Epoch 320, BestLoss: 2.910722416955587e-07, Temperature 8.243402048777457e-08, step_size 0.09730679486215461, test_acc: 1.0\n",
            "Epoch 321, BestLoss: 2.605906022332086e-07, Temperature 7.831231946338584e-08, step_size 0.0972970641826684, test_acc: 1.0\n",
            "Epoch 322, BestLoss: 3.269844003713552e-07, Temperature 7.439670349021654e-08, step_size 0.09728733447625013, test_acc: 1.0\n",
            "Epoch 323, BestLoss: 2.567310560572701e-07, Temperature 7.067686831570571e-08, step_size 0.0972776057428025, test_acc: 1.0\n",
            "Epoch 324, BestLoss: 2.1619617957060533e-07, Temperature 6.714302489992043e-08, step_size 0.09726787798222822, test_acc: 1.0\n",
            "Epoch 325, BestLoss: 2.109932223921646e-07, Temperature 6.378587365492441e-08, step_size 0.09725815119443, test_acc: 1.0\n",
            "Epoch 326, BestLoss: 8.44795347211541e-08, Temperature 6.059657997217818e-08, step_size 0.09724842537931055, test_acc: 1.0\n",
            "Epoch 327, BestLoss: 9.772140462255613e-08, Temperature 5.756675097356927e-08, step_size 0.09723870053677262, test_acc: 1.0\n",
            "Epoch 328, BestLoss: 1.4304922492440712e-07, Temperature 5.46884134248908e-08, step_size 0.09722897666671895, test_acc: 1.0\n",
            "Epoch 329, BestLoss: 7.656236804248163e-08, Temperature 5.1953992753646255e-08, step_size 0.09721925376905229, test_acc: 1.0\n",
            "Epoch 330, BestLoss: 9.65814577160186e-08, Temperature 4.935629311596394e-08, step_size 0.09720953184367538, test_acc: 1.0\n",
            "Epoch 331, BestLoss: 1.313752966437346e-07, Temperature 4.688847846016574e-08, step_size 0.09719981089049101, test_acc: 1.0\n",
            "Epoch 332, BestLoss: 1.0002992993980658e-07, Temperature 4.454405453715745e-08, step_size 0.09719009090940196, test_acc: 1.0\n",
            "Epoch 333, BestLoss: 5.983973824880753e-08, Temperature 4.231685181029957e-08, step_size 0.09718037190031102, test_acc: 1.0\n",
            "Epoch 334, BestLoss: 1.1243647020295226e-07, Temperature 4.020100921978459e-08, step_size 0.09717065386312099, test_acc: 1.0\n",
            "Epoch 335, BestLoss: 1.2520716346779697e-07, Temperature 3.819095875879536e-08, step_size 0.09716093679773467, test_acc: 1.0\n",
            "Epoch 336, BestLoss: 1.207516765620444e-07, Temperature 3.628141082085559e-08, step_size 0.0971512207040549, test_acc: 1.0\n",
            "Epoch 337, BestLoss: 1.1510565448433691e-07, Temperature 3.446734027981281e-08, step_size 0.09714150558198449, test_acc: 1.0\n",
            "Epoch 338, BestLoss: 8.16485809686722e-08, Temperature 3.2743973265822166e-08, step_size 0.09713179143142629, test_acc: 1.0\n",
            "Epoch 339, BestLoss: 1.0058583393761829e-07, Temperature 3.110677460253105e-08, step_size 0.09712207825228315, test_acc: 1.0\n",
            "Epoch 340, BestLoss: 7.58327772506042e-08, Temperature 2.95514358724045e-08, step_size 0.09711236604445793, test_acc: 1.0\n",
            "Epoch 341, BestLoss: 3.3241929466699326e-08, Temperature 2.8073864078784275e-08, step_size 0.09710265480785349, test_acc: 1.0\n",
            "Epoch 342, BestLoss: 5.8234121893617535e-08, Temperature 2.667017087484506e-08, step_size 0.0970929445423727, test_acc: 1.0\n",
            "Epoch 343, BestLoss: 5.186143934911075e-08, Temperature 2.5336662331102805e-08, step_size 0.09708323524791847, test_acc: 1.0\n",
            "Epoch 344, BestLoss: 2.990079665003256e-08, Temperature 2.4069829214547663e-08, step_size 0.09707352692439368, test_acc: 1.0\n",
            "Epoch 345, BestLoss: 3.53815985969995e-08, Temperature 2.286633775382028e-08, step_size 0.09706381957170124, test_acc: 1.0\n",
            "Epoch 346, BestLoss: 2.6960987503214973e-08, Temperature 2.1723020866129264e-08, step_size 0.09705411318974407, test_acc: 1.0\n",
            "Epoch 347, BestLoss: 2.474598189852822e-08, Temperature 2.06368698228228e-08, step_size 0.09704440777842509, test_acc: 1.0\n",
            "Epoch 348, BestLoss: 3.191615450847256e-08, Temperature 1.960502633168166e-08, step_size 0.09703470333764724, test_acc: 1.0\n",
            "Epoch 349, BestLoss: 1.6542315605973026e-08, Temperature 1.8624775015097576e-08, step_size 0.09702499986731349, test_acc: 1.0\n",
            "Epoch 350, BestLoss: 2.3422179594185093e-08, Temperature 1.7693536264342694e-08, step_size 0.09701529736732675, test_acc: 1.0\n",
            "Epoch 351, BestLoss: 2.831976459460218e-08, Temperature 1.680885945112556e-08, step_size 0.09700559583759003, test_acc: 1.0\n",
            "Epoch 352, BestLoss: 2.9100865758209162e-08, Temperature 1.596841647856928e-08, step_size 0.09699589527800627, test_acc: 1.0\n",
            "Epoch 353, BestLoss: 2.8825948828235178e-08, Temperature 1.5169995654640813e-08, step_size 0.09698619568847847, test_acc: 1.0\n",
            "Epoch 354, BestLoss: 4.36392770879611e-08, Temperature 1.4411495871908771e-08, step_size 0.09697649706890962, test_acc: 1.0\n",
            "Epoch 355, BestLoss: 4.384485435408539e-08, Temperature 1.3690921078313332e-08, step_size 0.09696679941920273, test_acc: 1.0\n",
            "Epoch 356, BestLoss: 6.832707660387508e-08, Temperature 1.3006375024397665e-08, step_size 0.09695710273926081, test_acc: 1.0\n",
            "Epoch 357, BestLoss: 3.293297995131845e-08, Temperature 1.2356056273177782e-08, step_size 0.09694740702898688, test_acc: 1.0\n",
            "Epoch 358, BestLoss: 2.3491017757222865e-08, Temperature 1.1738253459518891e-08, step_size 0.09693771228828399, test_acc: 1.0\n",
            "Epoch 359, BestLoss: 2.4999922619546358e-08, Temperature 1.1151340786542946e-08, step_size 0.09692801851705517, test_acc: 1.0\n",
            "Epoch 360, BestLoss: 1.8511807020663063e-08, Temperature 1.0593773747215798e-08, step_size 0.09691832571520347, test_acc: 1.0\n",
            "Epoch 361, BestLoss: 1.9071191616443038e-08, Temperature 1.0064085059855008e-08, step_size 0.09690863388263195, test_acc: 1.0\n",
            "Epoch 362, BestLoss: 2.5641655738539394e-08, Temperature 9.560880806862258e-09, step_size 0.09689894301924369, test_acc: 1.0\n",
            "Epoch 363, BestLoss: 1.913620068292725e-08, Temperature 9.082836766519144e-09, step_size 0.09688925312494177, test_acc: 1.0\n",
            "Epoch 364, BestLoss: 1.99396823230453e-08, Temperature 8.628694928193187e-09, step_size 0.09687956419962929, test_acc: 1.0\n",
            "Epoch 365, BestLoss: 2.3578599923532933e-08, Temperature 8.197260181783527e-09, step_size 0.09686987624320932, test_acc: 1.0\n",
            "Epoch 366, BestLoss: 2.065247419831221e-08, Temperature 7.787397172694349e-09, step_size 0.096860189255585, test_acc: 1.0\n",
            "Epoch 367, BestLoss: 3.46396560750335e-08, Temperature 7.398027314059631e-09, step_size 0.09685050323665943, test_acc: 1.0\n",
            "Epoch 368, BestLoss: 1.4015378937812414e-08, Temperature 7.028125948356649e-09, step_size 0.09684081818633576, test_acc: 1.0\n",
            "Epoch 369, BestLoss: 1.9107787182587302e-08, Temperature 6.676719650938816e-09, step_size 0.09683113410451713, test_acc: 1.0\n",
            "Epoch 370, BestLoss: 1.7042923490808812e-08, Temperature 6.342883668391875e-09, step_size 0.09682145099110669, test_acc: 1.0\n",
            "Epoch 371, BestLoss: 1.1089143346387943e-08, Temperature 6.025739484972281e-09, step_size 0.09681176884600758, test_acc: 1.0\n",
            "Epoch 372, BestLoss: 7.562280120356854e-09, Temperature 5.724452510723666e-09, step_size 0.09680208766912297, test_acc: 1.0\n",
            "Epoch 373, BestLoss: 1.0449107989680336e-08, Temperature 5.4382298851874825e-09, step_size 0.09679240746035607, test_acc: 1.0\n",
            "Epoch 374, BestLoss: 2.1470055646864987e-08, Temperature 5.166318390928108e-09, step_size 0.09678272821961004, test_acc: 1.0\n",
            "Epoch 375, BestLoss: 8.706530273240089e-09, Temperature 4.908002471381702e-09, step_size 0.09677304994678808, test_acc: 1.0\n",
            "Epoch 376, BestLoss: 1.4042656690821743e-08, Temperature 4.662602347812617e-09, step_size 0.0967633726417934, test_acc: 1.0\n",
            "Epoch 377, BestLoss: 9.360143463303332e-09, Temperature 4.429472230421986e-09, step_size 0.09675369630452922, test_acc: 1.0\n",
            "Epoch 378, BestLoss: 3.5955256922110086e-09, Temperature 4.207998618900887e-09, step_size 0.09674402093489877, test_acc: 1.0\n",
            "Epoch 379, BestLoss: 4.336094002024818e-09, Temperature 3.997598687955842e-09, step_size 0.09673434653280528, test_acc: 1.0\n",
            "Epoch 380, BestLoss: 5.181150035788677e-09, Temperature 3.79771875355805e-09, step_size 0.09672467309815201, test_acc: 1.0\n",
            "Epoch 381, BestLoss: 7.433194153470659e-09, Temperature 3.6078328158801476e-09, step_size 0.0967150006308422, test_acc: 1.0\n",
            "Epoch 382, BestLoss: 8.697566957051069e-09, Temperature 3.42744117508614e-09, step_size 0.09670532913077912, test_acc: 1.0\n",
            "Epoch 383, BestLoss: 9.8204670552601e-09, Temperature 3.256069116331833e-09, step_size 0.09669565859786604, test_acc: 1.0\n",
            "Epoch 384, BestLoss: 1.1237309248563523e-08, Temperature 3.093265660515241e-09, step_size 0.09668598903200626, test_acc: 1.0\n",
            "Epoch 385, BestLoss: 1.038321763584889e-08, Temperature 2.9386023774894787e-09, step_size 0.09667632043310306, test_acc: 1.0\n",
            "Epoch 386, BestLoss: 4.069028573252353e-09, Temperature 2.7916722586150046e-09, step_size 0.09666665280105975, test_acc: 1.0\n",
            "Epoch 387, BestLoss: 6.21269175728706e-09, Temperature 2.6520886456842543e-09, step_size 0.09665698613577965, test_acc: 1.0\n",
            "Epoch 388, BestLoss: 4.78375145639275e-09, Temperature 2.5194842134000415e-09, step_size 0.09664732043716608, test_acc: 1.0\n",
            "Epoch 389, BestLoss: 5.5356296386907545e-09, Temperature 2.3935100027300392e-09, step_size 0.09663765570512237, test_acc: 1.0\n",
            "Epoch 390, BestLoss: 4.513866822679261e-09, Temperature 2.273834502593537e-09, step_size 0.09662799193955185, test_acc: 1.0\n",
            "Epoch 391, BestLoss: 2.5060027363747966e-09, Temperature 2.16014277746386e-09, step_size 0.0966183291403579, test_acc: 1.0\n",
            "Epoch 392, BestLoss: 8.35926724053909e-09, Temperature 2.052135638590667e-09, step_size 0.09660866730744386, test_acc: 1.0\n",
            "Epoch 393, BestLoss: 2.0294829454307594e-09, Temperature 1.9495288566611337e-09, step_size 0.09659900644071312, test_acc: 1.0\n",
            "Epoch 394, BestLoss: 8.445666584823158e-10, Temperature 1.852052413828077e-09, step_size 0.09658934654006905, test_acc: 1.0\n",
            "Epoch 395, BestLoss: 2.2713086145295123e-09, Temperature 1.759449793136673e-09, step_size 0.09657968760541505, test_acc: 1.0\n",
            "Epoch 396, BestLoss: 1.4738155645634329e-09, Temperature 1.6714773034798393e-09, step_size 0.09657002963665451, test_acc: 1.0\n",
            "Epoch 397, BestLoss: 7.350671941375307e-10, Temperature 1.5879034383058473e-09, step_size 0.09656037263369084, test_acc: 1.0\n",
            "Epoch 398, BestLoss: 1.991004483268006e-09, Temperature 1.508508266390555e-09, step_size 0.09655071659642747, test_acc: 1.0\n",
            "Epoch 399, BestLoss: 1.924153255621625e-09, Temperature 1.4330828530710272e-09, step_size 0.09654106152476782, test_acc: 1.0\n",
            "Epoch 400, BestLoss: 3.215475793797736e-09, Temperature 1.3614287104174757e-09, step_size 0.09653140741861535, test_acc: 1.0\n",
            "Epoch 401, BestLoss: 2.251135034727276e-09, Temperature 1.293357274896602e-09, step_size 0.09652175427787349, test_acc: 1.0\n",
            "Epoch 402, BestLoss: 1.688807648230767e-09, Temperature 1.2286894111517717e-09, step_size 0.09651210210244571, test_acc: 1.0\n",
            "Epoch 403, BestLoss: 2.7668307072141877e-09, Temperature 1.167254940594183e-09, step_size 0.09650245089223547, test_acc: 1.0\n",
            "Epoch 404, BestLoss: 2.333038221892212e-09, Temperature 1.1088921935644738e-09, step_size 0.09649280064714624, test_acc: 1.0\n",
            "Epoch 405, BestLoss: 1.6023174477713131e-09, Temperature 1.0534475838862501e-09, step_size 0.09648315136708153, test_acc: 1.0\n",
            "Epoch 406, BestLoss: 1.0096003833369131e-09, Temperature 1.0007752046919376e-09, step_size 0.09647350305194483, test_acc: 1.0\n",
            "Epoch 407, BestLoss: 1.4790099877082657e-09, Temperature 9.507364444573407e-10, step_size 0.09646385570163964, test_acc: 1.0\n",
            "Epoch 408, BestLoss: 1.7243081778218525e-09, Temperature 9.031996222344736e-10, step_size 0.09645420931606948, test_acc: 1.0\n",
            "Epoch 409, BestLoss: 9.59382091081916e-10, Temperature 8.580396411227499e-10, step_size 0.09644456389513786, test_acc: 1.0\n",
            "Epoch 410, BestLoss: 6.861611623204542e-10, Temperature 8.151376590666125e-10, step_size 0.09643491943874835, test_acc: 1.0\n",
            "Epoch 411, BestLoss: 7.187447906464351e-10, Temperature 7.743807761132818e-10, step_size 0.09642527594680447, test_acc: 1.0\n",
            "Epoch 412, BestLoss: 1.6796549399737843e-09, Temperature 7.356617373076177e-10, step_size 0.09641563341920979, test_acc: 1.0\n",
            "Epoch 413, BestLoss: 9.191321895145759e-10, Temperature 6.988786504422368e-10, step_size 0.09640599185586787, test_acc: 1.0\n",
            "Epoch 414, BestLoss: 2.1227504953704724e-09, Temperature 6.639347179201249e-10, step_size 0.09639635125668229, test_acc: 1.0\n",
            "Epoch 415, BestLoss: 2.79069138717223e-09, Temperature 6.307379820241186e-10, step_size 0.09638671162155663, test_acc: 1.0\n",
            "Epoch 416, BestLoss: 5.21947185304557e-10, Temperature 5.992010829229127e-10, step_size 0.09637707295039447, test_acc: 1.0\n",
            "Epoch 417, BestLoss: 1.0323997767475884e-09, Temperature 5.69241028776767e-10, step_size 0.09636743524309943, test_acc: 1.0\n",
            "Epoch 418, BestLoss: 3.2962774297675087e-10, Temperature 5.407789773379287e-10, step_size 0.09635779849957513, test_acc: 1.0\n",
            "Epoch 419, BestLoss: 4.388525137714336e-10, Temperature 5.137400284710322e-10, step_size 0.09634816271972517, test_acc: 1.0\n",
            "Epoch 420, BestLoss: 2.7708266047138407e-10, Temperature 4.880530270474806e-10, step_size 0.0963385279034532, test_acc: 1.0\n",
            "Epoch 421, BestLoss: 2.6680387685134663e-10, Temperature 4.6365037569510656e-10, step_size 0.09632889405066285, test_acc: 1.0\n",
            "Epoch 422, BestLoss: 1.9599814630318816e-10, Temperature 4.4046785691035123e-10, step_size 0.09631926116125779, test_acc: 1.0\n",
            "Epoch 423, BestLoss: 6.807070131902532e-10, Temperature 4.1844446406483364e-10, step_size 0.09630962923514166, test_acc: 1.0\n",
            "Epoch 424, BestLoss: 9.258617151638298e-10, Temperature 3.9752224086159194e-10, step_size 0.09629999827221815, test_acc: 1.0\n",
            "Epoch 425, BestLoss: 3.095031532684174e-10, Temperature 3.776461288185123e-10, step_size 0.09629036827239093, test_acc: 1.0\n",
            "Epoch 426, BestLoss: 1.9438034489868634e-10, Temperature 3.5876382237758666e-10, step_size 0.0962807392355637, test_acc: 1.0\n",
            "Epoch 427, BestLoss: 1.1999331793627786e-10, Temperature 3.408256312587073e-10, step_size 0.09627111116164014, test_acc: 1.0\n",
            "Epoch 428, BestLoss: 1.4741086621738917e-10, Temperature 3.2378434969577195e-10, step_size 0.09626148405052398, test_acc: 1.0\n",
            "Epoch 429, BestLoss: 1.0527502835672408e-10, Temperature 3.0759513221098333e-10, step_size 0.09625185790211893, test_acc: 1.0\n",
            "Epoch 430, BestLoss: 6.11150432348368e-11, Temperature 2.9221537560043415e-10, step_size 0.09624223271632872, test_acc: 1.0\n",
            "Epoch 431, BestLoss: 1.1922045332289044e-10, Temperature 2.776046068204124e-10, step_size 0.09623260849305708, test_acc: 1.0\n",
            "Epoch 432, BestLoss: 2.477277460142574e-10, Temperature 2.637243764793918e-10, step_size 0.09622298523220778, test_acc: 1.0\n",
            "Epoch 433, BestLoss: 4.171317184878982e-11, Temperature 2.505381576554222e-10, step_size 0.09621336293368456, test_acc: 1.0\n",
            "Epoch 434, BestLoss: 1.9609000664758795e-10, Temperature 2.3801124977265104e-10, step_size 0.09620374159739119, test_acc: 1.0\n",
            "Epoch 435, BestLoss: 2.1718087027100127e-10, Temperature 2.2611068728401848e-10, step_size 0.09619412122323145, test_acc: 1.0\n",
            "Epoch 436, BestLoss: 2.955982506874546e-10, Temperature 2.1480515291981755e-10, step_size 0.09618450181110913, test_acc: 1.0\n",
            "Epoch 437, BestLoss: 1.700212120540339e-10, Temperature 2.0406489527382667e-10, step_size 0.09617488336092803, test_acc: 1.0\n",
            "Epoch 438, BestLoss: 1.4100065252264494e-10, Temperature 1.9386165051013534e-10, step_size 0.09616526587259193, test_acc: 1.0\n",
            "Epoch 439, BestLoss: 7.524564224078672e-11, Temperature 1.8416856798462855e-10, step_size 0.09615564934600467, test_acc: 1.0\n",
            "Epoch 440, BestLoss: 3.5503281639251065e-11, Temperature 1.749601395853971e-10, step_size 0.09614603378107006, test_acc: 1.0\n",
            "Epoch 441, BestLoss: 9.659845423462528e-11, Temperature 1.6621213260612725e-10, step_size 0.09613641917769196, test_acc: 1.0\n",
            "Epoch 442, BestLoss: 1.8710005856864965e-10, Temperature 1.5790152597582087e-10, step_size 0.09612680553577418, test_acc: 1.0\n",
            "Epoch 443, BestLoss: 5.206761524592148e-11, Temperature 1.5000644967702982e-10, step_size 0.0961171928552206, test_acc: 1.0\n",
            "Epoch 444, BestLoss: 3.2936617082289747e-11, Temperature 1.4250612719317831e-10, step_size 0.09610758113593508, test_acc: 1.0\n",
            "Epoch 445, BestLoss: 3.7491217464230435e-11, Temperature 1.3538082083351939e-10, step_size 0.09609797037782149, test_acc: 1.0\n",
            "Epoch 446, BestLoss: 1.6136523772360208e-11, Temperature 1.2861177979184341e-10, step_size 0.09608836058078371, test_acc: 1.0\n",
            "Epoch 447, BestLoss: 2.2229775814453973e-11, Temperature 1.2218119080225124e-10, step_size 0.09607875174472563, test_acc: 1.0\n",
            "Epoch 448, BestLoss: 2.6977097321247153e-11, Temperature 1.1607213126213867e-10, step_size 0.09606914386955116, test_acc: 1.0\n",
            "Epoch 449, BestLoss: 2.3608050837278412e-11, Temperature 1.1026852469903174e-10, step_size 0.09605953695516421, test_acc: 1.0\n",
            "Epoch 450, BestLoss: 1.795550095472922e-11, Temperature 1.0475509846408014e-10, step_size 0.09604993100146869, test_acc: 1.0\n",
            "Epoch 451, BestLoss: 1.3314784590090735e-11, Temperature 9.951734354087613e-11, step_size 0.09604032600836855, test_acc: 1.0\n",
            "Epoch 452, BestLoss: 1.9642235324289768e-11, Temperature 9.454147636383232e-11, step_size 0.09603072197576772, test_acc: 1.0\n",
            "Epoch 453, BestLoss: 9.929096756091617e-11, Temperature 8.98144025456407e-11, step_size 0.09602111890357014, test_acc: 1.0\n",
            "Epoch 454, BestLoss: 7.898937253642554e-11, Temperature 8.532368241835867e-11, step_size 0.09601151679167978, test_acc: 1.0\n",
            "Epoch 455, BestLoss: 2.5117236245477406e-10, Temperature 8.105749829744073e-11, step_size 0.09600191564000062, test_acc: 1.0\n",
            "Epoch 456, BestLoss: 2.651075588158335e-11, Temperature 7.700462338256869e-11, step_size 0.09599231544843662, test_acc: 1.0\n",
            "Epoch 457, BestLoss: 6.582323417416524e-12, Temperature 7.315439221344025e-11, step_size 0.09598271621689178, test_acc: 1.0\n",
            "Epoch 458, BestLoss: 8.222704746540195e-12, Temperature 6.949667260276823e-11, step_size 0.0959731179452701, test_acc: 1.0\n",
            "Epoch 459, BestLoss: 4.997911280379043e-12, Temperature 6.602183897262981e-11, step_size 0.09596352063347557, test_acc: 1.0\n",
            "Epoch 460, BestLoss: 9.157721259506111e-11, Temperature 6.272074702399832e-11, step_size 0.09595392428141222, test_acc: 1.0\n",
            "Epoch 461, BestLoss: 1.8887217069564046e-11, Temperature 5.95847096727984e-11, step_size 0.09594432888898408, test_acc: 1.0\n",
            "Epoch 462, BestLoss: 2.510273561703134e-11, Temperature 5.660547418915848e-11, step_size 0.09593473445609518, test_acc: 1.0\n",
            "Epoch 463, BestLoss: 3.881258013595697e-11, Temperature 5.377520047970055e-11, step_size 0.09592514098264958, test_acc: 1.0\n",
            "Epoch 464, BestLoss: 1.2825803866088554e-10, Temperature 5.108644045571552e-11, step_size 0.09591554846855131, test_acc: 1.0\n",
            "Epoch 465, BestLoss: 1.837779149176306e-11, Temperature 4.8532118432929744e-11, step_size 0.09590595691370446, test_acc: 1.0\n",
            "Epoch 466, BestLoss: 1.4102573291368683e-11, Temperature 4.6105512511283255e-11, step_size 0.09589636631801308, test_acc: 1.0\n",
            "Epoch 467, BestLoss: 2.5294184082495678e-11, Temperature 4.380023688571909e-11, step_size 0.09588677668138128, test_acc: 1.0\n",
            "Epoch 468, BestLoss: 4.857679229904889e-11, Temperature 4.1610225041433134e-11, step_size 0.09587718800371314, test_acc: 1.0\n",
            "Epoch 469, BestLoss: 9.488639459803713e-11, Temperature 3.9529713789361476e-11, step_size 0.09586760028491277, test_acc: 1.0\n",
            "Epoch 470, BestLoss: 4.533064933746451e-11, Temperature 3.75532280998934e-11, step_size 0.09585801352488428, test_acc: 1.0\n",
            "Epoch 471, BestLoss: 4.4378091144494445e-11, Temperature 3.567556669489873e-11, step_size 0.09584842772353179, test_acc: 1.0\n",
            "Epoch 472, BestLoss: 9.632241345289457e-11, Temperature 3.389178836015379e-11, step_size 0.09583884288075943, test_acc: 1.0\n",
            "Epoch 473, BestLoss: 3.9621971604347954e-11, Temperature 3.21971989421461e-11, step_size 0.09582925899647135, test_acc: 1.0\n",
            "Epoch 474, BestLoss: 1.351013491370699e-11, Temperature 3.0587338995038793e-11, step_size 0.09581967607057171, test_acc: 1.0\n",
            "Epoch 475, BestLoss: 1.6987232279435317e-11, Temperature 2.9057972045286852e-11, step_size 0.09581009410296465, test_acc: 1.0\n",
            "Epoch 476, BestLoss: 2.1774969847035122e-12, Temperature 2.760507344302251e-11, step_size 0.09580051309355435, test_acc: 1.0\n",
            "Epoch 477, BestLoss: 3.050092804169263e-12, Temperature 2.6224819770871382e-11, step_size 0.095790933042245, test_acc: 1.0\n",
            "Epoch 478, BestLoss: 2.914673059802451e-12, Temperature 2.4913578782327812e-11, step_size 0.09578135394894077, test_acc: 1.0\n",
            "Epoch 479, BestLoss: 4.458666015939677e-12, Temperature 2.366789984321142e-11, step_size 0.09577177581354587, test_acc: 1.0\n",
            "Epoch 480, BestLoss: 5.000888851254855e-11, Temperature 2.248450485105085e-11, step_size 0.09576219863596452, test_acc: 1.0\n",
            "Epoch 481, BestLoss: 1.994255339372709e-12, Temperature 2.1360279608498306e-11, step_size 0.09575262241610093, test_acc: 1.0\n",
            "Epoch 482, BestLoss: 1.6069988413829357e-11, Temperature 2.029226562807339e-11, step_size 0.09574304715385931, test_acc: 1.0\n",
            "Epoch 483, BestLoss: 1.7632594158800809e-12, Temperature 1.927765234666972e-11, step_size 0.09573347284914392, test_acc: 1.0\n",
            "Epoch 484, BestLoss: 1.4698228421478617e-12, Temperature 1.8313769729336233e-11, step_size 0.09572389950185901, test_acc: 1.0\n",
            "Epoch 485, BestLoss: 3.750713833200662e-12, Temperature 1.739808124286942e-11, step_size 0.09571432711190883, test_acc: 1.0\n",
            "Epoch 486, BestLoss: 1.7669732916701888e-12, Temperature 1.652817718072595e-11, step_size 0.09570475567919765, test_acc: 1.0\n",
            "Epoch 487, BestLoss: 3.5860999445471953e-12, Temperature 1.5701768321689653e-11, step_size 0.09569518520362973, test_acc: 1.0\n",
            "Epoch 488, BestLoss: 4.820260771661228e-12, Temperature 1.4916679905605168e-11, step_size 0.09568561568510936, test_acc: 1.0\n",
            "Epoch 489, BestLoss: 1.2103424030709035e-11, Temperature 1.4170845910324909e-11, step_size 0.09567604712354084, test_acc: 1.0\n",
            "Epoch 490, BestLoss: 2.985401890571773e-11, Temperature 1.3462303614808662e-11, step_size 0.09566647951882849, test_acc: 1.0\n",
            "Epoch 491, BestLoss: 1.1970020979181122e-11, Temperature 1.2789188434068229e-11, step_size 0.09565691287087662, test_acc: 1.0\n",
            "Epoch 492, BestLoss: 8.812045047669024e-12, Temperature 1.2149729012364817e-11, step_size 0.09564734717958953, test_acc: 1.0\n",
            "Epoch 493, BestLoss: 2.5295694626283598e-11, Temperature 1.1542242561746576e-11, step_size 0.09563778244487157, test_acc: 1.0\n",
            "Epoch 494, BestLoss: 3.604177681008e-11, Temperature 1.0965130433659246e-11, step_size 0.09562821866662709, test_acc: 1.0\n",
            "Epoch 495, BestLoss: 1.6414353859003398e-11, Temperature 1.0416873911976284e-11, step_size 0.09561865584476043, test_acc: 1.0\n",
            "Epoch 496, BestLoss: 3.2073821807294397e-11, Temperature 9.896030216377469e-12, step_size 0.09560909397917595, test_acc: 1.0\n",
            "Epoch 497, BestLoss: 3.844188811592173e-12, Temperature 9.401228705558595e-12, step_size 0.09559953306977804, test_acc: 1.0\n",
            "Epoch 498, BestLoss: 9.932811300058283e-12, Temperature 8.931167270280664e-12, step_size 0.09558997311647106, test_acc: 1.0\n",
            "Epoch 499, BestLoss: 7.79097279894682e-12, Temperature 8.48460890676663e-12, step_size 0.09558041411915942, test_acc: 1.0\n",
            "Epoch 500, BestLoss: 1.0069635355583346e-11, Temperature 8.0603784614283e-12, step_size 0.09557085607774751, test_acc: 1.0\n",
            "Epoch 501, BestLoss: 4.3123913377391016e-12, Temperature 7.657359538356884e-12, step_size 0.09556129899213973, test_acc: 1.0\n",
            "Epoch 502, BestLoss: 4.947150650262851e-12, Temperature 7.274491561439039e-12, step_size 0.09555174286224052, test_acc: 1.0\n",
            "Epoch 503, BestLoss: 5.364340735431585e-12, Temperature 6.9107669833670865e-12, step_size 0.0955421876879543, test_acc: 1.0\n",
            "Epoch 504, BestLoss: 5.2481104559743194e-12, Temperature 6.565228634198732e-12, step_size 0.09553263346918552, test_acc: 1.0\n",
            "Epoch 505, BestLoss: 4.206962499384106e-12, Temperature 6.236967202488796e-12, step_size 0.09552308020583859, test_acc: 1.0\n",
            "Epoch 506, BestLoss: 2.463694933651498e-12, Temperature 5.925118842364355e-12, step_size 0.09551352789781802, test_acc: 1.0\n",
            "Epoch 507, BestLoss: 3.2952161641092463e-12, Temperature 5.6288629002461375e-12, step_size 0.09550397654502824, test_acc: 1.0\n",
            "Epoch 508, BestLoss: 1.6499866466586546e-12, Temperature 5.347419755233831e-12, step_size 0.09549442614737373, test_acc: 1.0\n",
            "Epoch 509, BestLoss: 8.10636350760128e-12, Temperature 5.080048767472139e-12, step_size 0.095484876704759, test_acc: 1.0\n",
            "Epoch 510, BestLoss: 1.905166540646742e-12, Temperature 4.826046329098531e-12, step_size 0.09547532821708853, test_acc: 1.0\n",
            "Epoch 511, BestLoss: 2.0240785349941278e-12, Temperature 4.584744012643605e-12, step_size 0.09546578068426682, test_acc: 1.0\n",
            "Epoch 512, BestLoss: 6.673017021258428e-12, Temperature 4.355506812011424e-12, step_size 0.0954562341061984, test_acc: 1.0\n",
            "Epoch 513, BestLoss: 8.653381850448236e-12, Temperature 4.137731471410853e-12, step_size 0.09544668848278778, test_acc: 1.0\n",
            "Epoch 514, BestLoss: 2.1013461597871666e-12, Temperature 3.93084489784031e-12, step_size 0.0954371438139395, test_acc: 1.0\n",
            "Epoch 515, BestLoss: 1.985453779717955e-12, Temperature 3.734302652948294e-12, step_size 0.09542760009955811, test_acc: 1.0\n",
            "Epoch 516, BestLoss: 1.9211242350443437e-12, Temperature 3.547587520300879e-12, step_size 0.09541805733954815, test_acc: 1.0\n",
            "Epoch 517, BestLoss: 3.1903524517412473e-12, Temperature 3.370208144285835e-12, step_size 0.0954085155338142, test_acc: 1.0\n",
            "Epoch 518, BestLoss: 9.011814946470552e-12, Temperature 3.201697737071543e-12, step_size 0.09539897468226081, test_acc: 1.0\n",
            "Epoch 519, BestLoss: 2.679428956655173e-12, Temperature 3.0416128502179658e-12, step_size 0.09538943478479259, test_acc: 1.0\n",
            "Epoch 520, BestLoss: 1.3588503343769857e-12, Temperature 2.8895322077070673e-12, step_size 0.09537989584131411, test_acc: 1.0\n",
            "Epoch 521, BestLoss: 2.838726698397687e-12, Temperature 2.745055597321714e-12, step_size 0.09537035785172998, test_acc: 1.0\n",
            "Epoch 522, BestLoss: 5.472349940996337e-12, Temperature 2.607802817455628e-12, step_size 0.09536082081594481, test_acc: 1.0\n",
            "Epoch 523, BestLoss: 2.027360519666898e-12, Temperature 2.4774126765828464e-12, step_size 0.09535128473386321, test_acc: 1.0\n",
            "Epoch 524, BestLoss: 1.829100890240794e-12, Temperature 2.353542042753704e-12, step_size 0.09534174960538983, test_acc: 1.0\n",
            "Epoch 525, BestLoss: 5.07619708000116e-12, Temperature 2.2358649406160186e-12, step_size 0.09533221543042929, test_acc: 1.0\n",
            "Epoch 526, BestLoss: 1.5908846749947754e-12, Temperature 2.1240716935852176e-12, step_size 0.09532268220888625, test_acc: 1.0\n",
            "Epoch 527, BestLoss: 2.034351672163209e-12, Temperature 2.0178681089059567e-12, step_size 0.09531314994066536, test_acc: 1.0\n",
            "Epoch 528, BestLoss: 8.74689910455868e-13, Temperature 1.9169747034606587e-12, step_size 0.0953036186256713, test_acc: 1.0\n",
            "Epoch 529, BestLoss: 2.353664296646626e-13, Temperature 1.8211259682876257e-12, step_size 0.09529408826380874, test_acc: 1.0\n",
            "Epoch 530, BestLoss: 4.75946147206925e-13, Temperature 1.7300696698732444e-12, step_size 0.09528455885498235, test_acc: 1.0\n",
            "Epoch 531, BestLoss: 2.7833421005633716e-13, Temperature 1.643566186379582e-12, step_size 0.09527503039909686, test_acc: 1.0\n",
            "Epoch 532, BestLoss: 1.100803367633766e-12, Temperature 1.5613878770606029e-12, step_size 0.09526550289605695, test_acc: 1.0\n",
            "Epoch 533, BestLoss: 1.2277158468739659e-12, Temperature 1.4833184832075727e-12, step_size 0.09525597634576735, test_acc: 1.0\n",
            "Epoch 534, BestLoss: 9.072639652243181e-13, Temperature 1.409152559047194e-12, step_size 0.09524645074813277, test_acc: 1.0\n",
            "Epoch 535, BestLoss: 1.0852322764154794e-12, Temperature 1.3386949310948342e-12, step_size 0.09523692610305796, test_acc: 1.0\n",
            "Epoch 536, BestLoss: 1.4512596717005696e-12, Temperature 1.2717601845400924e-12, step_size 0.09522740241044766, test_acc: 1.0\n",
            "Epoch 537, BestLoss: 2.5211869711518406e-12, Temperature 1.2081721753130878e-12, step_size 0.09521787967020662, test_acc: 1.0\n",
            "Epoch 538, BestLoss: 1.546940006032082e-12, Temperature 1.1477635665474333e-12, step_size 0.0952083578822396, test_acc: 1.0\n",
            "Epoch 539, BestLoss: 1.8081513402824745e-12, Temperature 1.0903753882200615e-12, step_size 0.09519883704645137, test_acc: 1.0\n",
            "Epoch 540, BestLoss: 1.8855596764127503e-12, Temperature 1.0358566188090585e-12, step_size 0.09518931716274671, test_acc: 1.0\n",
            "Epoch 541, BestLoss: 3.3004944586864554e-12, Temperature 9.840637878686054e-13, step_size 0.09517979823103044, test_acc: 1.0\n",
            "Epoch 542, BestLoss: 2.686861899510543e-12, Temperature 9.34860598475175e-13, step_size 0.09517028025120734, test_acc: 1.0\n",
            "Epoch 543, BestLoss: 2.778270345198192e-12, Temperature 8.881175685514162e-13, step_size 0.09516076322318222, test_acc: 1.0\n",
            "Epoch 544, BestLoss: 2.893135447072651e-12, Temperature 8.437116901238454e-13, step_size 0.09515124714685991, test_acc: 1.0\n",
            "Epoch 545, BestLoss: 2.696044588456234e-12, Temperature 8.015261056176531e-13, step_size 0.09514173202214522, test_acc: 1.0\n",
            "Epoch 546, BestLoss: 2.485768297541046e-12, Temperature 7.614498003367704e-13, step_size 0.095132217848943, test_acc: 1.0\n",
            "Epoch 547, BestLoss: 1.952662582625184e-12, Temperature 7.233773103199318e-13, step_size 0.0951227046271581, test_acc: 1.0\n",
            "Epoch 548, BestLoss: 1.916201582861205e-12, Temperature 6.872084448039352e-13, step_size 0.0951131923566954, test_acc: 1.0\n",
            "Epoch 549, BestLoss: 1.8398245629630797e-12, Temperature 6.528480225637384e-13, step_size 0.09510368103745973, test_acc: 1.0\n",
            "Epoch 550, BestLoss: 1.98855608313533e-12, Temperature 6.202056214355515e-13, step_size 0.09509417066935598, test_acc: 1.0\n",
            "Epoch 551, BestLoss: 2.3622748899841165e-12, Temperature 5.891953403637739e-13, step_size 0.09508466125228904, test_acc: 1.0\n",
            "Epoch 552, BestLoss: 1.935223599933154e-12, Temperature 5.597355733455851e-13, step_size 0.09507515278616381, test_acc: 1.0\n",
            "Epoch 553, BestLoss: 2.196236206613794e-12, Temperature 5.317487946783058e-13, step_size 0.0950656452708852, test_acc: 1.0\n",
            "Epoch 554, BestLoss: 1.2595770854196189e-12, Temperature 5.051613549443905e-13, step_size 0.09505613870635811, test_acc: 1.0\n",
            "Epoch 555, BestLoss: 8.562717389369217e-13, Temperature 4.79903287197171e-13, step_size 0.09504663309248748, test_acc: 1.0\n",
            "Epoch 556, BestLoss: 1.1922938533798192e-12, Temperature 4.559081228373124e-13, step_size 0.09503712842917823, test_acc: 1.0\n",
            "Epoch 557, BestLoss: 9.631571651618237e-13, Temperature 4.3311271669544675e-13, step_size 0.09502762471633532, test_acc: 1.0\n",
            "Epoch 558, BestLoss: 1.2056013350673123e-12, Temperature 4.114570808606744e-13, step_size 0.09501812195386368, test_acc: 1.0\n",
            "Epoch 559, BestLoss: 1.2198850592381762e-12, Temperature 3.9088422681764065e-13, step_size 0.09500862014166829, test_acc: 1.0\n",
            "Epoch 560, BestLoss: 1.1308509700223374e-12, Temperature 3.713400154767586e-13, step_size 0.09499911927965413, test_acc: 1.0\n",
            "Epoch 561, BestLoss: 7.850210783444381e-13, Temperature 3.5277301470292065e-13, step_size 0.09498961936772617, test_acc: 1.0\n",
            "Epoch 562, BestLoss: 8.980553988951811e-13, Temperature 3.351343639677746e-13, step_size 0.09498012040578939, test_acc: 1.0\n",
            "Epoch 563, BestLoss: 1.1390513756200899e-12, Temperature 3.1837764576938583e-13, step_size 0.09497062239374882, test_acc: 1.0\n",
            "Epoch 564, BestLoss: 8.714995529722745e-13, Temperature 3.024587634809165e-13, step_size 0.09496112533150944, test_acc: 1.0\n",
            "Epoch 565, BestLoss: 5.98773883567647e-13, Temperature 2.8733582530687066e-13, step_size 0.09495162921897629, test_acc: 1.0\n",
            "Epoch 566, BestLoss: 7.912221014748551e-13, Temperature 2.729690340415271e-13, step_size 0.0949421340560544, test_acc: 1.0\n",
            "Epoch 567, BestLoss: 6.418751235476156e-13, Temperature 2.593205823394507e-13, step_size 0.0949326398426488, test_acc: 1.0\n",
            "Epoch 568, BestLoss: 6.408307754754474e-13, Temperature 2.463545532224782e-13, step_size 0.09492314657866453, test_acc: 1.0\n",
            "Epoch 569, BestLoss: 8.089339059438846e-13, Temperature 2.3403682556135424e-13, step_size 0.09491365426400666, test_acc: 1.0\n",
            "Epoch 570, BestLoss: 8.307876137917298e-13, Temperature 2.2233498428328652e-13, step_size 0.09490416289858027, test_acc: 1.0\n",
            "Epoch 571, BestLoss: 1.514552825031697e-12, Temperature 2.112182350691222e-13, step_size 0.09489467248229042, test_acc: 1.0\n",
            "Epoch 572, BestLoss: 5.876459506383439e-13, Temperature 2.0065732331566607e-13, step_size 0.09488518301504219, test_acc: 1.0\n",
            "Epoch 573, BestLoss: 5.752820555990038e-13, Temperature 1.9062445714988276e-13, step_size 0.09487569449674069, test_acc: 1.0\n",
            "Epoch 574, BestLoss: 8.097437994102893e-13, Temperature 1.810932342923886e-13, step_size 0.09486620692729102, test_acc: 1.0\n",
            "Epoch 575, BestLoss: 3.3342584859249307e-13, Temperature 1.7203857257776917e-13, step_size 0.0948567203065983, test_acc: 1.0\n",
            "Epoch 576, BestLoss: 4.081256427313584e-13, Temperature 1.634366439488807e-13, step_size 0.09484723463456764, test_acc: 1.0\n",
            "Epoch 577, BestLoss: 6.176650558822523e-13, Temperature 1.5526481175143667e-13, step_size 0.09483774991110418, test_acc: 1.0\n",
            "Epoch 578, BestLoss: 2.3713782302503696e-13, Temperature 1.4750157116386484e-13, step_size 0.09482826613611307, test_acc: 1.0\n",
            "Epoch 579, BestLoss: 1.680724926014936e-13, Temperature 1.4012649260567158e-13, step_size 0.09481878330949946, test_acc: 1.0\n",
            "Epoch 580, BestLoss: 2.2211173923321129e-13, Temperature 1.33120167975388e-13, step_size 0.0948093014311685, test_acc: 1.0\n",
            "Epoch 581, BestLoss: 1.9593947366727935e-13, Temperature 1.264641595766186e-13, step_size 0.09479982050102538, test_acc: 1.0\n",
            "Epoch 582, BestLoss: 3.921511309305811e-13, Temperature 1.2014095159778768e-13, step_size 0.09479034051897528, test_acc: 1.0\n",
            "Epoch 583, BestLoss: 5.204250828409887e-13, Temperature 1.141339040178983e-13, step_size 0.09478086148492339, test_acc: 1.0\n",
            "Epoch 584, BestLoss: 4.2732976172148286e-13, Temperature 1.0842720881700338e-13, step_size 0.0947713833987749, test_acc: 1.0\n",
            "Epoch 585, BestLoss: 4.331637096916643e-13, Temperature 1.0300584837615321e-13, step_size 0.09476190626043503, test_acc: 1.0\n",
            "Epoch 586, BestLoss: 3.3333300481516574e-13, Temperature 9.785555595734555e-14, step_size 0.09475243006980899, test_acc: 1.0\n",
            "Epoch 587, BestLoss: 4.376995958174351e-13, Temperature 9.296277815947827e-14, step_size 0.094742954826802, test_acc: 1.0\n",
            "Epoch 588, BestLoss: 3.789951168635391e-13, Temperature 8.831463925150435e-14, step_size 0.09473348053131933, test_acc: 1.0\n",
            "Epoch 589, BestLoss: 2.8494804093789646e-13, Temperature 8.389890728892913e-14, step_size 0.09472400718326619, test_acc: 1.0\n",
            "Epoch 590, BestLoss: 2.72780486318727e-13, Temperature 7.970396192448267e-14, step_size 0.09471453478254786, test_acc: 1.0\n",
            "Epoch 591, BestLoss: 6.957895600647808e-13, Temperature 7.571876382825853e-14, step_size 0.09470506332906961, test_acc: 1.0\n",
            "Epoch 592, BestLoss: 3.3897316921001893e-13, Temperature 7.19328256368456e-14, step_size 0.09469559282273671, test_acc: 1.0\n",
            "Epoch 593, BestLoss: 3.3990745449260386e-13, Temperature 6.833618435500332e-14, step_size 0.09468612326345444, test_acc: 1.0\n",
            "Epoch 594, BestLoss: 1.4847784754223333e-13, Temperature 6.491937513725315e-14, step_size 0.0946766546511281, test_acc: 1.0\n",
            "Epoch 595, BestLoss: 1.299924929304374e-13, Temperature 6.167340638039049e-14, step_size 0.09466718698566298, test_acc: 1.0\n",
            "Epoch 596, BestLoss: 8.479538998312831e-14, Temperature 5.858973606137097e-14, step_size 0.09465772026696442, test_acc: 1.0\n",
            "Epoch 597, BestLoss: 8.28595222404891e-14, Temperature 5.566024925830242e-14, step_size 0.09464825449493773, test_acc: 1.0\n",
            "Epoch 598, BestLoss: 8.573599932228485e-14, Temperature 5.2877236795387294e-14, step_size 0.09463878966948824, test_acc: 1.0\n",
            "Epoch 599, BestLoss: 4.8729248459819676e-14, Temperature 5.0233374955617924e-14, step_size 0.0946293257905213, test_acc: 1.0\n",
            "Epoch 600, BestLoss: 5.720813086237321e-14, Temperature 4.7721706207837024e-14, step_size 0.09461986285794224, test_acc: 1.0\n",
            "Epoch 601, BestLoss: 1.787234492665752e-13, Temperature 4.533562089744517e-14, step_size 0.09461040087165645, test_acc: 1.0\n",
            "Epoch 602, BestLoss: 1.0770943992507576e-13, Temperature 4.306883985257291e-14, step_size 0.0946009398315693, test_acc: 1.0\n",
            "Epoch 603, BestLoss: 1.2488060353405183e-13, Temperature 4.0915397859944265e-14, step_size 0.09459147973758614, test_acc: 1.0\n",
            "Epoch 604, BestLoss: 8.522522959838203e-14, Temperature 3.886962796694705e-14, step_size 0.09458202058961238, test_acc: 1.0\n",
            "Epoch 605, BestLoss: 7.460107933213697e-14, Temperature 3.6926146568599694e-14, step_size 0.09457256238755342, test_acc: 1.0\n",
            "Epoch 606, BestLoss: 7.465231649282916e-14, Temperature 3.507983924016971e-14, step_size 0.09456310513131466, test_acc: 1.0\n",
            "Epoch 607, BestLoss: 6.978863485802096e-14, Temperature 3.332584727816122e-14, step_size 0.09455364882080153, test_acc: 1.0\n",
            "Epoch 608, BestLoss: 1.7322908338385417e-13, Temperature 3.165955491425316e-14, step_size 0.09454419345591945, test_acc: 1.0\n",
            "Epoch 609, BestLoss: 1.4039327170669865e-13, Temperature 3.00765771685405e-14, step_size 0.09453473903657386, test_acc: 1.0\n",
            "Epoch 610, BestLoss: 1.4494715998941466e-13, Temperature 2.8572748310113474e-14, step_size 0.0945252855626702, test_acc: 1.0\n",
            "Epoch 611, BestLoss: 7.599805521855035e-14, Temperature 2.7144110894607798e-14, step_size 0.09451583303411394, test_acc: 1.0\n",
            "Epoch 612, BestLoss: 1.085510458264881e-13, Temperature 2.5786905349877408e-14, step_size 0.09450638145081053, test_acc: 1.0\n",
            "Epoch 613, BestLoss: 1.0780775574365259e-13, Temperature 2.4497560082383537e-14, step_size 0.09449693081266544, test_acc: 1.0\n",
            "Epoch 614, BestLoss: 1.093987366638064e-13, Temperature 2.327268207826436e-14, step_size 0.09448748111958417, test_acc: 1.0\n",
            "Epoch 615, BestLoss: 1.2151040218365507e-13, Temperature 2.210904797435114e-14, step_size 0.09447803237147222, test_acc: 1.0\n",
            "Epoch 616, BestLoss: 5.359471341543737e-14, Temperature 2.1003595575633584e-14, step_size 0.09446858456823506, test_acc: 1.0\n",
            "Epoch 617, BestLoss: 1.3452837230594876e-13, Temperature 1.9953415796851905e-14, step_size 0.09445913770977825, test_acc: 1.0\n",
            "Epoch 618, BestLoss: 1.394404615770439e-13, Temperature 1.8955745007009308e-14, step_size 0.09444969179600728, test_acc: 1.0\n",
            "Epoch 619, BestLoss: 1.08935557075113e-13, Temperature 1.8007957756658842e-14, step_size 0.09444024682682768, test_acc: 1.0\n",
            "Epoch 620, BestLoss: 9.689076278457338e-14, Temperature 1.71075598688259e-14, step_size 0.094430802802145, test_acc: 1.0\n",
            "Epoch 621, BestLoss: 6.453973246485995e-14, Temperature 1.6252181875384604e-14, step_size 0.09442135972186479, test_acc: 1.0\n",
            "Epoch 622, BestLoss: 5.890816200443404e-14, Temperature 1.5439572781615372e-14, step_size 0.0944119175858926, test_acc: 1.0\n",
            "Epoch 623, BestLoss: 4.0029940780859e-14, Temperature 1.4667594142534604e-14, step_size 0.094402476394134, test_acc: 1.0\n",
            "Epoch 624, BestLoss: 5.865680520260566e-14, Temperature 1.3934214435407872e-14, step_size 0.0943930361464946, test_acc: 1.0\n",
            "Epoch 625, BestLoss: 4.40911185296318e-14, Temperature 1.3237503713637477e-14, step_size 0.09438359684287995, test_acc: 1.0\n",
            "Epoch 626, BestLoss: 3.9444013924572184e-14, Temperature 1.2575628527955604e-14, step_size 0.09437415848319566, test_acc: 1.0\n",
            "Epoch 627, BestLoss: 4.651531242434815e-14, Temperature 1.1946847101557822e-14, step_size 0.09436472106734735, test_acc: 1.0\n",
            "Epoch 628, BestLoss: 5.6711936668738244e-14, Temperature 1.134950474647993e-14, step_size 0.09435528459524062, test_acc: 1.0\n",
            "Epoch 629, BestLoss: 3.0364133889580984e-14, Temperature 1.0782029509155933e-14, step_size 0.09434584906678109, test_acc: 1.0\n",
            "Epoch 630, BestLoss: 4.0109206507753247e-14, Temperature 1.0242928033698136e-14, step_size 0.09433641448187441, test_acc: 1.0\n",
            "Epoch 631, BestLoss: 2.6672403644780444e-14, Temperature 9.73078163201323e-15, step_size 0.09432698084042622, test_acc: 1.0\n",
            "Epoch 632, BestLoss: 2.506940464851397e-14, Temperature 9.244242550412568e-15, step_size 0.09431754814234218, test_acc: 1.0\n",
            "Epoch 633, BestLoss: 1.8907872186995975e-14, Temperature 8.782030422891939e-15, step_size 0.09430811638752795, test_acc: 1.0\n",
            "Epoch 634, BestLoss: 1.6609942046464165e-14, Temperature 8.342928901747341e-15, step_size 0.0942986855758892, test_acc: 1.0\n",
            "Epoch 635, BestLoss: 3.323622819912248e-14, Temperature 7.925782456659974e-15, step_size 0.09428925570733161, test_acc: 1.0\n",
            "Epoch 636, BestLoss: 1.3803332087732545e-14, Temperature 7.529493333826974e-15, step_size 0.09427982678176088, test_acc: 1.0\n",
            "Epoch 637, BestLoss: 2.6865198936512585e-14, Temperature 7.153018667135625e-15, step_size 0.0942703987990827, test_acc: 1.0\n",
            "Epoch 638, BestLoss: 1.2656265936278563e-14, Temperature 6.795367733778843e-15, step_size 0.09426097175920278, test_acc: 1.0\n",
            "Epoch 639, BestLoss: 1.027394422105337e-14, Temperature 6.4555993470899e-15, step_size 0.09425154566202687, test_acc: 1.0\n",
            "Epoch 640, BestLoss: 1.1175589872558452e-14, Temperature 6.132819379735405e-15, step_size 0.09424212050746067, test_acc: 1.0\n",
            "Epoch 641, BestLoss: 1.2470572800156187e-14, Temperature 5.826178410748634e-15, step_size 0.09423269629540992, test_acc: 1.0\n",
            "Epoch 642, BestLoss: 1.034082981759359e-14, Temperature 5.5348694902112026e-15, step_size 0.09422327302578039, test_acc: 1.0\n",
            "Epoch 643, BestLoss: 1.027474732497169e-14, Temperature 5.258126015700642e-15, step_size 0.0942138506984778, test_acc: 1.0\n",
            "Epoch 644, BestLoss: 9.066819502039763e-15, Temperature 4.995219714915609e-15, step_size 0.09420442931340796, test_acc: 1.0\n",
            "Epoch 645, BestLoss: 1.4315214191816135e-14, Temperature 4.745458729169829e-15, step_size 0.09419500887047662, test_acc: 1.0\n",
            "Epoch 646, BestLoss: 1.245833951310996e-14, Temperature 4.508185792711337e-15, step_size 0.09418558936958957, test_acc: 1.0\n",
            "Epoch 647, BestLoss: 1.506383628998236e-14, Temperature 4.282776503075771e-15, step_size 0.0941761708106526, test_acc: 1.0\n",
            "Epoch 648, BestLoss: 1.1662085353694192e-14, Temperature 4.068637677921982e-15, step_size 0.09416675319357154, test_acc: 1.0\n",
            "Epoch 649, BestLoss: 2.3010155737976993e-14, Temperature 3.8652057940258825e-15, step_size 0.09415733651825219, test_acc: 1.0\n",
            "Epoch 650, BestLoss: 2.371285002395876e-14, Temperature 3.6719455043245884e-15, step_size 0.09414792078460037, test_acc: 1.0\n",
            "Epoch 651, BestLoss: 1.2172372629047705e-14, Temperature 3.4883482291083587e-15, step_size 0.09413850599252191, test_acc: 1.0\n",
            "Epoch 652, BestLoss: 1.1506627428044093e-14, Temperature 3.3139308176529404e-15, step_size 0.09412909214192265, test_acc: 1.0\n",
            "Epoch 653, BestLoss: 1.3481604620118385e-14, Temperature 3.1482342767702933e-15, step_size 0.09411967923270846, test_acc: 1.0\n",
            "Epoch 654, BestLoss: 9.56079391687428e-15, Temperature 2.9908225629317786e-15, step_size 0.09411026726478519, test_acc: 1.0\n",
            "Epoch 655, BestLoss: 5.485434148161382e-15, Temperature 2.8412814347851897e-15, step_size 0.09410085623805871, test_acc: 1.0\n",
            "Epoch 656, BestLoss: 5.627114211812896e-15, Temperature 2.69921736304593e-15, step_size 0.09409144615243491, test_acc: 1.0\n",
            "Epoch 657, BestLoss: 5.24031442959806e-15, Temperature 2.5642564948936334e-15, step_size 0.09408203700781967, test_acc: 1.0\n",
            "Epoch 658, BestLoss: 6.694550263182994e-15, Temperature 2.4360436701489517e-15, step_size 0.09407262880411889, test_acc: 1.0\n",
            "Epoch 659, BestLoss: 2.95947451662008e-15, Temperature 2.314241486641504e-15, step_size 0.09406322154123847, test_acc: 1.0\n",
            "Epoch 660, BestLoss: 2.5252867396980456e-15, Temperature 2.1985294123094285e-15, step_size 0.09405381521908435, test_acc: 1.0\n",
            "Epoch 661, BestLoss: 2.696486573337958e-15, Temperature 2.088602941693957e-15, step_size 0.09404440983756245, test_acc: 1.0\n",
            "Epoch 662, BestLoss: 6.224295452495828e-15, Temperature 1.984172794609259e-15, step_size 0.09403500539657869, test_acc: 1.0\n",
            "Epoch 663, BestLoss: 4.004750504647032e-15, Temperature 1.884964154878796e-15, step_size 0.09402560189603903, test_acc: 1.0\n",
            "Epoch 664, BestLoss: 3.0932282896813853e-15, Temperature 1.790715947134856e-15, step_size 0.09401619933584943, test_acc: 1.0\n",
            "Epoch 665, BestLoss: 2.9698538502376034e-15, Temperature 1.701180149778113e-15, step_size 0.09400679771591584, test_acc: 1.0\n",
            "Epoch 666, BestLoss: 3.1841667735918017e-15, Temperature 1.6161211422892074e-15, step_size 0.09399739703614425, test_acc: 1.0\n",
            "Epoch 667, BestLoss: 4.360717617997385e-15, Temperature 1.535315085174747e-15, step_size 0.09398799729644064, test_acc: 1.0\n",
            "Epoch 668, BestLoss: 3.0279419331572613e-15, Temperature 1.4585493309160096e-15, step_size 0.093978598496711, test_acc: 1.0\n",
            "Epoch 669, BestLoss: 2.959463536218192e-15, Temperature 1.385621864370209e-15, step_size 0.09396920063686133, test_acc: 1.0\n",
            "Epoch 670, BestLoss: 2.8186452522019784e-15, Temperature 1.3163407711516985e-15, step_size 0.09395980371679764, test_acc: 1.0\n",
            "Epoch 671, BestLoss: 3.865621572038499e-15, Temperature 1.2505237325941136e-15, step_size 0.09395040773642596, test_acc: 1.0\n",
            "Epoch 672, BestLoss: 3.0958549438109763e-15, Temperature 1.187997545964408e-15, step_size 0.09394101269565232, test_acc: 1.0\n",
            "Epoch 673, BestLoss: 4.876155844892064e-15, Temperature 1.1285976686661875e-15, step_size 0.09393161859438276, test_acc: 1.0\n",
            "Epoch 674, BestLoss: 5.495585427680944e-15, Temperature 1.072167785232878e-15, step_size 0.09392222543252332, test_acc: 1.0\n",
            "Epoch 675, BestLoss: 5.0758696202216615e-15, Temperature 1.0185593959712341e-15, step_size 0.09391283320998008, test_acc: 1.0\n",
            "Epoch 676, BestLoss: 6.918802485252001e-15, Temperature 9.676314261726724e-16, step_size 0.09390344192665907, test_acc: 1.0\n",
            "Epoch 677, BestLoss: 5.5818502751651714e-15, Temperature 9.192498548640388e-16, step_size 0.09389405158246641, test_acc: 1.0\n",
            "Epoch 678, BestLoss: 5.6183248425624664e-15, Temperature 8.732873621208368e-16, step_size 0.09388466217730818, test_acc: 1.0\n",
            "Epoch 679, BestLoss: 4.972145356950175e-15, Temperature 8.296229940147949e-16, step_size 0.09387527371109045, test_acc: 1.0\n",
            "Epoch 680, BestLoss: 4.64798405899855e-15, Temperature 7.881418443140551e-16, step_size 0.09386588618371934, test_acc: 1.0\n",
            "Epoch 681, BestLoss: 5.4443329799031225e-15, Temperature 7.487347520983522e-16, step_size 0.09385649959510096, test_acc: 1.0\n",
            "Epoch 682, BestLoss: 3.346582196000017e-15, Temperature 7.112980144934346e-16, step_size 0.09384711394514146, test_acc: 1.0\n",
            "Epoch 683, BestLoss: 3.038985045459459e-15, Temperature 6.757331137687628e-16, step_size 0.09383772923374695, test_acc: 1.0\n",
            "Epoch 684, BestLoss: 2.635083807075783e-15, Temperature 6.419464580803247e-16, step_size 0.09382834546082357, test_acc: 1.0\n",
            "Epoch 685, BestLoss: 2.0726668756612934e-15, Temperature 6.098491351763084e-16, step_size 0.09381896262627748, test_acc: 1.0\n",
            "Epoch 686, BestLoss: 2.8995728888228252e-15, Temperature 5.79356678417493e-16, step_size 0.09380958073001486, test_acc: 1.0\n",
            "Epoch 687, BestLoss: 2.495200830270987e-15, Temperature 5.503888444966183e-16, step_size 0.09380019977194186, test_acc: 1.0\n",
            "Epoch 688, BestLoss: 1.1951157240445111e-15, Temperature 5.228694022717873e-16, step_size 0.09379081975196467, test_acc: 1.0\n",
            "Epoch 689, BestLoss: 1.1548110441667271e-15, Temperature 4.967259321581979e-16, step_size 0.09378144066998947, test_acc: 1.0\n",
            "Epoch 690, BestLoss: 7.999769340134177e-16, Temperature 4.71889635550288e-16, step_size 0.09377206252592248, test_acc: 1.0\n",
            "Epoch 691, BestLoss: 1.3133446832484792e-15, Temperature 4.482951537727736e-16, step_size 0.09376268531966989, test_acc: 1.0\n",
            "Epoch 692, BestLoss: 9.307582913526131e-16, Temperature 4.258803960841349e-16, step_size 0.09375330905113792, test_acc: 1.0\n",
            "Epoch 693, BestLoss: 5.843012867514377e-16, Temperature 4.0458637627992815e-16, step_size 0.0937439337202328, test_acc: 1.0\n",
            "Epoch 694, BestLoss: 5.175765631631025e-16, Temperature 3.843570574659317e-16, step_size 0.09373455932686078, test_acc: 1.0\n",
            "Epoch 695, BestLoss: 1.0634066502902378e-15, Temperature 3.651392045926351e-16, step_size 0.0937251858709281, test_acc: 1.0\n",
            "Epoch 696, BestLoss: 5.590551302786569e-16, Temperature 3.468822443630033e-16, step_size 0.093715813352341, test_acc: 1.0\n",
            "Epoch 697, BestLoss: 5.414185614072894e-16, Temperature 3.295381321448531e-16, step_size 0.09370644177100576, test_acc: 1.0\n",
            "Epoch 698, BestLoss: 3.9894928589097036e-16, Temperature 3.1306122553761046e-16, step_size 0.09369707112682867, test_acc: 1.0\n",
            "Epoch 699, BestLoss: 7.419279292508074e-16, Temperature 2.974081642607299e-16, step_size 0.09368770141971598, test_acc: 1.0\n",
            "Epoch 700, BestLoss: 2.9537783182730086e-16, Temperature 2.8253775604769343e-16, step_size 0.09367833264957401, test_acc: 1.0\n",
            "Epoch 701, BestLoss: 4.911680302828267e-16, Temperature 2.6841086824530873e-16, step_size 0.09366896481630906, test_acc: 1.0\n",
            "Epoch 702, BestLoss: 4.529867711038457e-16, Temperature 2.549903248330433e-16, step_size 0.09365959791982742, test_acc: 1.0\n",
            "Epoch 703, BestLoss: 1.7657989131763986e-16, Temperature 2.422408085913911e-16, step_size 0.09365023196003544, test_acc: 1.0\n",
            "Epoch 704, BestLoss: 3.0752310764419633e-16, Temperature 2.3012876816182154e-16, step_size 0.09364086693683944, test_acc: 1.0\n",
            "Epoch 705, BestLoss: 3.3197986694414405e-16, Temperature 2.1862232975373046e-16, step_size 0.09363150285014576, test_acc: 1.0\n",
            "Epoch 706, BestLoss: 1.5610016381074797e-16, Temperature 2.0769121326604392e-16, step_size 0.09362213969986075, test_acc: 1.0\n",
            "Epoch 707, BestLoss: 1.7388567868357174e-16, Temperature 1.9730665260274172e-16, step_size 0.09361277748589077, test_acc: 1.0\n",
            "Epoch 708, BestLoss: 1.5433339942308806e-16, Temperature 1.8744131997260461e-16, step_size 0.09360341620814218, test_acc: 1.0\n",
            "Epoch 709, BestLoss: 1.3399023314203054e-16, Temperature 1.780692539739744e-16, step_size 0.09359405586652136, test_acc: 1.0\n",
            "Epoch 710, BestLoss: 1.4932764221746984e-16, Temperature 1.6916579127527566e-16, step_size 0.09358469646093472, test_acc: 1.0\n",
            "Epoch 711, BestLoss: 2.2008056586109324e-16, Temperature 1.6070750171151186e-16, step_size 0.09357533799128863, test_acc: 1.0\n",
            "Epoch 712, BestLoss: 2.3970917304441895e-16, Temperature 1.5267212662593626e-16, step_size 0.0935659804574895, test_acc: 1.0\n",
            "Epoch 713, BestLoss: 3.1062005908100076e-16, Temperature 1.4503852029463943e-16, step_size 0.09355662385944374, test_acc: 1.0\n",
            "Epoch 714, BestLoss: 3.8851898737820814e-16, Temperature 1.3778659427990744e-16, step_size 0.0935472681970578, test_acc: 1.0\n",
            "Epoch 715, BestLoss: 2.9488956223097346e-16, Temperature 1.3089726456591207e-16, step_size 0.0935379134702381, test_acc: 1.0\n",
            "Epoch 716, BestLoss: 2.8771796170298956e-16, Temperature 1.2435240133761646e-16, step_size 0.09352855967889108, test_acc: 1.0\n",
            "Epoch 717, BestLoss: 4.345325483381793e-16, Temperature 1.1813478127073564e-16, step_size 0.0935192068229232, test_acc: 1.0\n",
            "Epoch 718, BestLoss: 2.4154142523256977e-16, Temperature 1.1222804220719885e-16, step_size 0.09350985490224091, test_acc: 1.0\n",
            "Epoch 719, BestLoss: 3.0854356496459117e-16, Temperature 1.0661664009683891e-16, step_size 0.09350050391675069, test_acc: 1.0\n",
            "Epoch 720, BestLoss: 1.146363474548395e-16, Temperature 1.0128580809199696e-16, step_size 0.09349115386635902, test_acc: 1.0\n",
            "Epoch 721, BestLoss: 2.083489908030408e-16, Temperature 9.622151768739711e-17, step_size 0.09348180475097238, test_acc: 1.0\n",
            "Epoch 722, BestLoss: 3.318502914728796e-16, Temperature 9.141044180302725e-17, step_size 0.09347245657049728, test_acc: 1.0\n",
            "Epoch 723, BestLoss: 2.8121752763609154e-16, Temperature 8.683991971287588e-17, step_size 0.09346310932484024, test_acc: 1.0\n",
            "Epoch 724, BestLoss: 3.7204770490127107e-16, Temperature 8.249792372723208e-17, step_size 0.09345376301390776, test_acc: 1.0\n",
            "Epoch 725, BestLoss: 2.73369792060221e-16, Temperature 7.837302754087048e-17, step_size 0.09344441763760637, test_acc: 1.0\n",
            "Epoch 726, BestLoss: 1.5730690040672384e-16, Temperature 7.445437616382695e-17, step_size 0.09343507319584261, test_acc: 1.0\n",
            "Epoch 727, BestLoss: 3.112949915954915e-16, Temperature 7.07316573556356e-17, step_size 0.09342572968852303, test_acc: 1.0\n",
            "Epoch 728, BestLoss: 6.269795543840873e-17, Temperature 6.719507448785381e-17, step_size 0.09341638711555418, test_acc: 1.0\n",
            "Epoch 729, BestLoss: 4.6515118717020505e-17, Temperature 6.383532076346112e-17, step_size 0.09340704547684263, test_acc: 1.0\n",
            "Epoch 730, BestLoss: 3.1272495866368715e-17, Temperature 6.064355472528806e-17, step_size 0.09339770477229495, test_acc: 1.0\n",
            "Epoch 731, BestLoss: 5.644180716968425e-17, Temperature 5.761137698902366e-17, step_size 0.09338836500181771, test_acc: 1.0\n",
            "Epoch 732, BestLoss: 1.6732511760922306e-16, Temperature 5.473080813957247e-17, step_size 0.09337902616531753, test_acc: 1.0\n",
            "Epoch 733, BestLoss: 5.66167567764544e-17, Temperature 5.199426773259384e-17, step_size 0.09336968826270099, test_acc: 1.0\n",
            "Epoch 734, BestLoss: 1.404646350790777e-16, Temperature 4.939455434596415e-17, step_size 0.09336035129387472, test_acc: 1.0\n",
            "Epoch 735, BestLoss: 1.3345782418127063e-16, Temperature 4.692482662866594e-17, step_size 0.09335101525874534, test_acc: 1.0\n",
            "Epoch 736, BestLoss: 7.897651060372196e-17, Temperature 4.457858529723264e-17, step_size 0.09334168015721947, test_acc: 1.0\n",
            "Epoch 737, BestLoss: 5.4356132591675206e-17, Temperature 4.234965603237101e-17, step_size 0.09333234598920374, test_acc: 1.0\n",
            "Epoch 738, BestLoss: 5.816924301855034e-17, Temperature 4.0232173230752455e-17, step_size 0.09332301275460482, test_acc: 1.0\n",
            "Epoch 739, BestLoss: 7.326869326742579e-17, Temperature 3.822056456921483e-17, step_size 0.09331368045332936, test_acc: 1.0\n",
            "Epoch 740, BestLoss: 7.226387795073952e-17, Temperature 3.6309536340754086e-17, step_size 0.09330434908528402, test_acc: 1.0\n",
            "Epoch 741, BestLoss: 1.195330464365542e-16, Temperature 3.449405952371638e-17, step_size 0.0932950186503755, test_acc: 1.0\n",
            "Epoch 742, BestLoss: 5.910052342341374e-17, Temperature 3.276935654753056e-17, step_size 0.09328568914851046, test_acc: 1.0\n",
            "Epoch 743, BestLoss: 3.498412327328832e-17, Temperature 3.1130888720154033e-17, step_size 0.09327636057959561, test_acc: 1.0\n",
            "Epoch 744, BestLoss: 2.0275991545720344e-17, Temperature 2.957434428414633e-17, step_size 0.09326703294353765, test_acc: 1.0\n",
            "Epoch 745, BestLoss: 3.4761873845744946e-17, Temperature 2.809562706993901e-17, step_size 0.0932577062402433, test_acc: 1.0\n",
            "Epoch 746, BestLoss: 4.698058603138535e-17, Temperature 2.669084571644206e-17, step_size 0.09324838046961927, test_acc: 1.0\n",
            "Epoch 747, BestLoss: 7.46506904612946e-17, Temperature 2.5356303430619956e-17, step_size 0.09323905563157231, test_acc: 1.0\n",
            "Epoch 748, BestLoss: 7.269798818838078e-17, Temperature 2.4088488259088957e-17, step_size 0.09322973172600915, test_acc: 1.0\n",
            "Epoch 749, BestLoss: 4.4359081457734835e-17, Temperature 2.288406384613451e-17, step_size 0.09322040875283655, test_acc: 1.0\n",
            "Epoch 750, BestLoss: 8.295277830365474e-17, Temperature 2.1739860653827782e-17, step_size 0.09321108671196127, test_acc: 1.0\n",
            "Epoch 751, BestLoss: 9.845533998322282e-17, Temperature 2.065286762113639e-17, step_size 0.09320176560329008, test_acc: 1.0\n",
            "Epoch 752, BestLoss: 1.0615160466761944e-16, Temperature 1.962022424007957e-17, step_size 0.09319244542672975, test_acc: 1.0\n",
            "Epoch 753, BestLoss: 5.90530182701834e-17, Temperature 1.863921302807559e-17, step_size 0.09318312618218708, test_acc: 1.0\n",
            "Epoch 754, BestLoss: 3.516387144447577e-17, Temperature 1.770725237667181e-17, step_size 0.09317380786956886, test_acc: 1.0\n",
            "Epoch 755, BestLoss: 6.52625379116952e-17, Temperature 1.682188975783822e-17, step_size 0.0931644904887819, test_acc: 1.0\n",
            "Epoch 756, BestLoss: 4.8983128320764844e-17, Temperature 1.5980795269946308e-17, step_size 0.09315517403973303, test_acc: 1.0\n",
            "Epoch 757, BestLoss: 5.1908755880447075e-17, Temperature 1.5181755506448993e-17, step_size 0.09314585852232905, test_acc: 1.0\n",
            "Epoch 758, BestLoss: 1.424243373120524e-17, Temperature 1.4422667731126544e-17, step_size 0.09313654393647681, test_acc: 1.0\n",
            "Epoch 759, BestLoss: 1.0581365095688697e-17, Temperature 1.3701534344570217e-17, step_size 0.09312723028208317, test_acc: 1.0\n",
            "Epoch 760, BestLoss: 1.1584140614311185e-17, Temperature 1.3016457627341705e-17, step_size 0.09311791755905496, test_acc: 1.0\n",
            "Epoch 761, BestLoss: 8.359819947400643e-18, Temperature 1.236563474597462e-17, step_size 0.09310860576729905, test_acc: 1.0\n",
            "Epoch 762, BestLoss: 1.40111633584757e-17, Temperature 1.1747353008675889e-17, step_size 0.09309929490672232, test_acc: 1.0\n",
            "Epoch 763, BestLoss: 3.565346846825857e-17, Temperature 1.1159985358242094e-17, step_size 0.09308998497723164, test_acc: 1.0\n",
            "Epoch 764, BestLoss: 1.2358264234176792e-17, Temperature 1.0601986090329988e-17, step_size 0.09308067597873391, test_acc: 1.0\n",
            "Epoch 765, BestLoss: 1.1734571204308387e-17, Temperature 1.0071886785813488e-17, step_size 0.09307136791113604, test_acc: 1.0\n",
            "Epoch 766, BestLoss: 5.4051697942840174e-18, Temperature 9.568292446522813e-18, step_size 0.09306206077434494, test_acc: 1.0\n",
            "Epoch 767, BestLoss: 6.0474385902781406e-18, Temperature 9.089877824196671e-18, step_size 0.0930527545682675, test_acc: 1.0\n",
            "Epoch 768, BestLoss: 5.770329475468551e-18, Temperature 8.635383932986837e-18, step_size 0.09304344929281068, test_acc: 1.0\n",
            "Epoch 769, BestLoss: 3.145168666473903e-18, Temperature 8.203614736337495e-18, step_size 0.0930341449478814, test_acc: 1.0\n",
            "Epoch 770, BestLoss: 1.3763650285187976e-18, Temperature 7.79343399952062e-18, step_size 0.09302484153338661, test_acc: 1.0\n",
            "Epoch 771, BestLoss: 4.576327569878633e-18, Temperature 7.403762299544589e-18, step_size 0.09301553904923328, test_acc: 1.0\n",
            "Epoch 772, BestLoss: 5.5553541976018805e-18, Temperature 7.033574184567359e-18, step_size 0.09300623749532835, test_acc: 1.0\n",
            "Epoch 773, BestLoss: 3.215812878636422e-18, Temperature 6.681895475338991e-18, step_size 0.09299693687157883, test_acc: 1.0\n",
            "Epoch 774, BestLoss: 1.1637550396876801e-17, Temperature 6.347800701572041e-18, step_size 0.09298763717789167, test_acc: 1.0\n",
            "Epoch 775, BestLoss: 3.9556024763327125e-18, Temperature 6.030410666493439e-18, step_size 0.09297833841417388, test_acc: 1.0\n",
            "Epoch 776, BestLoss: 8.99819799898163e-18, Temperature 5.728890133168767e-18, step_size 0.09296904058033247, test_acc: 1.0\n",
            "Epoch 777, BestLoss: 1.6635523826882217e-17, Temperature 5.442445626510329e-18, step_size 0.09295974367627444, test_acc: 1.0\n",
            "Epoch 778, BestLoss: 5.587764888908974e-18, Temperature 5.170323345184812e-18, step_size 0.09295044770190682, test_acc: 1.0\n",
            "Epoch 779, BestLoss: 2.6604796059580267e-18, Temperature 4.911807177925571e-18, step_size 0.09294115265713662, test_acc: 1.0\n",
            "Epoch 780, BestLoss: 1.648799058598973e-18, Temperature 4.6662168190292926e-18, step_size 0.09293185854187092, test_acc: 1.0\n",
            "Epoch 781, BestLoss: 1.6624926621856754e-18, Temperature 4.432905978077828e-18, step_size 0.09292256535601673, test_acc: 1.0\n",
            "Epoch 782, BestLoss: 5.274572487123558e-18, Temperature 4.211260679173937e-18, step_size 0.09291327309948112, test_acc: 1.0\n",
            "Epoch 783, BestLoss: 2.788953737244923e-18, Temperature 4.00069764521524e-18, step_size 0.09290398177217117, test_acc: 1.0\n",
            "Epoch 784, BestLoss: 1.2725619047636509e-18, Temperature 3.8006627629544775e-18, step_size 0.09289469137399395, test_acc: 1.0\n",
            "Epoch 785, BestLoss: 3.2208505245184474e-18, Temperature 3.610629624806753e-18, step_size 0.09288540190485654, test_acc: 1.0\n",
            "Epoch 786, BestLoss: 2.7503324328182397e-18, Temperature 3.4300981435664152e-18, step_size 0.09287611336466606, test_acc: 1.0\n",
            "Epoch 787, BestLoss: 1.773069554413807e-18, Temperature 3.2585932363880945e-18, step_size 0.0928668257533296, test_acc: 1.0\n",
            "Epoch 788, BestLoss: 1.2353221458803596e-18, Temperature 3.0956635745686896e-18, step_size 0.09285753907075427, test_acc: 1.0\n",
            "Epoch 789, BestLoss: 5.270689529655861e-19, Temperature 2.940880395840255e-18, step_size 0.09284825331684719, test_acc: 1.0\n",
            "Epoch 790, BestLoss: 4.305222985365423e-19, Temperature 2.793836376048242e-18, step_size 0.0928389684915155, test_acc: 1.0\n",
            "Epoch 791, BestLoss: 1.3456333442255593e-18, Temperature 2.6541445572458296e-18, step_size 0.09282968459466635, test_acc: 1.0\n",
            "Epoch 792, BestLoss: 5.365225404848257e-19, Temperature 2.521437329383538e-18, step_size 0.09282040162620689, test_acc: 1.0\n",
            "Epoch 793, BestLoss: 5.194180083309999e-19, Temperature 2.395365462914361e-18, step_size 0.09281111958604427, test_acc: 1.0\n",
            "Epoch 794, BestLoss: 1.0476257272050018e-18, Temperature 2.2755971897686428e-18, step_size 0.09280183847408566, test_acc: 1.0\n",
            "Epoch 795, BestLoss: 2.1415744265211157e-18, Temperature 2.1618173302802107e-18, step_size 0.09279255829023825, test_acc: 1.0\n",
            "Epoch 796, BestLoss: 5.444396240003996e-19, Temperature 2.0537264637662002e-18, step_size 0.09278327903440922, test_acc: 1.0\n",
            "Epoch 797, BestLoss: 1.1964431326990933e-18, Temperature 1.9510401405778902e-18, step_size 0.09277400070650578, test_acc: 1.0\n",
            "Epoch 798, BestLoss: 1.1783223131675209e-18, Temperature 1.853488133548996e-18, step_size 0.09276472330643513, test_acc: 1.0\n",
            "Epoch 799, BestLoss: 1.1116270456052226e-18, Temperature 1.760813726871546e-18, step_size 0.09275544683410449, test_acc: 1.0\n",
            "Epoch 800, BestLoss: 6.143923780639125e-19, Temperature 1.6727730405279685e-18, step_size 0.09274617128942107, test_acc: 1.0\n",
            "Epoch 801, BestLoss: 1.6422304214129158e-18, Temperature 1.58913438850157e-18, step_size 0.09273689667229214, test_acc: 1.0\n",
            "Epoch 802, BestLoss: 9.277069376920748e-19, Temperature 1.5096776690764913e-18, step_size 0.09272762298262491, test_acc: 1.0\n",
            "Epoch 803, BestLoss: 6.99955726721853e-19, Temperature 1.4341937856226666e-18, step_size 0.09271835022032665, test_acc: 1.0\n",
            "Epoch 804, BestLoss: 4.10358501324588e-18, Temperature 1.3624840963415331e-18, step_size 0.09270907838530462, test_acc: 1.0\n",
            "Epoch 805, BestLoss: 2.2443853539845026e-18, Temperature 1.2943598915244564e-18, step_size 0.09269980747746609, test_acc: 1.0\n",
            "Epoch 806, BestLoss: 1.0803607130128183e-18, Temperature 1.2296418969482335e-18, step_size 0.09269053749671835, test_acc: 1.0\n",
            "Epoch 807, BestLoss: 1.7344993345650169e-18, Temperature 1.1681598021008218e-18, step_size 0.09268126844296867, test_acc: 1.0\n",
            "Epoch 808, BestLoss: 2.6973237403777503e-18, Temperature 1.1097518119957807e-18, step_size 0.09267200031612438, test_acc: 1.0\n",
            "Epoch 809, BestLoss: 2.0493722454310202e-18, Temperature 1.0542642213959916e-18, step_size 0.09266273311609277, test_acc: 1.0\n",
            "Epoch 810, BestLoss: 1.5300402279747146e-18, Temperature 1.001551010326192e-18, step_size 0.09265346684278117, test_acc: 1.0\n",
            "Epoch 811, BestLoss: 5.8763875052011935e-19, Temperature 9.514734598098822e-19, step_size 0.0926442014960969, test_acc: 1.0\n",
            "Epoch 812, BestLoss: 1.3409943119641397e-18, Temperature 9.03899786819388e-19, step_size 0.09263493707594729, test_acc: 1.0\n",
            "Epoch 813, BestLoss: 1.1855669086449953e-18, Temperature 8.587047974784186e-19, step_size 0.09262567358223969, test_acc: 1.0\n",
            "Epoch 814, BestLoss: 1.4406731493972314e-18, Temperature 8.157695576044977e-19, step_size 0.09261641101488147, test_acc: 1.0\n",
            "Epoch 815, BestLoss: 2.968739670424417e-18, Temperature 7.749810797242728e-19, step_size 0.09260714937377998, test_acc: 1.0\n",
            "Epoch 816, BestLoss: 1.366084146580517e-18, Temperature 7.362320257380592e-19, step_size 0.09259788865884261, test_acc: 1.0\n",
            "Epoch 817, BestLoss: 7.109557039037567e-19, Temperature 6.994204244511562e-19, step_size 0.09258862886997672, test_acc: 1.0\n",
            "Epoch 818, BestLoss: 1.0899189736105605e-18, Temperature 6.644494032285983e-19, step_size 0.09257937000708973, test_acc: 1.0\n",
            "Epoch 819, BestLoss: 1.8277888551917815e-18, Temperature 6.312269330671683e-19, step_size 0.09257011207008901, test_acc: 1.0\n",
            "Epoch 820, BestLoss: 1.8973149847661903e-18, Temperature 5.996655864138099e-19, step_size 0.09256085505888201, test_acc: 1.0\n",
            "Epoch 821, BestLoss: 2.1254532750705123e-18, Temperature 5.696823070931194e-19, step_size 0.09255159897337611, test_acc: 1.0\n",
            "Epoch 822, BestLoss: 3.240007132861488e-18, Temperature 5.411981917384634e-19, step_size 0.09254234381347878, test_acc: 1.0\n",
            "Epoch 823, BestLoss: 9.533236244218901e-19, Temperature 5.141382821515402e-19, step_size 0.09253308957909744, test_acc: 1.0\n",
            "Epoch 824, BestLoss: 1.3555329165261137e-18, Temperature 4.884313680439631e-19, step_size 0.09252383627013952, test_acc: 1.0\n",
            "Epoch 825, BestLoss: 1.517786097790104e-18, Temperature 4.64009799641765e-19, step_size 0.09251458388651251, test_acc: 1.0\n",
            "Epoch 826, BestLoss: 1.7809374388400505e-18, Temperature 4.408093096596767e-19, step_size 0.09250533242812387, test_acc: 1.0\n",
            "Epoch 827, BestLoss: 1.1968416976493157e-18, Temperature 4.1876884417669284e-19, step_size 0.09249608189488105, test_acc: 1.0\n",
            "Epoch 828, BestLoss: 1.0433313585389654e-18, Temperature 3.978304019678582e-19, step_size 0.09248683228669156, test_acc: 1.0\n",
            "Epoch 829, BestLoss: 1.9520777852969995e-18, Temperature 3.7793888186946525e-19, step_size 0.09247758360346289, test_acc: 1.0\n",
            "Epoch 830, BestLoss: 9.610832292310441e-19, Temperature 3.59041937775992e-19, step_size 0.09246833584510254, test_acc: 1.0\n",
            "Epoch 831, BestLoss: 1.1491269928876426e-18, Temperature 3.410898408871924e-19, step_size 0.09245908901151803, test_acc: 1.0\n",
            "Epoch 832, BestLoss: 1.1462280415858714e-18, Temperature 3.2403534884283275e-19, step_size 0.09244984310261688, test_acc: 1.0\n",
            "Epoch 833, BestLoss: 1.65619426394727e-18, Temperature 3.078335814006911e-19, step_size 0.09244059811830661, test_acc: 1.0\n",
            "Epoch 834, BestLoss: 8.449271341774109e-19, Temperature 2.924419023306565e-19, step_size 0.09243135405849479, test_acc: 1.0\n",
            "Epoch 835, BestLoss: 6.989752629627437e-19, Temperature 2.7781980721412367e-19, step_size 0.09242211092308894, test_acc: 1.0\n",
            "Epoch 836, BestLoss: 8.679950863218428e-19, Temperature 2.639288168534175e-19, step_size 0.09241286871199664, test_acc: 1.0\n",
            "Epoch 837, BestLoss: 4.668046756376552e-19, Temperature 2.507323760107466e-19, step_size 0.09240362742512544, test_acc: 1.0\n",
            "Epoch 838, BestLoss: 5.57158721271658e-19, Temperature 2.3819575721020925e-19, step_size 0.09239438706238293, test_acc: 1.0\n",
            "Epoch 839, BestLoss: 4.356139282349156e-19, Temperature 2.262859693496988e-19, step_size 0.0923851476236767, test_acc: 1.0\n",
            "Epoch 840, BestLoss: 6.629816856385498e-19, Temperature 2.1497167088221383e-19, step_size 0.09237590910891433, test_acc: 1.0\n",
            "Epoch 841, BestLoss: 5.252832743029392e-19, Temperature 2.0422308733810313e-19, step_size 0.09236667151800344, test_acc: 1.0\n",
            "Epoch 842, BestLoss: 5.262872185214215e-19, Temperature 1.9401193297119797e-19, step_size 0.09235743485085164, test_acc: 1.0\n",
            "Epoch 843, BestLoss: 3.9615378987238686e-19, Temperature 1.8431133632263805e-19, step_size 0.09234819910736655, test_acc: 1.0\n",
            "Epoch 844, BestLoss: 3.2886789302484366e-19, Temperature 1.7509576950650615e-19, step_size 0.09233896428745582, test_acc: 1.0\n",
            "Epoch 845, BestLoss: 8.286337178642383e-19, Temperature 1.6634098103118084e-19, step_size 0.09232973039102707, test_acc: 1.0\n",
            "Epoch 846, BestLoss: 3.592300941802456e-19, Temperature 1.580239319796218e-19, step_size 0.09232049741798797, test_acc: 1.0\n",
            "Epoch 847, BestLoss: 3.7035651385552085e-19, Temperature 1.501227353806407e-19, step_size 0.09231126536824617, test_acc: 1.0\n",
            "Epoch 848, BestLoss: 2.583154045405555e-19, Temperature 1.4261659861160864e-19, step_size 0.09230203424170935, test_acc: 1.0\n",
            "Epoch 849, BestLoss: 1.2780301495094294e-19, Temperature 1.3548576868102821e-19, step_size 0.09229280403828519, test_acc: 1.0\n",
            "Epoch 850, BestLoss: 1.6757016167223484e-19, Temperature 1.287114802469768e-19, step_size 0.09228357475788136, test_acc: 1.0\n",
            "Epoch 851, BestLoss: 1.0864741779938942e-19, Temperature 1.2227590623462796e-19, step_size 0.09227434640040558, test_acc: 1.0\n",
            "Epoch 852, BestLoss: 9.180234990485302e-20, Temperature 1.1616211092289656e-19, step_size 0.09226511896576554, test_acc: 1.0\n",
            "Epoch 853, BestLoss: 6.603806359095617e-20, Temperature 1.1035400537675173e-19, step_size 0.09225589245386896, test_acc: 1.0\n",
            "Epoch 854, BestLoss: 1.7004208489111447e-19, Temperature 1.0483630510791414e-19, step_size 0.09224666686462357, test_acc: 1.0\n",
            "Epoch 855, BestLoss: 1.516786133470205e-19, Temperature 9.959448985251844e-20, step_size 0.09223744219793711, test_acc: 1.0\n",
            "Epoch 856, BestLoss: 1.9092244587556179e-19, Temperature 9.46147653598925e-20, step_size 0.09222821845371731, test_acc: 1.0\n",
            "Epoch 857, BestLoss: 2.6801523356037062e-19, Temperature 8.988402709189788e-20, step_size 0.09221899563187194, test_acc: 1.0\n",
            "Epoch 858, BestLoss: 1.1026516949646302e-19, Temperature 8.538982573730298e-20, step_size 0.09220977373230875, test_acc: 1.0\n",
            "Epoch 859, BestLoss: 1.220907679409961e-19, Temperature 8.112033445043783e-20, step_size 0.09220055275493552, test_acc: 1.0\n",
            "Epoch 860, BestLoss: 1.949983479663925e-19, Temperature 7.706431772791593e-20, step_size 0.09219133269966003, test_acc: 1.0\n",
            "Epoch 861, BestLoss: 1.6497586681322434e-19, Temperature 7.321110184152013e-20, step_size 0.09218211356639006, test_acc: 1.0\n",
            "Epoch 862, BestLoss: 7.343036340258205e-20, Temperature 6.955054674944412e-20, step_size 0.09217289535503342, test_acc: 1.0\n",
            "Epoch 863, BestLoss: 1.3720944750144984e-19, Temperature 6.607301941197191e-20, step_size 0.09216367806549793, test_acc: 1.0\n",
            "Epoch 864, BestLoss: 1.4358747781728266e-19, Temperature 6.276936844137331e-20, step_size 0.09215446169769137, test_acc: 1.0\n",
            "Epoch 865, BestLoss: 1.02548953098664e-19, Temperature 5.963090001930465e-20, step_size 0.0921452462515216, test_acc: 1.0\n",
            "Epoch 866, BestLoss: 1.8466967454031462e-19, Temperature 5.664935501833942e-20, step_size 0.09213603172689644, test_acc: 1.0\n",
            "Epoch 867, BestLoss: 2.799567792024787e-19, Temperature 5.3816887267422446e-20, step_size 0.09212681812372375, test_acc: 1.0\n",
            "Epoch 868, BestLoss: 1.0518103011535734e-19, Temperature 5.112604290405132e-20, step_size 0.09211760544191139, test_acc: 1.0\n",
            "Epoch 869, BestLoss: 5.1602569016667866e-20, Temperature 4.8569740758848757e-20, step_size 0.0921083936813672, test_acc: 1.0\n",
            "Epoch 870, BestLoss: 5.561276022143322e-20, Temperature 4.6141253720906314e-20, step_size 0.09209918284199906, test_acc: 1.0\n",
            "Epoch 871, BestLoss: 5.013842158613963e-20, Temperature 4.3834191034860995e-20, step_size 0.09208997292371486, test_acc: 1.0\n",
            "Epoch 872, BestLoss: 4.112072669301808e-20, Temperature 4.1642481483117944e-20, step_size 0.09208076392642249, test_acc: 1.0\n",
            "Epoch 873, BestLoss: 3.2116103382436986e-20, Temperature 3.9560357408962046e-20, step_size 0.09207155585002985, test_acc: 1.0\n",
            "Epoch 874, BestLoss: 5.011438679540882e-20, Temperature 3.758233953851394e-20, step_size 0.09206234869444485, test_acc: 1.0\n",
            "Epoch 875, BestLoss: 3.525193649413608e-20, Temperature 3.570322256158824e-20, step_size 0.09205314245957541, test_acc: 1.0\n",
            "Epoch 876, BestLoss: 2.4752844325691136e-20, Temperature 3.391806143350883e-20, step_size 0.09204393714532945, test_acc: 1.0\n",
            "Epoch 877, BestLoss: 1.1788287798408477e-20, Temperature 3.2222158361833386e-20, step_size 0.09203473275161492, test_acc: 1.0\n",
            "Epoch 878, BestLoss: 6.229932833757062e-20, Temperature 3.0611050443741715e-20, step_size 0.09202552927833976, test_acc: 1.0\n",
            "Epoch 879, BestLoss: 9.694659659454532e-20, Temperature 2.908049792155463e-20, step_size 0.09201632672541193, test_acc: 1.0\n",
            "Epoch 880, BestLoss: 4.8107535368794046e-20, Temperature 2.7626473025476896e-20, step_size 0.0920071250927394, test_acc: 1.0\n",
            "Epoch 881, BestLoss: 4.3041682737557766e-20, Temperature 2.624514937420305e-20, step_size 0.09199792438023012, test_acc: 1.0\n",
            "Epoch 882, BestLoss: 1.4101563238181336e-19, Temperature 2.4932891905492897e-20, step_size 0.0919887245877921, test_acc: 1.0\n",
            "Epoch 883, BestLoss: 1.0228509547388796e-19, Temperature 2.368624731021825e-20, step_size 0.09197952571533333, test_acc: 1.0\n",
            "Epoch 884, BestLoss: 8.478201837636708e-20, Temperature 2.2501934944707338e-20, step_size 0.09197032776276179, test_acc: 1.0\n",
            "Epoch 885, BestLoss: 3.095386537286815e-20, Temperature 2.137683819747197e-20, step_size 0.09196113072998552, test_acc: 1.0\n",
            "Epoch 886, BestLoss: 2.2830615723710793e-20, Temperature 2.030799628759837e-20, step_size 0.09195193461691252, test_acc: 1.0\n",
            "Epoch 887, BestLoss: 3.1609753251844933e-20, Temperature 1.929259647321845e-20, step_size 0.09194273942345083, test_acc: 1.0\n",
            "Epoch 888, BestLoss: 1.9555355937407002e-20, Temperature 1.8327966649557528e-20, step_size 0.09193354514950848, test_acc: 1.0\n",
            "Epoch 889, BestLoss: 1.9084177166145134e-20, Temperature 1.741156831707965e-20, step_size 0.09192435179499353, test_acc: 1.0\n",
            "Epoch 890, BestLoss: 2.1183507223930472e-20, Temperature 1.6540989901225668e-20, step_size 0.09191515935981404, test_acc: 1.0\n",
            "Epoch 891, BestLoss: 2.6350589772976483e-20, Temperature 1.5713940406164385e-20, step_size 0.09190596784387806, test_acc: 1.0\n",
            "Epoch 892, BestLoss: 5.89156753450368e-20, Temperature 1.4928243385856164e-20, step_size 0.09189677724709366, test_acc: 1.0\n",
            "Epoch 893, BestLoss: 5.700152949178356e-20, Temperature 1.4181831216563355e-20, step_size 0.09188758756936896, test_acc: 1.0\n",
            "Epoch 894, BestLoss: 4.6199938705489204e-20, Temperature 1.3472739655735187e-20, step_size 0.09187839881061202, test_acc: 1.0\n",
            "Epoch 895, BestLoss: 3.0193142676133133e-20, Temperature 1.2799102672948428e-20, step_size 0.09186921097073096, test_acc: 1.0\n",
            "Epoch 896, BestLoss: 2.4147138578474688e-20, Temperature 1.2159147539301006e-20, step_size 0.09186002404963389, test_acc: 1.0\n",
            "Epoch 897, BestLoss: 3.778230878475409e-20, Temperature 1.1551190162335955e-20, step_size 0.09185083804722893, test_acc: 1.0\n",
            "Epoch 898, BestLoss: 3.22780155956499e-20, Temperature 1.0973630654219156e-20, step_size 0.09184165296342421, test_acc: 1.0\n",
            "Epoch 899, BestLoss: 3.299003574307371e-20, Temperature 1.0424949121508198e-20, step_size 0.09183246879812787, test_acc: 1.0\n",
            "Epoch 900, BestLoss: 5.625567780206965e-20, Temperature 9.903701665432788e-21, step_size 0.09182328555124805, test_acc: 1.0\n",
            "Epoch 901, BestLoss: 5.355014928878378e-20, Temperature 9.408516582161148e-21, step_size 0.09181410322269293, test_acc: 1.0\n",
            "Epoch 902, BestLoss: 4.172176202491035e-20, Temperature 8.93809075305309e-21, step_size 0.09180492181237065, test_acc: 1.0\n",
            "Epoch 903, BestLoss: 4.4940388798829594e-20, Temperature 8.491186215400436e-21, step_size 0.09179574132018942, test_acc: 1.0\n",
            "Epoch 904, BestLoss: 2.718720640008422e-20, Temperature 8.066626904630413e-21, step_size 0.0917865617460574, test_acc: 1.0\n",
            "Epoch 905, BestLoss: 2.920673956842788e-20, Temperature 7.663295559398893e-21, step_size 0.0917773830898828, test_acc: 1.0\n",
            "Epoch 906, BestLoss: 1.2986754865190696e-20, Temperature 7.280130781428948e-21, step_size 0.09176820535157382, test_acc: 1.0\n",
            "Epoch 907, BestLoss: 6.517926947161615e-21, Temperature 6.9161242423575e-21, step_size 0.09175902853103866, test_acc: 1.0\n",
            "Epoch 908, BestLoss: 1.1117812816793429e-20, Temperature 6.570318030239625e-21, step_size 0.09174985262818557, test_acc: 1.0\n",
            "Epoch 909, BestLoss: 3.545838725622561e-20, Temperature 6.2418021287276435e-21, step_size 0.09174067764292275, test_acc: 1.0\n",
            "Epoch 910, BestLoss: 2.6289522859445897e-20, Temperature 5.929712022291261e-21, step_size 0.09173150357515845, test_acc: 1.0\n",
            "Epoch 911, BestLoss: 2.1401767393685437e-20, Temperature 5.6332264211766975e-21, step_size 0.09172233042480094, test_acc: 1.0\n",
            "Epoch 912, BestLoss: 3.769645645051307e-21, Temperature 5.3515651001178625e-21, step_size 0.09171315819175846, test_acc: 1.0\n",
            "Epoch 913, BestLoss: 3.3601900629472087e-21, Temperature 5.083986845111969e-21, step_size 0.09170398687593928, test_acc: 1.0\n",
            "Epoch 914, BestLoss: 3.2020687507294807e-21, Temperature 4.8297875028563705e-21, step_size 0.09169481647725168, test_acc: 1.0\n",
            "Epoch 915, BestLoss: 1.2069577274248525e-21, Temperature 4.5882981277135515e-21, step_size 0.09168564699560396, test_acc: 1.0\n",
            "Epoch 916, BestLoss: 1.0164312291883685e-21, Temperature 4.3588832213278734e-21, step_size 0.0916764784309044, test_acc: 1.0\n",
            "Epoch 917, BestLoss: 2.5870103232604818e-21, Temperature 4.140939060261479e-21, step_size 0.09166731078306131, test_acc: 1.0\n",
            "Epoch 918, BestLoss: 3.566915008895594e-21, Temperature 3.933892107248405e-21, step_size 0.09165814405198301, test_acc: 1.0\n",
            "Epoch 919, BestLoss: 1.7333802961862715e-21, Temperature 3.737197501885985e-21, step_size 0.09164897823757781, test_acc: 1.0\n",
            "Epoch 920, BestLoss: 4.6105302993813086e-21, Temperature 3.550337626791686e-21, step_size 0.09163981333975406, test_acc: 1.0\n",
            "Epoch 921, BestLoss: 4.068471580642617e-21, Temperature 3.3728207454521013e-21, step_size 0.09163064935842008, test_acc: 1.0\n",
            "Epoch 922, BestLoss: 1.2170961985202322e-20, Temperature 3.204179708179496e-21, step_size 0.09162148629348424, test_acc: 1.0\n",
            "Epoch 923, BestLoss: 4.105799093771615e-21, Temperature 3.043970722770521e-21, step_size 0.09161232414485489, test_acc: 1.0\n",
            "Epoch 924, BestLoss: 3.193779273402907e-21, Temperature 2.8917721866319947e-21, step_size 0.0916031629124404, test_acc: 1.0\n",
            "Epoch 925, BestLoss: 2.927487446675174e-21, Temperature 2.747183577300395e-21, step_size 0.09159400259614917, test_acc: 1.0\n",
            "Epoch 926, BestLoss: 3.709867821096792e-21, Temperature 2.6098243984353752e-21, step_size 0.09158484319588955, test_acc: 1.0\n",
            "Epoch 927, BestLoss: 4.41028372169421e-21, Temperature 2.4793331785136063e-21, step_size 0.09157568471156996, test_acc: 1.0\n",
            "Epoch 928, BestLoss: 4.5442598659226774e-21, Temperature 2.355366519587926e-21, step_size 0.0915665271430988, test_acc: 1.0\n",
            "Epoch 929, BestLoss: 3.995603675771794e-21, Temperature 2.2375981936085296e-21, step_size 0.0915573704903845, test_acc: 1.0\n",
            "Epoch 930, BestLoss: 5.225845275550406e-21, Temperature 2.125718283928103e-21, step_size 0.09154821475333545, test_acc: 1.0\n",
            "Epoch 931, BestLoss: 8.199666504800253e-21, Temperature 2.0194323697316975e-21, step_size 0.09153905993186012, test_acc: 1.0\n",
            "Epoch 932, BestLoss: 6.126431604855443e-21, Temperature 1.9184607512451125e-21, step_size 0.09152990602586694, test_acc: 1.0\n",
            "Epoch 933, BestLoss: 1.1804784247297217e-20, Temperature 1.8225377136828567e-21, step_size 0.09152075303526436, test_acc: 1.0\n",
            "Epoch 934, BestLoss: 1.2076421062646859e-20, Temperature 1.7314108279987138e-21, step_size 0.09151160095996083, test_acc: 1.0\n",
            "Epoch 935, BestLoss: 1.106285263954502e-20, Temperature 1.644840286598778e-21, step_size 0.09150244979986484, test_acc: 1.0\n",
            "Epoch 936, BestLoss: 9.437500594599728e-21, Temperature 1.562598272268839e-21, step_size 0.09149329955488486, test_acc: 1.0\n",
            "Epoch 937, BestLoss: 7.311014249638216e-21, Temperature 1.484468358655397e-21, step_size 0.09148415022492937, test_acc: 1.0\n",
            "Epoch 938, BestLoss: 5.129873134882615e-21, Temperature 1.410244940722627e-21, step_size 0.09147500180990688, test_acc: 1.0\n",
            "Epoch 939, BestLoss: 6.411927597664281e-21, Temperature 1.3397326936864957e-21, step_size 0.09146585430972588, test_acc: 1.0\n",
            "Epoch 940, BestLoss: 2.271464989658928e-21, Temperature 1.2727460590021708e-21, step_size 0.09145670772429491, test_acc: 1.0\n",
            "Epoch 941, BestLoss: 3.9131050706001695e-21, Temperature 1.2091087560520623e-21, step_size 0.09144756205352249, test_acc: 1.0\n",
            "Epoch 942, BestLoss: 3.614405127587074e-21, Temperature 1.1486533182494592e-21, step_size 0.09143841729731714, test_acc: 1.0\n",
            "Epoch 943, BestLoss: 4.115416996374023e-21, Temperature 1.091220652336986e-21, step_size 0.0914292734555874, test_acc: 1.0\n",
            "Epoch 944, BestLoss: 2.9932810359057313e-21, Temperature 1.0366596197201368e-21, step_size 0.09142013052824186, test_acc: 1.0\n",
            "Epoch 945, BestLoss: 4.9604035896799206e-21, Temperature 9.8482663873413e-22, step_size 0.09141098851518903, test_acc: 1.0\n",
            "Epoch 946, BestLoss: 2.2819962416486973e-21, Temperature 9.355853067974234e-22, step_size 0.09140184741633751, test_acc: 1.0\n",
            "Epoch 947, BestLoss: 2.2233453481954963e-21, Temperature 8.888060414575522e-22, step_size 0.09139270723159588, test_acc: 1.0\n",
            "Epoch 948, BestLoss: 1.6899023229767464e-21, Temperature 8.443657393846746e-22, step_size 0.09138356796087273, test_acc: 1.0\n",
            "Epoch 949, BestLoss: 1.8915119401597355e-21, Temperature 8.021474524154409e-22, step_size 0.09137442960407664, test_acc: 1.0\n",
            "Epoch 950, BestLoss: 2.2221873565409303e-21, Temperature 7.620400797946688e-22, step_size 0.09136529216111623, test_acc: 1.0\n",
            "Epoch 951, BestLoss: 1.9569518811157998e-21, Temperature 7.239380758049354e-22, step_size 0.09135615563190012, test_acc: 1.0\n",
            "Epoch 952, BestLoss: 2.4198682536268475e-21, Temperature 6.877411720146886e-22, step_size 0.09134702001633693, test_acc: 1.0\n",
            "Epoch 953, BestLoss: 9.71245686251122e-22, Temperature 6.533541134139541e-22, step_size 0.0913378853143353, test_acc: 1.0\n",
            "Epoch 954, BestLoss: 1.4944621746422738e-21, Temperature 6.206864077432564e-22, step_size 0.09132875152580387, test_acc: 1.0\n",
            "Epoch 955, BestLoss: 1.1878630869817408e-21, Temperature 5.896520873560936e-22, step_size 0.09131961865065129, test_acc: 1.0\n",
            "Epoch 956, BestLoss: 1.0341296930684005e-21, Temperature 5.601694829882889e-22, step_size 0.09131048668878622, test_acc: 1.0\n",
            "Epoch 957, BestLoss: 7.942519973793637e-22, Temperature 5.321610088388744e-22, step_size 0.09130135564011733, test_acc: 1.0\n",
            "Epoch 958, BestLoss: 7.997941559888683e-22, Temperature 5.055529583969306e-22, step_size 0.09129222550455332, test_acc: 1.0\n",
            "Epoch 959, BestLoss: 8.325830011101848e-22, Temperature 4.80275310477084e-22, step_size 0.09128309628200286, test_acc: 1.0\n",
            "Epoch 960, BestLoss: 8.706980924629397e-22, Temperature 4.562615449532298e-22, step_size 0.09127396797237466, test_acc: 1.0\n",
            "Epoch 961, BestLoss: 1.1278345369148891e-21, Temperature 4.334484677055683e-22, step_size 0.09126484057557743, test_acc: 1.0\n",
            "Epoch 962, BestLoss: 7.8497304529757615e-22, Temperature 4.1177604432028985e-22, step_size 0.09125571409151988, test_acc: 1.0\n",
            "Epoch 963, BestLoss: 7.946862290568589e-22, Temperature 3.9118724210427534e-22, step_size 0.09124658852011072, test_acc: 1.0\n",
            "Epoch 964, BestLoss: 9.066755826269128e-22, Temperature 3.7162787999906155e-22, step_size 0.09123746386125871, test_acc: 1.0\n",
            "Epoch 965, BestLoss: 1.1127842025242113e-21, Temperature 3.5304648599910847e-22, step_size 0.09122834011487259, test_acc: 1.0\n",
            "Epoch 966, BestLoss: 8.083770546992386e-22, Temperature 3.3539416169915304e-22, step_size 0.0912192172808611, test_acc: 1.0\n",
            "Epoch 967, BestLoss: 9.443871593955552e-22, Temperature 3.1862445361419537e-22, step_size 0.09121009535913302, test_acc: 1.0\n",
            "Epoch 968, BestLoss: 7.847326028268458e-22, Temperature 3.026932309334856e-22, step_size 0.0912009743495971, test_acc: 1.0\n",
            "Epoch 969, BestLoss: 9.257063082623e-22, Temperature 2.875585693868113e-22, step_size 0.09119185425216214, test_acc: 1.0\n",
            "Epoch 970, BestLoss: 1.3181209002397196e-21, Temperature 2.7318064091747076e-22, step_size 0.09118273506673692, test_acc: 1.0\n",
            "Epoch 971, BestLoss: 5.9266983985678895e-22, Temperature 2.595216088715972e-22, step_size 0.09117361679323024, test_acc: 1.0\n",
            "Epoch 972, BestLoss: 7.896032955985114e-22, Temperature 2.4654552842801735e-22, step_size 0.09116449943155092, test_acc: 1.0\n",
            "Epoch 973, BestLoss: 4.819784858963661e-22, Temperature 2.342182520066165e-22, step_size 0.09115538298160776, test_acc: 1.0\n",
            "Epoch 974, BestLoss: 5.761653031786006e-22, Temperature 2.2250733940628566e-22, step_size 0.0911462674433096, test_acc: 1.0\n",
            "Epoch 975, BestLoss: 3.5494582322457284e-22, Temperature 2.1138197243597138e-22, step_size 0.09113715281656527, test_acc: 1.0\n",
            "Epoch 976, BestLoss: 2.9686828261635676e-22, Temperature 2.008128738141728e-22, step_size 0.09112803910128361, test_acc: 1.0\n",
            "Epoch 977, BestLoss: 3.5177382352818347e-22, Temperature 1.9077223012346415e-22, step_size 0.09111892629737349, test_acc: 1.0\n",
            "Epoch 978, BestLoss: 5.060577542097483e-22, Temperature 1.8123361861729093e-22, step_size 0.09110981440474375, test_acc: 1.0\n",
            "Epoch 979, BestLoss: 3.1829389424089667e-22, Temperature 1.7217193768642638e-22, step_size 0.09110070342330327, test_acc: 1.0\n",
            "Epoch 980, BestLoss: 5.2812799944296535e-22, Temperature 1.6356334080210505e-22, step_size 0.09109159335296094, test_acc: 1.0\n",
            "Epoch 981, BestLoss: 3.2380147626538263e-22, Temperature 1.5538517376199979e-22, step_size 0.09108248419362565, test_acc: 1.0\n",
            "Epoch 982, BestLoss: 5.70283742835964e-22, Temperature 1.476159150738998e-22, step_size 0.09107337594520629, test_acc: 1.0\n",
            "Epoch 983, BestLoss: 3.3865235827976887e-22, Temperature 1.402351193202048e-22, step_size 0.09106426860761177, test_acc: 1.0\n",
            "Epoch 984, BestLoss: 3.7632391465795265e-22, Temperature 1.3322336335419457e-22, step_size 0.091055162180751, test_acc: 1.0\n",
            "Epoch 985, BestLoss: 6.26490330815799e-22, Temperature 1.2656219518648483e-22, step_size 0.09104605666453293, test_acc: 1.0\n",
            "Epoch 986, BestLoss: 2.7668110916138467e-22, Temperature 1.202340854271606e-22, step_size 0.09103695205886647, test_acc: 1.0\n",
            "Epoch 987, BestLoss: 2.4339652579852085e-22, Temperature 1.1422238115580257e-22, step_size 0.09102784836366058, test_acc: 1.0\n",
            "Epoch 988, BestLoss: 2.609725107760899e-22, Temperature 1.0851126209801244e-22, step_size 0.09101874557882422, test_acc: 1.0\n",
            "Epoch 989, BestLoss: 3.201014100274504e-22, Temperature 1.0308569899311182e-22, step_size 0.09100964370426634, test_acc: 1.0\n",
            "Epoch 990, BestLoss: 3.8271940833160263e-22, Temperature 9.793141404345622e-23, step_size 0.09100054273989591, test_acc: 1.0\n",
            "Epoch 991, BestLoss: 3.784304701162492e-22, Temperature 9.30348433412834e-23, step_size 0.09099144268562193, test_acc: 1.0\n",
            "Epoch 992, BestLoss: 5.046196853562525e-22, Temperature 8.838310117421923e-23, step_size 0.09098234354135336, test_acc: 1.0\n",
            "Epoch 993, BestLoss: 3.136771830702562e-22, Temperature 8.396394611550826e-23, step_size 0.09097324530699923, test_acc: 1.0\n",
            "Epoch 994, BestLoss: 2.2339949520678345e-22, Temperature 7.976574880973285e-23, step_size 0.09096414798246853, test_acc: 1.0\n",
            "Epoch 995, BestLoss: 2.851665049490811e-22, Temperature 7.57774613692462e-23, step_size 0.09095505156767028, test_acc: 1.0\n",
            "Epoch 996, BestLoss: 1.4853391893611764e-22, Temperature 7.198858830078389e-23, step_size 0.09094595606251352, test_acc: 1.0\n",
            "Epoch 997, BestLoss: 1.201412919872168e-22, Temperature 6.838915888574469e-23, step_size 0.09093686146690727, test_acc: 1.0\n",
            "Epoch 998, BestLoss: 3.4361932100802813e-22, Temperature 6.496970094145745e-23, step_size 0.09092776778076059, test_acc: 1.0\n",
            "Epoch 999, BestLoss: 2.2124144829625815e-22, Temperature 6.172121589438457e-23, step_size 0.09091867500398251, test_acc: 1.0\n",
            "expected [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "preds [[1.22159456e-11]\n",
            " [1.00000000e+00]\n",
            " [1.00000000e+00]\n",
            " [3.44053182e-11]\n",
            " [1.00000000e+00]\n",
            " [1.22159456e-11]\n",
            " [3.44053182e-11]]\n"
          ]
        }
      ],
      "source": [
        "# XOR\n",
        "np.random.seed(4)\n",
        "layers = [\n",
        "    nl.SADense(2, 3),\n",
        "    nl.Sigmoid(),\n",
        "    nl.SADense(3, 1),\n",
        "    nl.Sigmoid(),\n",
        "]\n",
        "\n",
        "net = nl.NeuralNet(layers)\n",
        "optimizer = SimulatedAnnealingOptimizer(net, learning_rate = 0.1, loss=nl.MeanSquaredError())\n",
        "\n",
        "optimizer.fit((xor_x_train, xor_y_train), (xor_x_train, xor_y_train), epochs=1000, batch_size=60000, sample_per_batch=20, initial_temp=1.0, cooling_rate=0.95, verbose=True)\n",
        "# Infer on cx_test, cy_test\n",
        "preds = net(xor_x_train)\n",
        "expected = xor_y_train\n",
        "\n",
        "print(\"expected\", expected)\n",
        "print(\"preds\", preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRm8RtSe2V5U"
      },
      "outputs": [],
      "source": [
        "# MNIST\n",
        "np.random.seed(4)\n",
        "layers = [\n",
        "    nl.SADense(784, 128),\n",
        "    nl.ReLU(),\n",
        "    nl.SADense(128, 10),\n",
        "    nl.Softmax(),\n",
        "]\n",
        "\n",
        "net = nl.NeuralNet(layers)\n",
        "optimizer = SimulatedAnnealingOptimizer(net, learning_rate = 1, loss=nl.MeanSquaredError())\n",
        "\n",
        "optimizer.fit((cx_train, cy_train), (cx_test, cy_test), epochs=10000, batch_size=60000, sample_per_batch=1, initial_temp=1.0, cooling_rate=0.95, verbose=True)\n",
        "# Infer on cx_test, cy_test\n",
        "predictions = net.forward(cx_test)\n",
        "preds = np.array(nl.antiCategorical(predictions))\n",
        "expected = np.array(nl.antiCategorical(cy_test))\n",
        "\n",
        "print(f'Accuracy: {sklearn.metrics.accuracy_score(expected.get(), preds.get())}')\n",
        "print(f'Confusion Matrix: {sklearn.metrics.confusion_matrix(expected.get(), preds.get())}')\n",
        "print(f'Classification Report: {sklearn.metrics.classification_report(expected.get(), preds.get())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihnJ0B6oDT7X"
      },
      "source": [
        "# Extreme Learning Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1VGWIuDRDbW8"
      },
      "outputs": [],
      "source": [
        "# Extreme Learning Machines\n",
        "class ELM_Optimizer(nl.Optimizer):\n",
        "    def __init__(self, model, learning_rate, loss):\n",
        "        super().__init__(model, loss)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def train_step(self, x_batch, y_batch, alpha=0):\n",
        "        predictions = self.model.forward(x_batch)\n",
        "        loss_value = self.loss.forward(predictions, y_batch)\n",
        "\n",
        "        # Smooth out y_batch\n",
        "        y_batch = np.where(y_batch > 0.5, 0.9, 0.1)\n",
        "        expected = y_batch\n",
        "        for layer in reversed(self.model.get_layers()):\n",
        "            if isinstance(layer, nl.Dense):\n",
        "                x_inv = np.linalg.pinv(layer.last_inputs)\n",
        "                weight_approx = np.dot(x_inv, expected)\n",
        "                layer.weights = layer.weights * alpha + weight_approx * (1 - alpha)\n",
        "                # layer.weights = weight_approx\n",
        "                # expected = layer.last_inputs\n",
        "                expected = layer.inverse(expected)\n",
        "            else:\n",
        "                expected = layer.inverse(expected)\n",
        "        return loss_value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hplic86C31-r",
        "outputId": "8f789b3b-06d0-4908-af3c-94c309b11d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.16489561130438207, test_acc: 1.0\n",
            "Epoch 1, Loss: 0.14321428571428574, test_acc: 1.0\n",
            "Epoch 2, Loss: 0.1432142857142857, test_acc: 1.0\n",
            "Epoch 3, Loss: 0.1432142857142857, test_acc: 1.0\n",
            "Epoch 4, Loss: 0.1432142857142857, test_acc: 1.0\n",
            "Epoch 5, Loss: 0.14321428571428574, test_acc: 1.0\n",
            "Epoch 6, Loss: 0.1432142857142857, test_acc: 1.0\n",
            "Epoch 7, Loss: 0.1432142857142857, test_acc: 1.0\n",
            "Epoch 8, Loss: 0.1432142857142857, test_acc: 1.0\n",
            "Epoch 9, Loss: 0.1432142857142857, test_acc: 1.0\n",
            "expected [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "preds [[0.   ]\n",
            " [0.05 ]\n",
            " [0.475]\n",
            " [0.525]\n",
            " [0.475]\n",
            " [0.   ]\n",
            " [0.525]]\n"
          ]
        }
      ],
      "source": [
        "# XOR\n",
        "np.random.seed(4)\n",
        "layers = [\n",
        "    nl.Dense(2, 3, False),\n",
        "    nl.LeakyReLU(),\n",
        "    nl.Dense(3, 1, False),\n",
        "    nl.LeakyReLU(),\n",
        "]\n",
        "\n",
        "net = nl.NeuralNet(layers)\n",
        "optimizer = ELM_Optimizer(net, learning_rate = 0.1, loss=nl.MeanSquaredError())\n",
        "\n",
        "optimizer.fit((xor_x_train, xor_y_train), (xor_x_train, xor_y_train), epochs=10, batch_size=60000, verbose=True)\n",
        "# Infer on cx_test, cy_test\n",
        "preds = net(xor_x_train)\n",
        "expected = xor_y_train\n",
        "\n",
        "print(\"expected\", expected)\n",
        "print(\"preds\", preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFjPB4kq30EY",
        "outputId": "63096f35-71c1-421f-ce48-d72d52847f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.10928068989420225, test_acc: 0.8509\n",
            "Accuracy: 0.8509\n",
            "Confusion Matrix: [[ 936    0    3    3    2    9   17    1    7    2]\n",
            " [   0 1108    2    2    1    1    5    1   14    1]\n",
            " [  18   59  805   29   15    0   38   24   39    5]\n",
            " [   5   19   24  875    1   19    9   20   24   14]\n",
            " [   0   24    8    4  866    5    9    2   14   50]\n",
            " [  20   15    6   77   16  632   22   14   66   24]\n",
            " [  19   10   10    0   19   21  870    0    9    0]\n",
            " [   5   40   18    6   22    1    1  868    5   62]\n",
            " [  16   56    9   32   26   47   15   11  736   26]\n",
            " [  19   11    3   15   69    0    1   67   11  813]]\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93       980\n",
            "           1       0.83      0.98      0.89      1135\n",
            "           2       0.91      0.78      0.84      1032\n",
            "           3       0.84      0.87      0.85      1010\n",
            "           4       0.84      0.88      0.86       982\n",
            "           5       0.86      0.71      0.78       892\n",
            "           6       0.88      0.91      0.89       958\n",
            "           7       0.86      0.84      0.85      1028\n",
            "           8       0.80      0.76      0.78       974\n",
            "           9       0.82      0.81      0.81      1009\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# MNIST\n",
        "np.random.seed(4)\n",
        "layers = [\n",
        "    Dense(784, 128, False),\n",
        "    # LeakyReLU(),\n",
        "    # Linear(),\n",
        "    # Dense(128, 128, False),\n",
        "    # LeakyReLU(),\n",
        "    # Dense(128, 10, False),\n",
        "    # Sigmoid(),\n",
        "]\n",
        "\n",
        "net = NeuralNet(layers)\n",
        "optimizer = ELM_Optimizer(net, learning_rate = 0.01, loss=MeanSquaredError())\n",
        "\n",
        "optimizer.fit((cx_train, cy_train), (cx_test, cy_test), epochs=1, batch_size=60000, verbose=True)\n",
        "# Infer on cx_test, cy_test\n",
        "predictions = net.forward(cx_test)\n",
        "preds = np.array(antiCategorical(predictions))\n",
        "expected = np.array(antiCategorical(cy_test))\n",
        "\n",
        "print(f'Accuracy: {sklearn.metrics.accuracy_score(expected.get(), preds.get())}')\n",
        "print(f'Confusion Matrix: {sklearn.metrics.confusion_matrix(expected.get(), preds.get())}')\n",
        "print(f'Classification Report: {sklearn.metrics.classification_report(expected.get(), preds.get())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "2f5L3HXfD3ku"
      },
      "outputs": [],
      "source": [
        "# Given matrices\n",
        "a = cy_train[:30000]\n",
        "b = cx_train[:30000]\n",
        "c = cy_train[30000:]\n",
        "d = cx_train[30000:]\n",
        "\n",
        "# Randomly initialize x and y\n",
        "x = np.random.rand(b.shape[1], c.shape[1])\n",
        "y = np.random.rand(a.shape[0], c.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uybi_KwYC0dr",
        "outputId": "8abeaa53-c080-4551-8c48-683efe7fdeaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8534\n",
            "Confusion Matrix: [[ 942    0    2    2    1    7   15    2    7    2]\n",
            " [   0 1107    2    2    1    1    5    2   15    0]\n",
            " [  17   56  809   28   16    0   42   21   39    4]\n",
            " [   4   15   26  887    2   14    9   21   21   11]\n",
            " [   0   23    6    3  872    5   10    2   13   48]\n",
            " [  20   17    2   84   19  624   22   13   69   22]\n",
            " [  17    9   10    0   21   20  872    0    9    0]\n",
            " [   5   38   18    8   20    0    1  877    3   58]\n",
            " [  17   54    9   32   27   42   15   12  743   23]\n",
            " [  18   10    2   15   72    1    1   76   13  801]]\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       980\n",
            "           1       0.83      0.98      0.90      1135\n",
            "           2       0.91      0.78      0.84      1032\n",
            "           3       0.84      0.88      0.86      1010\n",
            "           4       0.83      0.89      0.86       982\n",
            "           5       0.87      0.70      0.78       892\n",
            "           6       0.88      0.91      0.89       958\n",
            "           7       0.85      0.85      0.85      1028\n",
            "           8       0.80      0.76      0.78       974\n",
            "           9       0.83      0.79      0.81      1009\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Calculate intermediate variables\n",
        "# z1 = np.dot(b, x)\n",
        "# z2 = np.dot(d, x)\n",
        "z1 = b\n",
        "z2 = d\n",
        "\n",
        "# Form stacked matrices\n",
        "A = np.vstack((a, c))\n",
        "Z = np.vstack((z1, z2))\n",
        "\n",
        "# Compute pseudoinverse and solve for y\n",
        "Z_pseudo_inv = np.linalg.pinv(Z)\n",
        "y = np.dot(Z_pseudo_inv, A)\n",
        "\n",
        "# Substitute back to solve for x\n",
        "# B = np.vstack((b, d))\n",
        "# Z_prime = np.vstack((np.dot(b, x), np.dot(d, x)))\n",
        "# B_pseudo_inv = np.linalg.pinv(B)\n",
        "# x = np.dot(B_pseudo_inv, Z_prime)\n",
        "\n",
        "# output = np.dot(cx_test, x)\n",
        "output = np.dot(cx_test, y)\n",
        "\n",
        "preds = np.array(antiCategorical(output))\n",
        "expected = np.array(antiCategorical(cy_test))\n",
        "\n",
        "print(f'Accuracy: {sklearn.metrics.accuracy_score(expected.get(), preds.get())}')\n",
        "print(f'Confusion Matrix: {sklearn.metrics.confusion_matrix(expected.get(), preds.get())}')\n",
        "print(f'Classification Report: {sklearn.metrics.classification_report(expected.get(), preds.get())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkzlISnxmDe2"
      },
      "source": [
        "# ELM + Simulated Annealing Hybrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "0tWEJtvmmHUH"
      },
      "outputs": [],
      "source": [
        "# Extreme Learning Machines Hybrid\n",
        "class ELM_SA_Optimizer(SimulatedAnnealingOptimizer):\n",
        "    def __init__(self, model, learning_rate, loss):\n",
        "        super().__init__(model, learning_rate, loss)\n",
        "\n",
        "    def train_step(self, x_batch, y_batch, alpha=0.8):\n",
        "        predictions = self.model.forward(x_batch)\n",
        "        # loss_value = self.loss.forward(predictions, y_batch)\n",
        "\n",
        "        # Smooth out y_batch\n",
        "        y_batch = np.where(y_batch > 0.5, 0.9, 0.1)\n",
        "        expected = y_batch\n",
        "        for layer in reversed(self.model.get_layers()):\n",
        "            if isinstance(layer, Dense):\n",
        "                x_inv = np.linalg.pinv(layer.last_inputs)\n",
        "                weight_approx = np.dot(x_inv, expected)\n",
        "                layer.weights = layer.weights * alpha + weight_approx * (1 - alpha)\n",
        "                # layer.weights = weight_approx\n",
        "                # expected = layer.last_inputs\n",
        "                expected = layer.inverse(expected)\n",
        "            else:\n",
        "                expected = layer.inverse(expected)\n",
        "\n",
        "        loss_value = super().train_step(x_batch, y_batch)\n",
        "        return loss_value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1_zDDNt_mUW4",
        "outputId": "b9fcf645-bfe8-46c4-c743-8b7149f36d27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, BestLoss: 0.18332151059265014, Temperature 1.0, step_size 1, test_acc: 0.0949\n",
            "Epoch 1, BestLoss: 0.2409566013195773, Temperature 0.95, step_size 0.9999, test_acc: 0.1291\n",
            "Epoch 2, BestLoss: 0.23312620153584057, Temperature 0.9025, step_size 0.9998000100000001, test_acc: 0.101\n",
            "Epoch 3, BestLoss: 0.21234784144866736, Temperature 0.8573749999999999, step_size 0.9997000299990001, test_acc: 0.0721\n",
            "Epoch 4, BestLoss: 0.20036266550948745, Temperature 0.8145062499999999, step_size 0.9996000599960002, test_acc: 0.0824\n",
            "Epoch 5, BestLoss: 0.22088384759242521, Temperature 0.7737809374999999, step_size 0.9995000999900007, test_acc: 0.0419\n",
            "Epoch 6, BestLoss: 0.18670273352967434, Temperature 0.7350918906249998, step_size 0.9994001499800017, test_acc: 0.065\n",
            "Epoch 7, BestLoss: 0.18920608779592904, Temperature 0.6983372960937497, step_size 0.9993002099650037, test_acc: 0.0987\n",
            "Epoch 8, BestLoss: 0.19674538165629463, Temperature 0.6634204312890623, step_size 0.9992002799440072, test_acc: 0.0846\n",
            "Epoch 9, BestLoss: 0.2050671946657237, Temperature 0.6302494097246091, step_size 0.9991003599160128, test_acc: 0.1315\n",
            "Epoch 10, BestLoss: 0.217624886730905, Temperature 0.5987369392383786, step_size 0.9990004498800211, test_acc: 0.164\n",
            "Epoch 11, BestLoss: 0.20820868343615057, Temperature 0.5688000922764596, step_size 0.9989005498350332, test_acc: 0.1142\n",
            "Epoch 12, BestLoss: 0.20820868343615057, Temperature 0.5403600876626365, step_size 0.9988006597800497, test_acc: 0.1087\n",
            "Epoch 13, BestLoss: 0.24743846060821048, Temperature 0.5688000922764596, step_size 1, test_acc: 0.1109\n",
            "Epoch 14, BestLoss: 0.16679051158004535, Temperature 0.5403600876626365, step_size 0.9999, test_acc: 0.102\n",
            "Epoch 15, BestLoss: 0.18091339159367426, Temperature 0.5133420832795047, step_size 0.9998000100000001, test_acc: 0.0742\n",
            "Epoch 16, BestLoss: 0.22183750741512262, Temperature 0.48767497911552943, step_size 0.9997000299990001, test_acc: 0.045\n",
            "Epoch 17, BestLoss: 0.21533005363183658, Temperature 0.46329123015975293, step_size 0.9996000599960002, test_acc: 0.0683\n",
            "Epoch 18, BestLoss: 0.20377294068168644, Temperature 0.44012666865176525, step_size 0.9995000999900007, test_acc: 0.1105\n",
            "Epoch 19, BestLoss: 0.20860277909722663, Temperature 0.41812033521917696, step_size 0.9994001499800017, test_acc: 0.1287\n",
            "Epoch 20, BestLoss: 0.18070472627008966, Temperature 0.3972143184582181, step_size 0.9993002099650037, test_acc: 0.0658\n",
            "Epoch 21, BestLoss: 0.12835330629287497, Temperature 0.37735360253530714, step_size 0.9992002799440072, test_acc: 0.0782\n",
            "Epoch 22, BestLoss: 0.1763003880383555, Temperature 0.35848592240854177, step_size 0.9991003599160128, test_acc: 0.0544\n",
            "Epoch 23, BestLoss: 0.1763003880383555, Temperature 0.34056162628811465, step_size 0.9990004498800211, test_acc: 0.0601\n",
            "Epoch 24, BestLoss: 0.16540836908589235, Temperature 0.35848592240854177, step_size 0.9999, test_acc: 0.0323\n",
            "Epoch 25, BestLoss: 0.16540836908589235, Temperature 0.34056162628811465, step_size 0.9998000100000001, test_acc: 0.034\n",
            "Epoch 26, BestLoss: 0.15962824161322464, Temperature 0.35848592240854177, step_size 0.9998000100000001, test_acc: 0.0426\n",
            "Epoch 27, BestLoss: 0.1709145222019245, Temperature 0.34056162628811465, step_size 0.9997000299990001, test_acc: 0.0517\n",
            "Epoch 28, BestLoss: 0.14802724246282095, Temperature 0.3235335449737089, step_size 0.9996000599960002, test_acc: 0.1197\n",
            "Epoch 29, BestLoss: 0.13163030910661602, Temperature 0.30735686772502346, step_size 0.9995000999900007, test_acc: 0.1186\n",
            "Epoch 30, BestLoss: 0.1434259676545826, Temperature 0.2919890243387723, step_size 0.9994001499800017, test_acc: 0.0805\n",
            "Epoch 31, BestLoss: 0.1434259676545826, Temperature 0.27738957312183365, step_size 0.9993002099650037, test_acc: 0.084\n",
            "Epoch 32, BestLoss: 0.17008511148723005, Temperature 0.2919890243387723, step_size 0.9997000299990001, test_acc: 0.1541\n",
            "Epoch 33, BestLoss: 0.156793333334567, Temperature 0.27738957312183365, step_size 0.9996000599960002, test_acc: 0.08\n",
            "Epoch 34, BestLoss: 0.140440493977072, Temperature 0.263520094465742, step_size 0.9995000999900007, test_acc: 0.0788\n",
            "Epoch 35, BestLoss: 0.1369709842667985, Temperature 0.25034408974245487, step_size 0.9994001499800017, test_acc: 0.0976\n",
            "Epoch 36, BestLoss: 0.14993824956256846, Temperature 0.2378268852553321, step_size 0.9993002099650037, test_acc: 0.0828\n",
            "Epoch 37, BestLoss: 0.18106718358369234, Temperature 0.2259355409925655, step_size 0.9992002799440072, test_acc: 0.04\n",
            "Epoch 38, BestLoss: 0.16465725302708042, Temperature 0.2146387639429372, step_size 0.9991003599160128, test_acc: 0.028\n",
            "Epoch 39, BestLoss: 0.22363767286432543, Temperature 0.20390682574579033, step_size 0.9990004498800211, test_acc: 0.0201\n",
            "Epoch 40, BestLoss: 0.24524992705791016, Temperature 0.1937114844585008, step_size 0.9989005498350332, test_acc: 0.0086\n",
            "Epoch 41, BestLoss: 0.2648419338660049, Temperature 0.18402591023557577, step_size 0.9988006597800497, test_acc: 0.0386\n",
            "Epoch 42, BestLoss: 0.22559288773823336, Temperature 0.17482461472379698, step_size 0.9987007797140718, test_acc: 0.0411\n",
            "Epoch 43, BestLoss: 0.2184325475639922, Temperature 0.16608338398760714, step_size 0.9986009096361004, test_acc: 0.0754\n",
            "Epoch 44, BestLoss: 0.21938996918445725, Temperature 0.15777921478822676, step_size 0.9985010495451367, test_acc: 0.0962\n",
            "Epoch 45, BestLoss: 0.19143913023276676, Temperature 0.14989025404881542, step_size 0.9984011994401822, test_acc: 0.0903\n",
            "Epoch 46, BestLoss: 0.19174028889276734, Temperature 0.14239574134637464, step_size 0.9983013593202382, test_acc: 0.0896\n",
            "Epoch 47, BestLoss: 0.22817880674677105, Temperature 0.1352759542790559, step_size 0.9982015291843062, test_acc: 0.1271\n",
            "Epoch 48, BestLoss: 0.25378821646205346, Temperature 0.1285121565651031, step_size 0.9981017090313877, test_acc: 0.1034\n",
            "Epoch 49, BestLoss: 0.22243144039576407, Temperature 0.12208654873684793, step_size 0.9980018988604846, test_acc: 0.1013\n",
            "Epoch 50, BestLoss: 0.20363244895067806, Temperature 0.11598222130000553, step_size 0.9979020986705985, test_acc: 0.1236\n",
            "Epoch 51, BestLoss: 0.1968413926345728, Temperature 0.11018311023500525, step_size 0.9978023084607315, test_acc: 0.118\n",
            "Epoch 52, BestLoss: 0.22103423175797157, Temperature 0.10467395472325498, step_size 0.9977025282298854, test_acc: 0.1238\n",
            "Epoch 53, BestLoss: 0.192255458519247, Temperature 0.09944025698709223, step_size 0.9976027579770624, test_acc: 0.0983\n",
            "Epoch 54, BestLoss: 0.17263123992778626, Temperature 0.09446824413773762, step_size 0.9975029977012647, test_acc: 0.1155\n",
            "Epoch 55, BestLoss: 0.22117628536970946, Temperature 0.08974483193085074, step_size 0.9974032474014946, test_acc: 0.131\n",
            "Epoch 56, BestLoss: 0.1863616396991053, Temperature 0.0852575903343082, step_size 0.9973035070767544, test_acc: 0.11\n",
            "Epoch 57, BestLoss: 0.22419385339172299, Temperature 0.08099471081759278, step_size 0.9972037767260468, test_acc: 0.161\n",
            "Epoch 58, BestLoss: 0.19908845397631947, Temperature 0.07694497527671314, step_size 0.9971040563483742, test_acc: 0.1619\n",
            "Epoch 59, BestLoss: 0.1950628867730104, Temperature 0.07309772651287748, step_size 0.9970043459427393, test_acc: 0.1199\n",
            "Epoch 60, BestLoss: 0.19925673458302545, Temperature 0.0694428401872336, step_size 0.9969046455081451, test_acc: 0.0972\n",
            "Epoch 61, BestLoss: 0.2060983154469786, Temperature 0.0659706981778719, step_size 0.9968049550435942, test_acc: 0.0802\n",
            "Epoch 62, BestLoss: 0.2116804599347443, Temperature 0.0626721632689783, step_size 0.9967052745480899, test_acc: 0.0782\n",
            "Epoch 63, BestLoss: 0.20677237995807216, Temperature 0.059538555105529384, step_size 0.9966056040206351, test_acc: 0.0734\n",
            "Epoch 64, BestLoss: 0.20677237995807216, Temperature 0.05656162735025291, step_size 0.9965059434602331, test_acc: 0.0718\n",
            "Epoch 65, BestLoss: 0.15431100115235238, Temperature 0.059538555105529384, step_size 0.9996000599960002, test_acc: 0.1393\n",
            "Epoch 66, BestLoss: 0.19087527900126222, Temperature 0.05656162735025291, step_size 0.9995000999900007, test_acc: 0.0688\n",
            "Epoch 67, BestLoss: 0.18274962455106647, Temperature 0.053733545982740265, step_size 0.9994001499800017, test_acc: 0.078\n",
            "Epoch 68, BestLoss: 0.21002100502419854, Temperature 0.05104686868360325, step_size 0.9993002099650037, test_acc: 0.1014\n",
            "Epoch 69, BestLoss: 0.1864296270473025, Temperature 0.04849452524942309, step_size 0.9992002799440072, test_acc: 0.0841\n",
            "Epoch 70, BestLoss: 0.1864296270473025, Temperature 0.04606979898695193, step_size 0.9991003599160128, test_acc: 0.0876\n",
            "Epoch 71, BestLoss: 0.1864296270473025, Temperature 0.04849452524942309, step_size 0.9995000999900007, test_acc: 0.0934\n",
            "Epoch 72, BestLoss: 0.17167448231137106, Temperature 0.05104686868360325, step_size 0.9994001499800017, test_acc: 0.1192\n",
            "Epoch 73, BestLoss: 0.21958011457013685, Temperature 0.04849452524942309, step_size 0.9993002099650037, test_acc: 0.0923\n",
            "Epoch 74, BestLoss: 0.21958011457013685, Temperature 0.04606979898695193, step_size 0.9992002799440072, test_acc: 0.0916\n",
            "Epoch 75, BestLoss: 0.2121814826763579, Temperature 0.04849452524942309, step_size 0.9993002099650037, test_acc: 0.125\n",
            "Epoch 76, BestLoss: 0.21274014725284182, Temperature 0.04606979898695193, step_size 0.9992002799440072, test_acc: 0.1006\n",
            "Epoch 77, BestLoss: 0.18576672384225099, Temperature 0.04376630903760433, step_size 0.9991003599160128, test_acc: 0.1473\n",
            "Epoch 78, BestLoss: 0.18576672384225099, Temperature 0.041577993585724116, step_size 0.9990004498800211, test_acc: 0.1508\n",
            "Epoch 79, BestLoss: 0.18576672384225099, Temperature 0.04376630903760433, step_size 0.9992002799440072, test_acc: 0.1579\n",
            "Epoch 80, BestLoss: 0.20973951693141432, Temperature 0.04606979898695193, step_size 0.9991003599160128, test_acc: 0.2014\n",
            "Epoch 81, BestLoss: 0.20550154915971341, Temperature 0.04376630903760433, step_size 0.9990004498800211, test_acc: 0.1837\n",
            "Epoch 82, BestLoss: 0.17751276745870836, Temperature 0.041577993585724116, step_size 0.9989005498350332, test_acc: 0.2028\n",
            "Epoch 83, BestLoss: 0.18974179321534732, Temperature 0.03949909390643791, step_size 0.9988006597800497, test_acc: 0.1945\n",
            "Epoch 84, BestLoss: 0.20510336421953576, Temperature 0.03752413921111601, step_size 0.9987007797140718, test_acc: 0.1664\n",
            "Epoch 85, BestLoss: 0.1829986066050327, Temperature 0.03564793225056021, step_size 0.9986009096361004, test_acc: 0.1656\n",
            "Epoch 86, BestLoss: 0.1829986066050327, Temperature 0.0338655356380322, step_size 0.9985010495451367, test_acc: 0.1674\n",
            "Epoch 87, BestLoss: 0.1829986066050327, Temperature 0.03564793225056021, step_size 0.9990004498800211, test_acc: 0.1642\n",
            "Epoch 88, BestLoss: 0.1812349628132282, Temperature 0.03752413921111601, step_size 0.9989005498350332, test_acc: 0.1929\n",
            "Epoch 89, BestLoss: 0.1311078281000299, Temperature 0.03564793225056021, step_size 0.9988006597800497, test_acc: 0.1622\n",
            "Epoch 90, BestLoss: 0.1411107354451519, Temperature 0.0338655356380322, step_size 0.9987007797140718, test_acc: 0.1186\n",
            "Epoch 91, BestLoss: 0.1411107354451519, Temperature 0.032172258856130585, step_size 0.9986009096361004, test_acc: 0.1188\n",
            "Epoch 92, BestLoss: 0.1411107354451519, Temperature 0.0338655356380322, step_size 0.9988006597800497, test_acc: 0.1229\n",
            "Epoch 93, BestLoss: 0.1411107354451519, Temperature 0.03564793225056021, step_size 0.9987007797140718, test_acc: 0.1252\n",
            "Epoch 94, BestLoss: 0.22379110250030868, Temperature 0.03752413921111601, step_size 0.9986009096361004, test_acc: 0.043\n",
            "Epoch 95, BestLoss: 0.21602259295254655, Temperature 0.03564793225056021, step_size 0.9985010495451367, test_acc: 0.0745\n",
            "Epoch 96, BestLoss: 0.19939765246337907, Temperature 0.0338655356380322, step_size 0.9984011994401822, test_acc: 0.0659\n",
            "Epoch 97, BestLoss: 0.19076599972748978, Temperature 0.032172258856130585, step_size 0.9983013593202382, test_acc: 0.0496\n",
            "Epoch 98, BestLoss: 0.18010165476401696, Temperature 0.030563645913324056, step_size 0.9982015291843062, test_acc: 0.1082\n",
            "Epoch 99, BestLoss: 0.1855289734842588, Temperature 0.029035463617657853, step_size 0.9981017090313877, test_acc: 0.0889\n",
            "Epoch 100, BestLoss: 0.1434225078583362, Temperature 0.027583690436774957, step_size 0.9980018988604846, test_acc: 0.0557\n",
            "Epoch 101, BestLoss: 0.1434225078583362, Temperature 0.02620450591493621, step_size 0.9979020986705985, test_acc: 0.0536\n",
            "Epoch 102, BestLoss: 0.17765105692822525, Temperature 0.027583690436774957, step_size 0.9985010495451367, test_acc: 0.0992\n",
            "Epoch 103, BestLoss: 0.1876374724051958, Temperature 0.02620450591493621, step_size 0.9984011994401822, test_acc: 0.0944\n",
            "Epoch 104, BestLoss: 0.20532505229090472, Temperature 0.0248942806191894, step_size 0.9983013593202382, test_acc: 0.0931\n",
            "Epoch 105, BestLoss: 0.20532505229090472, Temperature 0.023649566588229927, step_size 0.9982015291843062, test_acc: 0.0955\n",
            "Epoch 106, BestLoss: 0.21546988917833396, Temperature 0.0248942806191894, step_size 0.9984011994401822, test_acc: 0.0895\n",
            "Epoch 107, BestLoss: 0.194676369920477, Temperature 0.023649566588229927, step_size 0.9983013593202382, test_acc: 0.0599\n",
            "Epoch 108, BestLoss: 0.19118031033788313, Temperature 0.022467088258818428, step_size 0.9982015291843062, test_acc: 0.0673\n",
            "Epoch 109, BestLoss: 0.19118031033788313, Temperature 0.021343733845877507, step_size 0.9981017090313877, test_acc: 0.0602\n",
            "Epoch 110, BestLoss: 0.1919249542444511, Temperature 0.022467088258818428, step_size 0.9983013593202382, test_acc: 0.0728\n",
            "Epoch 111, BestLoss: 0.1695258297198312, Temperature 0.021343733845877507, step_size 0.9982015291843062, test_acc: 0.066\n",
            "Epoch 112, BestLoss: 0.1695258297198312, Temperature 0.02027654715358363, step_size 0.9981017090313877, test_acc: 0.0702\n",
            "Epoch 113, BestLoss: 0.20810573010746777, Temperature 0.021343733845877507, step_size 0.9982015291843062, test_acc: 0.0454\n",
            "Epoch 114, BestLoss: 0.21819584807063014, Temperature 0.02027654715358363, step_size 0.9981017090313877, test_acc: 0.0598\n",
            "Epoch 115, BestLoss: 0.18500002250954276, Temperature 0.019262719795904448, step_size 0.9980018988604846, test_acc: 0.0858\n",
            "Epoch 116, BestLoss: 0.16621777845872523, Temperature 0.018299583806109226, step_size 0.9979020986705985, test_acc: 0.082\n",
            "Epoch 117, BestLoss: 0.15667192748003006, Temperature 0.017384604615803764, step_size 0.9978023084607315, test_acc: 0.1378\n",
            "Epoch 118, BestLoss: 0.15667192748003006, Temperature 0.016515374385013576, step_size 0.9977025282298854, test_acc: 0.1458\n",
            "Epoch 119, BestLoss: 0.13690989525762662, Temperature 0.017384604615803764, step_size 0.9981017090313877, test_acc: 0.1247\n",
            "Epoch 120, BestLoss: 0.16217632955784628, Temperature 0.016515374385013576, step_size 0.9980018988604846, test_acc: 0.1514\n",
            "Epoch 121, BestLoss: 0.14720464892516613, Temperature 0.015689605665762895, step_size 0.9979020986705985, test_acc: 0.1178\n",
            "Epoch 122, BestLoss: 0.14720464892516613, Temperature 0.01490512538247475, step_size 0.9978023084607315, test_acc: 0.1226\n",
            "Epoch 123, BestLoss: 0.14720464892516613, Temperature 0.015689605665762895, step_size 0.9980018988604846, test_acc: 0.1212\n",
            "Epoch 124, BestLoss: 0.14720464892516613, Temperature 0.016515374385013576, step_size 0.9979020986705985, test_acc: 0.103\n",
            "Epoch 125, BestLoss: 0.14720464892516613, Temperature 0.017384604615803764, step_size 0.9978023084607315, test_acc: 0.0897\n",
            "Epoch 126, BestLoss: 0.1305323176890788, Temperature 0.018299583806109226, step_size 0.9977025282298854, test_acc: 0.1047\n",
            "Epoch 127, BestLoss: 0.1305323176890788, Temperature 0.017384604615803764, step_size 0.9976027579770624, test_acc: 0.1026\n",
            "Epoch 128, BestLoss: 0.1469976550075056, Temperature 0.018299583806109226, step_size 0.9976027579770624, test_acc: 0.0955\n",
            "Epoch 129, BestLoss: 0.1469976550075056, Temperature 0.017384604615803764, step_size 0.9975029977012647, test_acc: 0.1037\n",
            "Epoch 130, BestLoss: 0.1469976550075056, Temperature 0.018299583806109226, step_size 0.9975029977012647, test_acc: 0.1126\n",
            "Epoch 131, BestLoss: 0.14679213072786018, Temperature 0.019262719795904448, step_size 0.9974032474014946, test_acc: 0.0688\n",
            "Epoch 132, BestLoss: 0.14679213072786018, Temperature 0.018299583806109226, step_size 0.9973035070767544, test_acc: 0.073\n",
            "Epoch 133, BestLoss: 0.11459345520345786, Temperature 0.019262719795904448, step_size 0.9973035070767544, test_acc: 0.1052\n",
            "Epoch 134, BestLoss: 0.11459345520345786, Temperature 0.018299583806109226, step_size 0.9972037767260468, test_acc: 0.1067\n",
            "Epoch 135, BestLoss: 0.11459345520345786, Temperature 0.019262719795904448, step_size 0.9972037767260468, test_acc: 0.1152\n",
            "Epoch 136, BestLoss: 0.11459345520345786, Temperature 0.02027654715358363, step_size 0.9971040563483742, test_acc: 0.1265\n",
            "Epoch 137, BestLoss: 0.11459345520345786, Temperature 0.021343733845877507, step_size 0.9970043459427393, test_acc: 0.1348\n",
            "Epoch 138, BestLoss: 0.11459345520345786, Temperature 0.022467088258818428, step_size 0.9969046455081451, test_acc: 0.1459\n",
            "Epoch 139, BestLoss: 0.11459345520345786, Temperature 0.023649566588229927, step_size 0.9968049550435942, test_acc: 0.1645\n",
            "Epoch 140, BestLoss: 0.11459345520345786, Temperature 0.0248942806191894, step_size 0.9967052745480899, test_acc: 0.1984\n",
            "Epoch 141, BestLoss: 0.11459345520345786, Temperature 0.02620450591493621, step_size 0.9966056040206351, test_acc: 0.2511\n",
            "Epoch 142, BestLoss: 0.11459345520345786, Temperature 0.027583690436774957, step_size 0.9965059434602331, test_acc: 0.3442\n",
            "Epoch 143, BestLoss: 0.11459345520345786, Temperature 0.029035463617657853, step_size 0.9964062928658871, test_acc: 0.5014\n",
            "Epoch 144, BestLoss: 0.11459345520345786, Temperature 0.030563645913324056, step_size 0.9963066522366005, test_acc: 0.6586\n",
            "Epoch 145, BestLoss: 0.11459345520345786, Temperature 0.032172258856130585, step_size 0.9962070215713769, test_acc: 0.7593\n",
            "Epoch 146, BestLoss: 0.11459345520345786, Temperature 0.0338655356380322, step_size 0.9961074008692198, test_acc: 0.8127\n",
            "Epoch 147, BestLoss: 0.1933408912293271, Temperature 0.03564793225056021, step_size 0.9960077901291329, test_acc: 0.0761\n",
            "Epoch 148, BestLoss: 0.20593646382300082, Temperature 0.0338655356380322, step_size 0.9959081893501199, test_acc: 0.0723\n",
            "Epoch 149, BestLoss: 0.2098666448298917, Temperature 0.032172258856130585, step_size 0.995808598531185, test_acc: 0.0805\n",
            "Epoch 150, BestLoss: 0.17230762481145068, Temperature 0.030563645913324056, step_size 0.9957090176713319, test_acc: 0.0947\n",
            "Epoch 151, BestLoss: 0.1924467344630763, Temperature 0.029035463617657853, step_size 0.9956094467695648, test_acc: 0.0691\n",
            "Epoch 152, BestLoss: 0.14718342817208102, Temperature 0.027583690436774957, step_size 0.9955098858248879, test_acc: 0.1081\n",
            "Epoch 153, BestLoss: 0.14718342817208102, Temperature 0.02620450591493621, step_size 0.9954103348363054, test_acc: 0.107\n",
            "Epoch 154, BestLoss: 0.1470680600913203, Temperature 0.027583690436774957, step_size 0.9959081893501199, test_acc: 0.0527\n",
            "Epoch 155, BestLoss: 0.1470680600913203, Temperature 0.02620450591493621, step_size 0.995808598531185, test_acc: 0.0561\n",
            "Epoch 156, BestLoss: 0.1470680600913203, Temperature 0.027583690436774957, step_size 0.995808598531185, test_acc: 0.0629\n",
            "Epoch 157, BestLoss: 0.1470680600913203, Temperature 0.029035463617657853, step_size 0.9957090176713319, test_acc: 0.0725\n",
            "Epoch 158, BestLoss: 0.1470680600913203, Temperature 0.030563645913324056, step_size 0.9956094467695648, test_acc: 0.0807\n",
            "Epoch 159, BestLoss: 0.1957427926305536, Temperature 0.032172258856130585, step_size 0.9955098858248879, test_acc: 0.0592\n",
            "Epoch 160, BestLoss: 0.1957427926305536, Temperature 0.030563645913324056, step_size 0.9954103348363054, test_acc: 0.0643\n",
            "Epoch 161, BestLoss: 0.22767168992855388, Temperature 0.032172258856130585, step_size 0.9954103348363054, test_acc: 0.0992\n",
            "Epoch 162, BestLoss: 0.22767168992855388, Temperature 0.030563645913324056, step_size 0.9953107938028217, test_acc: 0.1061\n",
            "Epoch 163, BestLoss: 0.22300437729463957, Temperature 0.032172258856130585, step_size 0.9953107938028217, test_acc: 0.1126\n",
            "Epoch 164, BestLoss: 0.222904414255757, Temperature 0.030563645913324056, step_size 0.9952112627234414, test_acc: 0.1191\n",
            "Epoch 165, BestLoss: 0.16495199658865034, Temperature 0.029035463617657853, step_size 0.9951117415971691, test_acc: 0.1226\n",
            "Epoch 166, BestLoss: 0.12048889633767644, Temperature 0.027583690436774957, step_size 0.9950122304230093, test_acc: 0.1311\n",
            "Epoch 167, BestLoss: 0.12048889633767644, Temperature 0.02620450591493621, step_size 0.9949127291999671, test_acc: 0.136\n",
            "Epoch 168, BestLoss: 0.12048889633767644, Temperature 0.027583690436774957, step_size 0.9952112627234414, test_acc: 0.1443\n",
            "Epoch 169, BestLoss: 0.16377186992528328, Temperature 0.029035463617657853, step_size 0.9951117415971691, test_acc: 0.0653\n",
            "Epoch 170, BestLoss: 0.16377186992528328, Temperature 0.027583690436774957, step_size 0.9950122304230093, test_acc: 0.0659\n",
            "Epoch 171, BestLoss: 0.1554671769713248, Temperature 0.029035463617657853, step_size 0.9950122304230093, test_acc: 0.0965\n",
            "Epoch 172, BestLoss: 0.14823766647389786, Temperature 0.027583690436774957, step_size 0.9949127291999671, test_acc: 0.1008\n",
            "Epoch 173, BestLoss: 0.20654180313409515, Temperature 0.02620450591493621, step_size 0.9948132379270471, test_acc: 0.1414\n",
            "Epoch 174, BestLoss: 0.22210882027025233, Temperature 0.0248942806191894, step_size 0.9947137566032545, test_acc: 0.103\n",
            "Epoch 175, BestLoss: 0.2333551025551044, Temperature 0.023649566588229927, step_size 0.9946142852275941, test_acc: 0.156\n",
            "Epoch 176, BestLoss: 0.2333551025551044, Temperature 0.022467088258818428, step_size 0.9945148237990713, test_acc: 0.1538\n",
            "Epoch 177, BestLoss: 0.25195607285070326, Temperature 0.023649566588229927, step_size 0.9949127291999671, test_acc: 0.0936\n",
            "Epoch 178, BestLoss: 0.2503657021975712, Temperature 0.022467088258818428, step_size 0.9948132379270471, test_acc: 0.1486\n",
            "Epoch 179, BestLoss: 0.24989076029888602, Temperature 0.021343733845877507, step_size 0.9947137566032545, test_acc: 0.1671\n",
            "Epoch 180, BestLoss: 0.2489005078104756, Temperature 0.02027654715358363, step_size 0.9946142852275941, test_acc: 0.1405\n",
            "Epoch 181, BestLoss: 0.22300045651414896, Temperature 0.019262719795904448, step_size 0.9945148237990713, test_acc: 0.1197\n",
            "Epoch 182, BestLoss: 0.19940302713154046, Temperature 0.018299583806109226, step_size 0.9944153723166914, test_acc: 0.1425\n",
            "Epoch 183, BestLoss: 0.1979316797918366, Temperature 0.017384604615803764, step_size 0.9943159307794598, test_acc: 0.1282\n",
            "Epoch 184, BestLoss: 0.1979316797918366, Temperature 0.016515374385013576, step_size 0.9942164991863819, test_acc: 0.1299\n",
            "Epoch 185, BestLoss: 0.20957261439942146, Temperature 0.017384604615803764, step_size 0.9948132379270471, test_acc: 0.113\n",
            "Epoch 186, BestLoss: 0.20957261439942146, Temperature 0.016515374385013576, step_size 0.9947137566032545, test_acc: 0.1167\n",
            "Epoch 187, BestLoss: 0.21472292536012386, Temperature 0.017384604615803764, step_size 0.9947137566032545, test_acc: 0.1019\n",
            "Epoch 188, BestLoss: 0.22955459625500724, Temperature 0.016515374385013576, step_size 0.9946142852275941, test_acc: 0.0703\n",
            "Epoch 189, BestLoss: 0.22955459625500724, Temperature 0.015689605665762895, step_size 0.9945148237990713, test_acc: 0.0735\n",
            "Epoch 190, BestLoss: 0.20788531586210146, Temperature 0.016515374385013576, step_size 0.9946142852275941, test_acc: 0.0768\n",
            "Epoch 191, BestLoss: 0.20788531586210146, Temperature 0.015689605665762895, step_size 0.9945148237990713, test_acc: 0.0826\n",
            "Epoch 192, BestLoss: 0.20788531586210146, Temperature 0.016515374385013576, step_size 0.9945148237990713, test_acc: 0.0897\n",
            "Epoch 193, BestLoss: 0.18284724553410334, Temperature 0.017384604615803764, step_size 0.9944153723166914, test_acc: 0.0899\n",
            "Epoch 194, BestLoss: 0.22335618832349446, Temperature 0.016515374385013576, step_size 0.9943159307794598, test_acc: 0.054\n",
            "Epoch 195, BestLoss: 0.2009771255699669, Temperature 0.015689605665762895, step_size 0.9942164991863819, test_acc: 0.0834\n",
            "Epoch 196, BestLoss: 0.18485351195830538, Temperature 0.01490512538247475, step_size 0.9941170775364633, test_acc: 0.1251\n",
            "Epoch 197, BestLoss: 0.16442671106445578, Temperature 0.014159869113351011, step_size 0.9940176658287097, test_acc: 0.1175\n",
            "Epoch 198, BestLoss: 0.16442671106445578, Temperature 0.01345187565768346, step_size 0.9939182640621268, test_acc: 0.1174\n",
            "Epoch 199, BestLoss: 0.17337814314715846, Temperature 0.014159869113351011, step_size 0.9943159307794598, test_acc: 0.0692\n",
            "Epoch 200, BestLoss: 0.16157880963842133, Temperature 0.01345187565768346, step_size 0.9942164991863819, test_acc: 0.0817\n",
            "Epoch 201, BestLoss: 0.12140229079013581, Temperature 0.012779281874799287, step_size 0.9941170775364633, test_acc: 0.1114\n",
            "Epoch 202, BestLoss: 0.12140229079013581, Temperature 0.012140317781059323, step_size 0.9940176658287097, test_acc: 0.1102\n",
            "Epoch 203, BestLoss: 0.12140229079013581, Temperature 0.012779281874799287, step_size 0.9942164991863819, test_acc: 0.105\n",
            "Epoch 204, BestLoss: 0.12140229079013581, Temperature 0.01345187565768346, step_size 0.9941170775364633, test_acc: 0.1015\n",
            "Epoch 205, BestLoss: 0.12140229079013581, Temperature 0.014159869113351011, step_size 0.9940176658287097, test_acc: 0.0977\n",
            "Epoch 206, BestLoss: 0.12140229079013581, Temperature 0.01490512538247475, step_size 0.9939182640621268, test_acc: 0.098\n",
            "Epoch 207, BestLoss: 0.11702935370352771, Temperature 0.015689605665762895, step_size 0.9938188722357206, test_acc: 0.1296\n",
            "Epoch 208, BestLoss: 0.11702935370352771, Temperature 0.01490512538247475, step_size 0.993719490348497, test_acc: 0.1352\n",
            "Epoch 209, BestLoss: 0.10914670900874777, Temperature 0.015689605665762895, step_size 0.993719490348497, test_acc: 0.0987\n",
            "Epoch 210, BestLoss: 0.135070885231178, Temperature 0.01490512538247475, step_size 0.9936201183994622, test_acc: 0.1001\n",
            "Epoch 211, BestLoss: 0.135070885231178, Temperature 0.014159869113351011, step_size 0.9935207563876223, test_acc: 0.1005\n",
            "Epoch 212, BestLoss: 0.135070885231178, Temperature 0.01490512538247475, step_size 0.9936201183994622, test_acc: 0.1\n",
            "Epoch 213, BestLoss: 0.135070885231178, Temperature 0.015689605665762895, step_size 0.9935207563876223, test_acc: 0.1018\n",
            "Epoch 214, BestLoss: 0.135070885231178, Temperature 0.016515374385013576, step_size 0.9934214043119836, test_acc: 0.1025\n",
            "Epoch 215, BestLoss: 0.135070885231178, Temperature 0.017384604615803764, step_size 0.9933220621715524, test_acc: 0.1039\n",
            "Epoch 216, BestLoss: 0.135070885231178, Temperature 0.018299583806109226, step_size 0.9932227299653352, test_acc: 0.1069\n",
            "Epoch 217, BestLoss: 0.135070885231178, Temperature 0.019262719795904448, step_size 0.9931234076923388, test_acc: 0.1136\n",
            "Epoch 218, BestLoss: 0.135070885231178, Temperature 0.02027654715358363, step_size 0.9930240953515695, test_acc: 0.1247\n",
            "Epoch 219, BestLoss: 0.135070885231178, Temperature 0.021343733845877507, step_size 0.9929247929420344, test_acc: 0.1521\n",
            "Epoch 220, BestLoss: 0.135070885231178, Temperature 0.022467088258818428, step_size 0.9928255004627402, test_acc: 0.2151\n",
            "Epoch 221, BestLoss: 0.135070885231178, Temperature 0.023649566588229927, step_size 0.992726217912694, test_acc: 0.3511\n",
            "Epoch 222, BestLoss: 0.135070885231178, Temperature 0.0248942806191894, step_size 0.9926269452909028, test_acc: 0.5507\n",
            "Epoch 223, BestLoss: 0.135070885231178, Temperature 0.02620450591493621, step_size 0.9925276825963737, test_acc: 0.7295\n",
            "Epoch 224, BestLoss: 0.135070885231178, Temperature 0.027583690436774957, step_size 0.9924284298281141, test_acc: 0.8114\n",
            "Epoch 225, BestLoss: 0.135070885231178, Temperature 0.029035463617657853, step_size 0.9923291869851313, test_acc: 0.8424\n",
            "Epoch 226, BestLoss: 0.135070885231178, Temperature 0.030563645913324056, step_size 0.9922299540664328, test_acc: 0.8527\n",
            "Epoch 227, BestLoss: 0.135070885231178, Temperature 0.032172258856130585, step_size 0.9921307310710261, test_acc: 0.8572\n",
            "Epoch 228, BestLoss: 0.23177318613995143, Temperature 0.0338655356380322, step_size 0.992031517997919, test_acc: 0.107\n",
            "Epoch 229, BestLoss: 0.21741080542128585, Temperature 0.032172258856130585, step_size 0.9919323148461192, test_acc: 0.0998\n",
            "Epoch 230, BestLoss: 0.14380468138272806, Temperature 0.030563645913324056, step_size 0.9918331216146347, test_acc: 0.0741\n",
            "Epoch 231, BestLoss: 0.14380468138272806, Temperature 0.029035463617657853, step_size 0.9917339383024733, test_acc: 0.0796\n",
            "Epoch 232, BestLoss: 0.14380468138272806, Temperature 0.030563645913324056, step_size 0.9919323148461192, test_acc: 0.0926\n",
            "Epoch 233, BestLoss: 0.18519535621498087, Temperature 0.032172258856130585, step_size 0.9918331216146347, test_acc: 0.0908\n",
            "Epoch 234, BestLoss: 0.18519535621498087, Temperature 0.030563645913324056, step_size 0.9917339383024733, test_acc: 0.0943\n",
            "Epoch 235, BestLoss: 0.19836288855406625, Temperature 0.032172258856130585, step_size 0.9917339383024733, test_acc: 0.0887\n",
            "Epoch 236, BestLoss: 0.16710843576662027, Temperature 0.030563645913324056, step_size 0.991634764908643, test_acc: 0.1446\n",
            "Epoch 237, BestLoss: 0.17269954635479018, Temperature 0.029035463617657853, step_size 0.9915356014321521, test_acc: 0.0697\n",
            "Epoch 238, BestLoss: 0.17269954635479018, Temperature 0.027583690436774957, step_size 0.9914364478720089, test_acc: 0.0717\n",
            "Epoch 239, BestLoss: 0.15965198347157913, Temperature 0.029035463617657853, step_size 0.991634764908643, test_acc: 0.0631\n",
            "Epoch 240, BestLoss: 0.17139372085843654, Temperature 0.027583690436774957, step_size 0.9915356014321521, test_acc: 0.1305\n",
            "Epoch 241, BestLoss: 0.17139372085843654, Temperature 0.02620450591493621, step_size 0.9914364478720089, test_acc: 0.1294\n",
            "Epoch 242, BestLoss: 0.17139372085843654, Temperature 0.027583690436774957, step_size 0.9915356014321521, test_acc: 0.1235\n",
            "Epoch 243, BestLoss: 0.17139372085843654, Temperature 0.029035463617657853, step_size 0.9914364478720089, test_acc: 0.1164\n",
            "Epoch 244, BestLoss: 0.17676283220056468, Temperature 0.030563645913324056, step_size 0.9913373042272218, test_acc: 0.0949\n",
            "Epoch 245, BestLoss: 0.1594962188955103, Temperature 0.029035463617657853, step_size 0.9912381704967991, test_acc: 0.0913\n",
            "Epoch 246, BestLoss: 0.1594962188955103, Temperature 0.027583690436774957, step_size 0.9911390466797494, test_acc: 0.0885\n",
            "Epoch 247, BestLoss: 0.19270713554501212, Temperature 0.029035463617657853, step_size 0.9912381704967991, test_acc: 0.1157\n",
            "Epoch 248, BestLoss: 0.1971233160886838, Temperature 0.027583690436774957, step_size 0.9911390466797494, test_acc: 0.1165\n",
            "Epoch 249, BestLoss: 0.1971233160886838, Temperature 0.02620450591493621, step_size 0.9910399327750814, test_acc: 0.1186\n",
            "Epoch 250, BestLoss: 0.1971233160886838, Temperature 0.027583690436774957, step_size 0.9911390466797494, test_acc: 0.1165\n",
            "Epoch 251, BestLoss: 0.18409663539675444, Temperature 0.029035463617657853, step_size 0.9910399327750814, test_acc: 0.1123\n",
            "Epoch 252, BestLoss: 0.17962449442220196, Temperature 0.027583690436774957, step_size 0.9909408287818039, test_acc: 0.1078\n",
            "Epoch 253, BestLoss: 0.1922438801614466, Temperature 0.02620450591493621, step_size 0.9908417346989258, test_acc: 0.1075\n",
            "Epoch 254, BestLoss: 0.2006361935822493, Temperature 0.0248942806191894, step_size 0.9907426505254558, test_acc: 0.1104\n",
            "Epoch 255, BestLoss: 0.20204453512270915, Temperature 0.023649566588229927, step_size 0.9906435762604033, test_acc: 0.1027\n",
            "Epoch 256, BestLoss: 0.20204453512270915, Temperature 0.022467088258818428, step_size 0.9905445119027773, test_acc: 0.1033\n",
            "Epoch 257, BestLoss: 0.15929946684513377, Temperature 0.023649566588229927, step_size 0.9909408287818039, test_acc: 0.1024\n",
            "Epoch 258, BestLoss: 0.16499510985820434, Temperature 0.022467088258818428, step_size 0.9908417346989258, test_acc: 0.1047\n",
            "Epoch 259, BestLoss: 0.16503608343073867, Temperature 0.021343733845877507, step_size 0.9907426505254558, test_acc: 0.0922\n",
            "Epoch 260, BestLoss: 0.16503608343073867, Temperature 0.02027654715358363, step_size 0.9906435762604033, test_acc: 0.102\n",
            "Epoch 261, BestLoss: 0.15409431188176834, Temperature 0.021343733845877507, step_size 0.9908417346989258, test_acc: 0.1533\n",
            "Epoch 262, BestLoss: 0.15409431188176834, Temperature 0.02027654715358363, step_size 0.9907426505254558, test_acc: 0.154\n",
            "Epoch 263, BestLoss: 0.12290789575958407, Temperature 0.021343733845877507, step_size 0.9907426505254558, test_acc: 0.0614\n",
            "Epoch 264, BestLoss: 0.12290789575958407, Temperature 0.02027654715358363, step_size 0.9906435762604033, test_acc: 0.0638\n",
            "Epoch 265, BestLoss: 0.1281261501632891, Temperature 0.021343733845877507, step_size 0.9906435762604033, test_acc: 0.0797\n",
            "Epoch 266, BestLoss: 0.15204978573369154, Temperature 0.02027654715358363, step_size 0.9905445119027773, test_acc: 0.0954\n",
            "Epoch 267, BestLoss: 0.15627078234742328, Temperature 0.019262719795904448, step_size 0.990445457451587, test_acc: 0.0867\n",
            "Epoch 268, BestLoss: 0.15627078234742328, Temperature 0.018299583806109226, step_size 0.9903464129058418, test_acc: 0.0871\n",
            "Epoch 269, BestLoss: 0.15627078234742328, Temperature 0.019262719795904448, step_size 0.9905445119027773, test_acc: 0.0894\n",
            "Epoch 270, BestLoss: 0.15627078234742328, Temperature 0.02027654715358363, step_size 0.990445457451587, test_acc: 0.0945\n",
            "Epoch 271, BestLoss: 0.15584160382989604, Temperature 0.021343733845877507, step_size 0.9903464129058418, test_acc: 0.0581\n",
            "Epoch 272, BestLoss: 0.17867778031263698, Temperature 0.02027654715358363, step_size 0.9902473782645512, test_acc: 0.0994\n",
            "Epoch 273, BestLoss: 0.17867778031263698, Temperature 0.019262719795904448, step_size 0.9901483535267248, test_acc: 0.0995\n",
            "Epoch 274, BestLoss: 0.17867778031263698, Temperature 0.02027654715358363, step_size 0.9902473782645512, test_acc: 0.0988\n",
            "Epoch 275, BestLoss: 0.13122335775750574, Temperature 0.021343733845877507, step_size 0.9901483535267248, test_acc: 0.088\n",
            "Epoch 276, BestLoss: 0.12939951853952836, Temperature 0.02027654715358363, step_size 0.9900493386913721, test_acc: 0.1\n",
            "Epoch 277, BestLoss: 0.12939951853952836, Temperature 0.019262719795904448, step_size 0.989950333757503, test_acc: 0.1012\n",
            "Epoch 278, BestLoss: 0.12939951853952836, Temperature 0.02027654715358363, step_size 0.9900493386913721, test_acc: 0.1003\n",
            "Epoch 279, BestLoss: 0.12939951853952836, Temperature 0.021343733845877507, step_size 0.989950333757503, test_acc: 0.1018\n",
            "Epoch 280, BestLoss: 0.12939951853952836, Temperature 0.022467088258818428, step_size 0.9898513387241272, test_acc: 0.0968\n",
            "Epoch 281, BestLoss: 0.12939951853952836, Temperature 0.023649566588229927, step_size 0.9897523535902548, test_acc: 0.1028\n",
            "Epoch 282, BestLoss: 0.12939951853952836, Temperature 0.0248942806191894, step_size 0.9896533783548958, test_acc: 0.1105\n",
            "Epoch 283, BestLoss: 0.12939951853952836, Temperature 0.02620450591493621, step_size 0.9895544130170603, test_acc: 0.1231\n",
            "Epoch 284, BestLoss: 0.12939951853952836, Temperature 0.027583690436774957, step_size 0.9894554575757586, test_acc: 0.151\n",
            "Epoch 285, BestLoss: 0.12939951853952836, Temperature 0.029035463617657853, step_size 0.9893565120300011, test_acc: 0.2077\n",
            "Epoch 286, BestLoss: 0.16255927713833493, Temperature 0.030563645913324056, step_size 0.9892575763787981, test_acc: 0.0945\n",
            "Epoch 287, BestLoss: 0.11341331811058876, Temperature 0.029035463617657853, step_size 0.9891586506211602, test_acc: 0.1066\n",
            "Epoch 288, BestLoss: 0.11341331811058876, Temperature 0.027583690436774957, step_size 0.9890597347560981, test_acc: 0.1083\n",
            "Epoch 289, BestLoss: 0.09273616218349345, Temperature 0.029035463617657853, step_size 0.9891586506211602, test_acc: 0.0574\n",
            "Epoch 290, BestLoss: 0.09273616218349345, Temperature 0.027583690436774957, step_size 0.9890597347560981, test_acc: 0.0666\n",
            "Epoch 291, BestLoss: 0.09273616218349345, Temperature 0.029035463617657853, step_size 0.9890597347560981, test_acc: 0.0832\n",
            "Epoch 292, BestLoss: 0.09273616218349345, Temperature 0.030563645913324056, step_size 0.9889608287826225, test_acc: 0.1116\n",
            "Epoch 293, BestLoss: 0.15430826120544305, Temperature 0.032172258856130585, step_size 0.9888619326997442, test_acc: 0.1205\n",
            "Epoch 294, BestLoss: 0.14451225321117056, Temperature 0.030563645913324056, step_size 0.9887630465064743, test_acc: 0.1495\n",
            "Epoch 295, BestLoss: 0.14451225321117056, Temperature 0.029035463617657853, step_size 0.9886641702018236, test_acc: 0.1449\n",
            "Epoch 296, BestLoss: 0.14451225321117056, Temperature 0.030563645913324056, step_size 0.9887630465064743, test_acc: 0.1335\n",
            "Epoch 297, BestLoss: 0.1410683190985572, Temperature 0.032172258856130585, step_size 0.9886641702018236, test_acc: 0.1065\n",
            "Epoch 298, BestLoss: 0.12696870783630923, Temperature 0.030563645913324056, step_size 0.9885653037848035, test_acc: 0.1272\n",
            "Epoch 299, BestLoss: 0.12696870783630923, Temperature 0.029035463617657853, step_size 0.988466447254425, test_acc: 0.1307\n",
            "Epoch 300, BestLoss: 0.12696870783630923, Temperature 0.030563645913324056, step_size 0.9885653037848035, test_acc: 0.1368\n",
            "Epoch 301, BestLoss: 0.12696870783630923, Temperature 0.032172258856130585, step_size 0.988466447254425, test_acc: 0.1446\n",
            "Epoch 302, BestLoss: 0.12696870783630923, Temperature 0.0338655356380322, step_size 0.9883676006096995, test_acc: 0.1593\n",
            "Epoch 303, BestLoss: 0.12696870783630923, Temperature 0.03564793225056021, step_size 0.9882687638496386, test_acc: 0.1691\n",
            "Epoch 304, BestLoss: 0.12696870783630923, Temperature 0.03752413921111601, step_size 0.9881699369732536, test_acc: 0.1821\n",
            "Epoch 305, BestLoss: 0.14434809803187615, Temperature 0.03949909390643791, step_size 0.9880711199795563, test_acc: 0.0835\n",
            "Epoch 306, BestLoss: 0.14434809803187615, Temperature 0.03752413921111601, step_size 0.9879723128675584, test_acc: 0.0749\n",
            "Epoch 307, BestLoss: 0.14434809803187615, Temperature 0.03949909390643791, step_size 0.9879723128675584, test_acc: 0.0717\n",
            "Epoch 308, BestLoss: 0.14434809803187615, Temperature 0.041577993585724116, step_size 0.9878735156362717, test_acc: 0.0815\n",
            "Epoch 309, BestLoss: 0.16248952359454644, Temperature 0.04376630903760433, step_size 0.987774728284708, test_acc: 0.1058\n",
            "Epoch 310, BestLoss: 0.18091955843755816, Temperature 0.041577993585724116, step_size 0.9876759508118795, test_acc: 0.178\n",
            "Epoch 311, BestLoss: 0.19730564368391784, Temperature 0.03949909390643791, step_size 0.9875771832167983, test_acc: 0.1403\n",
            "Epoch 312, BestLoss: 0.17472152915684852, Temperature 0.03752413921111601, step_size 0.9874784254984766, test_acc: 0.0665\n",
            "Epoch 313, BestLoss: 0.20255996707462726, Temperature 0.03564793225056021, step_size 0.9873796776559268, test_acc: 0.109\n",
            "Epoch 314, BestLoss: 0.2314806584149194, Temperature 0.0338655356380322, step_size 0.9872809396881612, test_acc: 0.1048\n",
            "Epoch 315, BestLoss: 0.20920411229007432, Temperature 0.032172258856130585, step_size 0.9871822115941924, test_acc: 0.1092\n",
            "Epoch 316, BestLoss: 0.24082016879609894, Temperature 0.030563645913324056, step_size 0.987083493373033, test_acc: 0.1108\n",
            "Epoch 317, BestLoss: 0.208937479321929, Temperature 0.029035463617657853, step_size 0.9869847850236957, test_acc: 0.1001\n",
            "Epoch 318, BestLoss: 0.16203059169951953, Temperature 0.027583690436774957, step_size 0.9868860865451933, test_acc: 0.0826\n",
            "Epoch 319, BestLoss: 0.16203059169951953, Temperature 0.02620450591493621, step_size 0.9867873979365388, test_acc: 0.0827\n",
            "Epoch 320, BestLoss: 0.16203059169951953, Temperature 0.027583690436774957, step_size 0.9876759508118795, test_acc: 0.0855\n",
            "Epoch 321, BestLoss: 0.14169435586003132, Temperature 0.029035463617657853, step_size 0.9875771832167983, test_acc: 0.0821\n",
            "Epoch 322, BestLoss: 0.14169435586003132, Temperature 0.027583690436774957, step_size 0.9874784254984766, test_acc: 0.0939\n",
            "Epoch 323, BestLoss: 0.1441109351474167, Temperature 0.029035463617657853, step_size 0.9874784254984766, test_acc: 0.1678\n",
            "Epoch 324, BestLoss: 0.1441109351474167, Temperature 0.027583690436774957, step_size 0.9873796776559268, test_acc: 0.1753\n",
            "Epoch 325, BestLoss: 0.13951906925685464, Temperature 0.029035463617657853, step_size 0.9873796776559268, test_acc: 0.135\n",
            "Epoch 326, BestLoss: 0.19057985275457617, Temperature 0.027583690436774957, step_size 0.9872809396881612, test_acc: 0.0934\n",
            "Epoch 327, BestLoss: 0.15668800732427332, Temperature 0.02620450591493621, step_size 0.9871822115941924, test_acc: 0.0639\n",
            "Epoch 328, BestLoss: 0.15668800732427332, Temperature 0.0248942806191894, step_size 0.987083493373033, test_acc: 0.0657\n",
            "Epoch 329, BestLoss: 0.19201982267239867, Temperature 0.02620450591493621, step_size 0.9872809396881612, test_acc: 0.0659\n",
            "Epoch 330, BestLoss: 0.18642298315534175, Temperature 0.0248942806191894, step_size 0.9871822115941924, test_acc: 0.0791\n",
            "Epoch 331, BestLoss: 0.1396385146574083, Temperature 0.023649566588229927, step_size 0.987083493373033, test_acc: 0.1198\n",
            "Epoch 332, BestLoss: 0.1396385146574083, Temperature 0.022467088258818428, step_size 0.9869847850236957, test_acc: 0.1219\n",
            "Epoch 333, BestLoss: 0.1396385146574083, Temperature 0.023649566588229927, step_size 0.9871822115941924, test_acc: 0.1264\n",
            "Epoch 334, BestLoss: 0.17659694475202609, Temperature 0.0248942806191894, step_size 0.987083493373033, test_acc: 0.0571\n",
            "Epoch 335, BestLoss: 0.16556192589355634, Temperature 0.023649566588229927, step_size 0.9869847850236957, test_acc: 0.0704\n",
            "Epoch 336, BestLoss: 0.18156262726363415, Temperature 0.022467088258818428, step_size 0.9868860865451933, test_acc: 0.0697\n",
            "Epoch 337, BestLoss: 0.17044306716172727, Temperature 0.021343733845877507, step_size 0.9867873979365388, test_acc: 0.1068\n",
            "Epoch 338, BestLoss: 0.17044306716172727, Temperature 0.02027654715358363, step_size 0.9866887191967452, test_acc: 0.1111\n",
            "Epoch 339, BestLoss: 0.17044306716172727, Temperature 0.021343733845877507, step_size 0.9869847850236957, test_acc: 0.1136\n",
            "Epoch 340, BestLoss: 0.17044306716172727, Temperature 0.022467088258818428, step_size 0.9868860865451933, test_acc: 0.1192\n",
            "Epoch 341, BestLoss: 0.17044306716172727, Temperature 0.023649566588229927, step_size 0.9867873979365388, test_acc: 0.1218\n",
            "Epoch 342, BestLoss: 0.16118795584697374, Temperature 0.0248942806191894, step_size 0.9866887191967452, test_acc: 0.1013\n",
            "Epoch 343, BestLoss: 0.16118795584697374, Temperature 0.023649566588229927, step_size 0.9865900503248255, test_acc: 0.1011\n",
            "Epoch 344, BestLoss: 0.16118795584697374, Temperature 0.0248942806191894, step_size 0.9865900503248255, test_acc: 0.1003\n",
            "Epoch 345, BestLoss: 0.16118795584697374, Temperature 0.02620450591493621, step_size 0.986491391319793, test_acc: 0.0962\n",
            "Epoch 346, BestLoss: 0.16118795584697374, Temperature 0.027583690436774957, step_size 0.9863927421806611, test_acc: 0.0882\n",
            "Epoch 347, BestLoss: 0.16118795584697374, Temperature 0.029035463617657853, step_size 0.986294102906443, test_acc: 0.0968\n",
            "Epoch 348, BestLoss: 0.16118795584697374, Temperature 0.030563645913324056, step_size 0.9861954734961524, test_acc: 0.1064\n",
            "Epoch 349, BestLoss: 0.16118795584697374, Temperature 0.032172258856130585, step_size 0.9860968539488029, test_acc: 0.1219\n",
            "Epoch 350, BestLoss: 0.21662919369656686, Temperature 0.0338655356380322, step_size 0.985998244263408, test_acc: 0.0916\n",
            "Epoch 351, BestLoss: 0.22734066512839257, Temperature 0.032172258856130585, step_size 0.9858996444389817, test_acc: 0.1095\n",
            "Epoch 352, BestLoss: 0.22734066512839257, Temperature 0.030563645913324056, step_size 0.9858010544745378, test_acc: 0.1107\n",
            "Epoch 353, BestLoss: 0.21307424963689667, Temperature 0.032172258856130585, step_size 0.9858996444389817, test_acc: 0.1068\n",
            "Epoch 354, BestLoss: 0.21458597971777102, Temperature 0.030563645913324056, step_size 0.9858010544745378, test_acc: 0.0644\n",
            "Epoch 355, BestLoss: 0.20377231148800948, Temperature 0.029035463617657853, step_size 0.9857024743690904, test_acc: 0.0338\n",
            "Epoch 356, BestLoss: 0.1860398431032783, Temperature 0.027583690436774957, step_size 0.9856039041216536, test_acc: 0.0718\n",
            "Epoch 357, BestLoss: 0.195811770092358, Temperature 0.02620450591493621, step_size 0.9855053437312414, test_acc: 0.0631\n",
            "Epoch 358, BestLoss: 0.195811770092358, Temperature 0.0248942806191894, step_size 0.9854067931968683, test_acc: 0.0692\n",
            "Epoch 359, BestLoss: 0.16923271254840405, Temperature 0.02620450591493621, step_size 0.9858010544745378, test_acc: 0.0769\n",
            "Epoch 360, BestLoss: 0.16923271254840405, Temperature 0.0248942806191894, step_size 0.9857024743690904, test_acc: 0.0825\n",
            "Epoch 361, BestLoss: 0.1620985453271126, Temperature 0.02620450591493621, step_size 0.9857024743690904, test_acc: 0.1201\n",
            "Epoch 362, BestLoss: 0.17721887843256126, Temperature 0.0248942806191894, step_size 0.9856039041216536, test_acc: 0.1168\n",
            "Epoch 363, BestLoss: 0.17721887843256126, Temperature 0.023649566588229927, step_size 0.9855053437312414, test_acc: 0.1126\n",
            "Epoch 364, BestLoss: 0.17721887843256126, Temperature 0.0248942806191894, step_size 0.9856039041216536, test_acc: 0.1072\n",
            "Epoch 365, BestLoss: 0.17721887843256126, Temperature 0.02620450591493621, step_size 0.9855053437312414, test_acc: 0.1027\n",
            "Epoch 366, BestLoss: 0.15226545255100946, Temperature 0.027583690436774957, step_size 0.9854067931968683, test_acc: 0.2111\n",
            "Epoch 367, BestLoss: 0.17319288933908092, Temperature 0.02620450591493621, step_size 0.9853082525175486, test_acc: 0.1684\n",
            "Epoch 368, BestLoss: 0.17672880765452031, Temperature 0.0248942806191894, step_size 0.9852097216922968, test_acc: 0.1677\n",
            "Epoch 369, BestLoss: 0.17672880765452031, Temperature 0.023649566588229927, step_size 0.9851112007201276, test_acc: 0.1553\n",
            "Epoch 370, BestLoss: 0.15633872482176348, Temperature 0.0248942806191894, step_size 0.9853082525175486, test_acc: 0.131\n",
            "Epoch 371, BestLoss: 0.15633872482176348, Temperature 0.023649566588229927, step_size 0.9852097216922968, test_acc: 0.1308\n",
            "Epoch 372, BestLoss: 0.18836901617816437, Temperature 0.0248942806191894, step_size 0.9852097216922968, test_acc: 0.1156\n",
            "Epoch 373, BestLoss: 0.1566150958594287, Temperature 0.023649566588229927, step_size 0.9851112007201276, test_acc: 0.15\n",
            "Epoch 374, BestLoss: 0.1566150958594287, Temperature 0.022467088258818428, step_size 0.9850126896000556, test_acc: 0.1482\n",
            "Epoch 375, BestLoss: 0.1593804843880097, Temperature 0.023649566588229927, step_size 0.9851112007201276, test_acc: 0.2141\n",
            "Epoch 376, BestLoss: 0.18278869683634372, Temperature 0.022467088258818428, step_size 0.9850126896000556, test_acc: 0.1618\n",
            "Epoch 377, BestLoss: 0.22320191723604488, Temperature 0.021343733845877507, step_size 0.9849141883310956, test_acc: 0.17\n",
            "Epoch 378, BestLoss: 0.20624576484766324, Temperature 0.02027654715358363, step_size 0.9848156969122625, test_acc: 0.1577\n",
            "Epoch 379, BestLoss: 0.20624576484766324, Temperature 0.019262719795904448, step_size 0.9847172153425713, test_acc: 0.1538\n",
            "Epoch 380, BestLoss: 0.18832792905962437, Temperature 0.02027654715358363, step_size 0.9850126896000556, test_acc: 0.1188\n",
            "Epoch 381, BestLoss: 0.20513011611824414, Temperature 0.019262719795904448, step_size 0.9849141883310956, test_acc: 0.0497\n",
            "Epoch 382, BestLoss: 0.19316011433964517, Temperature 0.018299583806109226, step_size 0.9848156969122625, test_acc: 0.1128\n",
            "Epoch 383, BestLoss: 0.14376877507655403, Temperature 0.017384604615803764, step_size 0.9847172153425713, test_acc: 0.1412\n",
            "Epoch 384, BestLoss: 0.14376877507655403, Temperature 0.016515374385013576, step_size 0.9846187436210371, test_acc: 0.1418\n",
            "Epoch 385, BestLoss: 0.14942251637347928, Temperature 0.017384604615803764, step_size 0.9849141883310956, test_acc: 0.1151\n",
            "Epoch 386, BestLoss: 0.14942251637347928, Temperature 0.016515374385013576, step_size 0.9848156969122625, test_acc: 0.1104\n",
            "Epoch 387, BestLoss: 0.1366258760435128, Temperature 0.017384604615803764, step_size 0.9848156969122625, test_acc: 0.0509\n",
            "Epoch 388, BestLoss: 0.1366258760435128, Temperature 0.016515374385013576, step_size 0.9847172153425713, test_acc: 0.0438\n",
            "Epoch 389, BestLoss: 0.1366258760435128, Temperature 0.017384604615803764, step_size 0.9847172153425713, test_acc: 0.0415\n",
            "Epoch 390, BestLoss: 0.1366258760435128, Temperature 0.018299583806109226, step_size 0.9846187436210371, test_acc: 0.0478\n",
            "Epoch 391, BestLoss: 0.1366258760435128, Temperature 0.019262719795904448, step_size 0.9845202817466749, test_acc: 0.0632\n",
            "Epoch 392, BestLoss: 0.1366258760435128, Temperature 0.02027654715358363, step_size 0.9844218297185002, test_acc: 0.0733\n",
            "Epoch 393, BestLoss: 0.14599306240766327, Temperature 0.021343733845877507, step_size 0.9843233875355284, test_acc: 0.097\n",
            "Epoch 394, BestLoss: 0.14599306240766327, Temperature 0.02027654715358363, step_size 0.9842249551967749, test_acc: 0.0982\n",
            "Epoch 395, BestLoss: 0.14599306240766327, Temperature 0.021343733845877507, step_size 0.9842249551967749, test_acc: 0.0933\n",
            "Epoch 396, BestLoss: 0.09907565181155953, Temperature 0.022467088258818428, step_size 0.9841265327012553, test_acc: 0.1585\n",
            "Epoch 397, BestLoss: 0.09907565181155953, Temperature 0.021343733845877507, step_size 0.9840281200479851, test_acc: 0.164\n",
            "Epoch 398, BestLoss: 0.09907565181155953, Temperature 0.022467088258818428, step_size 0.9840281200479851, test_acc: 0.1665\n",
            "Epoch 399, BestLoss: 0.09907565181155953, Temperature 0.023649566588229927, step_size 0.9839297172359803, test_acc: 0.1636\n",
            "Epoch 400, BestLoss: 0.09907565181155953, Temperature 0.0248942806191894, step_size 0.9838313242642568, test_acc: 0.1494\n",
            "Epoch 401, BestLoss: 0.09907565181155953, Temperature 0.02620450591493621, step_size 0.9837329411318304, test_acc: 0.1472\n",
            "Epoch 402, BestLoss: 0.1303064194701967, Temperature 0.027583690436774957, step_size 0.9836345678377172, test_acc: 0.0676\n",
            "Epoch 403, BestLoss: 0.1303064194701967, Temperature 0.02620450591493621, step_size 0.9835362043809335, test_acc: 0.0699\n",
            "Epoch 404, BestLoss: 0.1303064194701967, Temperature 0.027583690436774957, step_size 0.9835362043809335, test_acc: 0.0763\n",
            "Epoch 405, BestLoss: 0.1303064194701967, Temperature 0.029035463617657853, step_size 0.9834378507604954, test_acc: 0.0854\n",
            "Epoch 406, BestLoss: 0.1303064194701967, Temperature 0.030563645913324056, step_size 0.9833395069754194, test_acc: 0.0902\n",
            "Epoch 407, BestLoss: 0.1303064194701967, Temperature 0.032172258856130585, step_size 0.9832411730247218, test_acc: 0.0985\n",
            "Epoch 408, BestLoss: 0.11883132385020312, Temperature 0.0338655356380322, step_size 0.9831428489074193, test_acc: 0.0527\n",
            "Epoch 409, BestLoss: 0.11883132385020312, Temperature 0.032172258856130585, step_size 0.9830445346225286, test_acc: 0.0579\n",
            "Epoch 410, BestLoss: 0.11883132385020312, Temperature 0.0338655356380322, step_size 0.9830445346225286, test_acc: 0.0692\n",
            "Epoch 411, BestLoss: 0.11883132385020312, Temperature 0.03564793225056021, step_size 0.9829462301690663, test_acc: 0.0846\n",
            "Epoch 412, BestLoss: 0.1409830477158993, Temperature 0.03752413921111601, step_size 0.9828479355460494, test_acc: 0.1267\n",
            "Epoch 413, BestLoss: 0.11149420816743376, Temperature 0.03564793225056021, step_size 0.9827496507524949, test_acc: 0.1057\n",
            "Epoch 414, BestLoss: 0.08997588506656864, Temperature 0.0338655356380322, step_size 0.9826513757874197, test_acc: 0.0783\n",
            "Epoch 415, BestLoss: 0.08997588506656864, Temperature 0.032172258856130585, step_size 0.982553110649841, test_acc: 0.0774\n",
            "Epoch 416, BestLoss: 0.08997588506656864, Temperature 0.0338655356380322, step_size 0.9827496507524949, test_acc: 0.0791\n",
            "Epoch 417, BestLoss: 0.08997588506656864, Temperature 0.03564793225056021, step_size 0.9826513757874197, test_acc: 0.0786\n",
            "Epoch 418, BestLoss: 0.08997588506656864, Temperature 0.03752413921111601, step_size 0.982553110649841, test_acc: 0.0764\n",
            "Epoch 419, BestLoss: 0.08997588506656864, Temperature 0.03949909390643791, step_size 0.982454855338776, test_acc: 0.0851\n",
            "Epoch 420, BestLoss: 0.1439383588420603, Temperature 0.041577993585724116, step_size 0.9823566098532421, test_acc: 0.1243\n",
            "Epoch 421, BestLoss: 0.1439383588420603, Temperature 0.03949909390643791, step_size 0.9822583741922568, test_acc: 0.1191\n",
            "Epoch 422, BestLoss: 0.1439383588420603, Temperature 0.041577993585724116, step_size 0.9822583741922568, test_acc: 0.1092\n",
            "Epoch 423, BestLoss: 0.1439383588420603, Temperature 0.04376630903760433, step_size 0.9821601483548376, test_acc: 0.1052\n",
            "Epoch 424, BestLoss: 0.1439383588420603, Temperature 0.04606979898695193, step_size 0.982061932340002, test_acc: 0.1155\n",
            "Epoch 425, BestLoss: 0.17515600723351163, Temperature 0.04849452524942309, step_size 0.9819637261467681, test_acc: 0.1386\n",
            "Epoch 426, BestLoss: 0.18940229667726644, Temperature 0.04606979898695193, step_size 0.9818655297741534, test_acc: 0.1406\n",
            "Epoch 427, BestLoss: 0.17865553843756718, Temperature 0.04376630903760433, step_size 0.981767343221176, test_acc: 0.0997\n",
            "Epoch 428, BestLoss: 0.1576276668190574, Temperature 0.041577993585724116, step_size 0.9816691664868539, test_acc: 0.0779\n",
            "Epoch 429, BestLoss: 0.1576276668190574, Temperature 0.03949909390643791, step_size 0.9815709995702052, test_acc: 0.0905\n",
            "Epoch 430, BestLoss: 0.1058122247789114, Temperature 0.041577993585724116, step_size 0.9818655297741534, test_acc: 0.0756\n",
            "Epoch 431, BestLoss: 0.1142877322882637, Temperature 0.03949909390643791, step_size 0.981767343221176, test_acc: 0.0659\n",
            "Epoch 432, BestLoss: 0.1300509303772227, Temperature 0.03752413921111601, step_size 0.9816691664868539, test_acc: 0.0673\n",
            "Epoch 433, BestLoss: 0.1300509303772227, Temperature 0.03564793225056021, step_size 0.9815709995702052, test_acc: 0.0706\n",
            "Epoch 434, BestLoss: 0.11417929675172507, Temperature 0.03752413921111601, step_size 0.981767343221176, test_acc: 0.0944\n",
            "Epoch 435, BestLoss: 0.13920668187565627, Temperature 0.03564793225056021, step_size 0.9816691664868539, test_acc: 0.0783\n",
            "Epoch 436, BestLoss: 0.16137054189631206, Temperature 0.0338655356380322, step_size 0.9815709995702052, test_acc: 0.1422\n",
            "Epoch 437, BestLoss: 0.16137054189631206, Temperature 0.032172258856130585, step_size 0.9814728424702482, test_acc: 0.1424\n",
            "Epoch 438, BestLoss: 0.18603671878784064, Temperature 0.0338655356380322, step_size 0.9816691664868539, test_acc: 0.1213\n",
            "Epoch 439, BestLoss: 0.18603671878784064, Temperature 0.032172258856130585, step_size 0.9815709995702052, test_acc: 0.1253\n",
            "Epoch 440, BestLoss: 0.2152629242633987, Temperature 0.0338655356380322, step_size 0.9815709995702052, test_acc: 0.1177\n",
            "Epoch 441, BestLoss: 0.2152629242633987, Temperature 0.032172258856130585, step_size 0.9814728424702482, test_acc: 0.1192\n",
            "Epoch 442, BestLoss: 0.2510865874707474, Temperature 0.0338655356380322, step_size 0.9814728424702482, test_acc: 0.1191\n",
            "Epoch 443, BestLoss: 0.23693209936738335, Temperature 0.032172258856130585, step_size 0.9813746951860012, test_acc: 0.0335\n",
            "Epoch 444, BestLoss: 0.24097326736016733, Temperature 0.030563645913324056, step_size 0.9812765577164826, test_acc: 0.0928\n",
            "Epoch 445, BestLoss: 0.23948955191123997, Temperature 0.029035463617657853, step_size 0.9811784300607109, test_acc: 0.0511\n",
            "Epoch 446, BestLoss: 0.2227263967689858, Temperature 0.027583690436774957, step_size 0.9810803122177049, test_acc: 0.0626\n",
            "Epoch 447, BestLoss: 0.21026772400526775, Temperature 0.02620450591493621, step_size 0.9809822041864831, test_acc: 0.0697\n",
            "Epoch 448, BestLoss: 0.1977891888065666, Temperature 0.0248942806191894, step_size 0.9808841059660645, test_acc: 0.1139\n",
            "Epoch 449, BestLoss: 0.22320918427688216, Temperature 0.023649566588229927, step_size 0.9807860175554679, test_acc: 0.0797\n",
            "Epoch 450, BestLoss: 0.23756564534041716, Temperature 0.022467088258818428, step_size 0.9806879389537123, test_acc: 0.1322\n",
            "Epoch 451, BestLoss: 0.23004346784609564, Temperature 0.021343733845877507, step_size 0.9805898701598169, test_acc: 0.0954\n",
            "Epoch 452, BestLoss: 0.20647301441234805, Temperature 0.02027654715358363, step_size 0.9804918111728009, test_acc: 0.0983\n",
            "Epoch 453, BestLoss: 0.15098578596004297, Temperature 0.019262719795904448, step_size 0.9803937619916837, test_acc: 0.1255\n",
            "Epoch 454, BestLoss: 0.13995719253296213, Temperature 0.018299583806109226, step_size 0.9802957226154846, test_acc: 0.0611\n",
            "Epoch 455, BestLoss: 0.11595604935719406, Temperature 0.017384604615803764, step_size 0.980197693043223, test_acc: 0.1086\n",
            "Epoch 456, BestLoss: 0.14854355408233627, Temperature 0.016515374385013576, step_size 0.9800996732739187, test_acc: 0.0798\n",
            "Epoch 457, BestLoss: 0.14854355408233627, Temperature 0.015689605665762895, step_size 0.9800016633065913, test_acc: 0.085\n",
            "Epoch 458, BestLoss: 0.13009463783396596, Temperature 0.016515374385013576, step_size 0.9813746951860012, test_acc: 0.0332\n",
            "Epoch 459, BestLoss: 0.13009463783396596, Temperature 0.015689605665762895, step_size 0.9812765577164826, test_acc: 0.03\n",
            "Epoch 460, BestLoss: 0.13030453067670963, Temperature 0.016515374385013576, step_size 0.9812765577164826, test_acc: 0.0617\n",
            "Epoch 461, BestLoss: 0.13402912514816137, Temperature 0.015689605665762895, step_size 0.9811784300607109, test_acc: 0.0859\n",
            "Epoch 462, BestLoss: 0.13402912514816137, Temperature 0.01490512538247475, step_size 0.9810803122177049, test_acc: 0.0861\n",
            "Epoch 463, BestLoss: 0.13402912514816137, Temperature 0.015689605665762895, step_size 0.9811784300607109, test_acc: 0.0893\n",
            "Epoch 464, BestLoss: 0.13402912514816137, Temperature 0.016515374385013576, step_size 0.9810803122177049, test_acc: 0.092\n",
            "Epoch 465, BestLoss: 0.13402912514816137, Temperature 0.017384604615803764, step_size 0.9809822041864831, test_acc: 0.0942\n",
            "Epoch 466, BestLoss: 0.12873153108692476, Temperature 0.018299583806109226, step_size 0.9808841059660645, test_acc: 0.0953\n",
            "Epoch 467, BestLoss: 0.12873153108692476, Temperature 0.017384604615803764, step_size 0.9807860175554679, test_acc: 0.0921\n",
            "Epoch 468, BestLoss: 0.12873153108692476, Temperature 0.018299583806109226, step_size 0.9807860175554679, test_acc: 0.092\n",
            "Epoch 469, BestLoss: 0.12873153108692476, Temperature 0.019262719795904448, step_size 0.9806879389537123, test_acc: 0.086\n",
            "Epoch 470, BestLoss: 0.12873153108692476, Temperature 0.02027654715358363, step_size 0.9805898701598169, test_acc: 0.0869\n",
            "Epoch 471, BestLoss: 0.12873153108692476, Temperature 0.021343733845877507, step_size 0.9804918111728009, test_acc: 0.0945\n",
            "Epoch 472, BestLoss: 0.12873153108692476, Temperature 0.022467088258818428, step_size 0.9803937619916837, test_acc: 0.1105\n",
            "Epoch 473, BestLoss: 0.12873153108692476, Temperature 0.023649566588229927, step_size 0.9802957226154846, test_acc: 0.1389\n",
            "Epoch 474, BestLoss: 0.12873153108692476, Temperature 0.0248942806191894, step_size 0.980197693043223, test_acc: 0.1929\n",
            "Epoch 475, BestLoss: 0.16874428216385703, Temperature 0.02620450591493621, step_size 0.9800996732739187, test_acc: 0.0834\n",
            "Epoch 476, BestLoss: 0.1745057231794719, Temperature 0.0248942806191894, step_size 0.9800016633065913, test_acc: 0.0987\n",
            "Epoch 477, BestLoss: 0.1745057231794719, Temperature 0.023649566588229927, step_size 0.9799036631402607, test_acc: 0.0936\n",
            "Epoch 478, BestLoss: 0.1745057231794719, Temperature 0.0248942806191894, step_size 0.9800016633065913, test_acc: 0.0918\n",
            "Epoch 479, BestLoss: 0.14590962063250462, Temperature 0.02620450591493621, step_size 0.9799036631402607, test_acc: 0.0996\n",
            "Epoch 480, BestLoss: 0.14590962063250462, Temperature 0.0248942806191894, step_size 0.9798056727739467, test_acc: 0.1053\n",
            "Epoch 481, BestLoss: 0.14590962063250462, Temperature 0.02620450591493621, step_size 0.9798056727739467, test_acc: 0.1187\n",
            "Epoch 482, BestLoss: 0.14590962063250462, Temperature 0.027583690436774957, step_size 0.9797076922066693, test_acc: 0.1404\n",
            "Epoch 483, BestLoss: 0.12342065482232414, Temperature 0.029035463617657853, step_size 0.9796097214374486, test_acc: 0.0806\n",
            "Epoch 484, BestLoss: 0.12342065482232414, Temperature 0.027583690436774957, step_size 0.9795117604653049, test_acc: 0.0934\n",
            "Epoch 485, BestLoss: 0.12342065482232414, Temperature 0.029035463617657853, step_size 0.9795117604653049, test_acc: 0.1077\n",
            "Epoch 486, BestLoss: 0.12342065482232414, Temperature 0.030563645913324056, step_size 0.9794138092892584, test_acc: 0.1136\n",
            "Epoch 487, BestLoss: 0.12342065482232414, Temperature 0.032172258856130585, step_size 0.9793158679083295, test_acc: 0.1186\n",
            "Epoch 488, BestLoss: 0.12342065482232414, Temperature 0.0338655356380322, step_size 0.9792179363215386, test_acc: 0.1307\n",
            "Epoch 489, BestLoss: 0.12342065482232414, Temperature 0.03564793225056021, step_size 0.9791200145279065, test_acc: 0.1455\n",
            "Epoch 490, BestLoss: 0.12342065482232414, Temperature 0.03752413921111601, step_size 0.9790221025264537, test_acc: 0.1769\n",
            "Epoch 491, BestLoss: 0.12342065482232414, Temperature 0.03949909390643791, step_size 0.9789242003162011, test_acc: 0.231\n",
            "Epoch 492, BestLoss: 0.16579905264288614, Temperature 0.041577993585724116, step_size 0.9788263078961694, test_acc: 0.0871\n",
            "Epoch 493, BestLoss: 0.1718146066225043, Temperature 0.03949909390643791, step_size 0.9787284252653798, test_acc: 0.1456\n",
            "Epoch 494, BestLoss: 0.1916851704029982, Temperature 0.03752413921111601, step_size 0.9786305524228534, test_acc: 0.0681\n",
            "Epoch 495, BestLoss: 0.18258025672148323, Temperature 0.03564793225056021, step_size 0.9785326893676111, test_acc: 0.0965\n",
            "Epoch 496, BestLoss: 0.21406302569530186, Temperature 0.0338655356380322, step_size 0.9784348360986743, test_acc: 0.0503\n",
            "Epoch 497, BestLoss: 0.14993661009650727, Temperature 0.032172258856130585, step_size 0.9783369926150645, test_acc: 0.0091\n",
            "Epoch 498, BestLoss: 0.16626448674606614, Temperature 0.030563645913324056, step_size 0.9782391589158029, test_acc: 0.0951\n",
            "Epoch 499, BestLoss: 0.16872342798302678, Temperature 0.029035463617657853, step_size 0.9781413349999114, test_acc: 0.0402\n",
            "Epoch 500, BestLoss: 0.18396566668338937, Temperature 0.027583690436774957, step_size 0.9780435208664114, test_acc: 0.0537\n",
            "Epoch 501, BestLoss: 0.19497986358263433, Temperature 0.02620450591493621, step_size 0.9779457165143248, test_acc: 0.1047\n",
            "Epoch 502, BestLoss: 0.19497986358263433, Temperature 0.0248942806191894, step_size 0.9778479219426734, test_acc: 0.107\n",
            "Epoch 503, BestLoss: 0.19497986358263433, Temperature 0.02620450591493621, step_size 0.9787284252653798, test_acc: 0.112\n",
            "Epoch 504, BestLoss: 0.1744139969266664, Temperature 0.027583690436774957, step_size 0.9786305524228534, test_acc: 0.0866\n",
            "Epoch 505, BestLoss: 0.1744139969266664, Temperature 0.02620450591493621, step_size 0.9785326893676111, test_acc: 0.0967\n",
            "Epoch 506, BestLoss: 0.15346109362158353, Temperature 0.027583690436774957, step_size 0.9785326893676111, test_acc: 0.0955\n",
            "Epoch 507, BestLoss: 0.14343226060418363, Temperature 0.02620450591493621, step_size 0.9784348360986743, test_acc: 0.1012\n",
            "Epoch 508, BestLoss: 0.14343226060418363, Temperature 0.0248942806191894, step_size 0.9783369926150645, test_acc: 0.1032\n",
            "Epoch 509, BestLoss: 0.14497721348539006, Temperature 0.02620450591493621, step_size 0.9784348360986743, test_acc: 0.1236\n",
            "Epoch 510, BestLoss: 0.14497721348539006, Temperature 0.0248942806191894, step_size 0.9783369926150645, test_acc: 0.1263\n",
            "Epoch 511, BestLoss: 0.12880394563813033, Temperature 0.02620450591493621, step_size 0.9783369926150645, test_acc: 0.1067\n",
            "Epoch 512, BestLoss: 0.12880394563813033, Temperature 0.0248942806191894, step_size 0.9782391589158029, test_acc: 0.1074\n",
            "Epoch 513, BestLoss: 0.12880394563813033, Temperature 0.02620450591493621, step_size 0.9782391589158029, test_acc: 0.1128\n",
            "Epoch 514, BestLoss: 0.16779352962961486, Temperature 0.027583690436774957, step_size 0.9781413349999114, test_acc: 0.1032\n",
            "Epoch 515, BestLoss: 0.16779352962961486, Temperature 0.02620450591493621, step_size 0.9780435208664114, test_acc: 0.1032\n",
            "Epoch 516, BestLoss: 0.17293504515511343, Temperature 0.027583690436774957, step_size 0.9780435208664114, test_acc: 0.0859\n",
            "Epoch 517, BestLoss: 0.12253946551802425, Temperature 0.02620450591493621, step_size 0.9779457165143248, test_acc: 0.1068\n",
            "Epoch 518, BestLoss: 0.16052401583209605, Temperature 0.0248942806191894, step_size 0.9778479219426734, test_acc: 0.0657\n",
            "Epoch 519, BestLoss: 0.1498227420314159, Temperature 0.023649566588229927, step_size 0.9777501371504792, test_acc: 0.0377\n",
            "Epoch 520, BestLoss: 0.1498227420314159, Temperature 0.022467088258818428, step_size 0.9776523621367642, test_acc: 0.0398\n",
            "Epoch 521, BestLoss: 0.1498227420314159, Temperature 0.023649566588229927, step_size 0.9779457165143248, test_acc: 0.0433\n",
            "Epoch 522, BestLoss: 0.15100682582435962, Temperature 0.0248942806191894, step_size 0.9778479219426734, test_acc: 0.0631\n",
            "Epoch 523, BestLoss: 0.15100682582435962, Temperature 0.023649566588229927, step_size 0.9777501371504792, test_acc: 0.0695\n",
            "Epoch 524, BestLoss: 0.15100682582435962, Temperature 0.0248942806191894, step_size 0.9777501371504792, test_acc: 0.0767\n",
            "Epoch 525, BestLoss: 0.15100682582435962, Temperature 0.02620450591493621, step_size 0.9776523621367642, test_acc: 0.091\n",
            "Epoch 526, BestLoss: 0.15949449755309578, Temperature 0.027583690436774957, step_size 0.9775545969005506, test_acc: 0.0781\n",
            "Epoch 527, BestLoss: 0.15949449755309578, Temperature 0.02620450591493621, step_size 0.9774568414408605, test_acc: 0.0757\n",
            "Epoch 528, BestLoss: 0.15949449755309578, Temperature 0.027583690436774957, step_size 0.9774568414408605, test_acc: 0.0777\n",
            "Epoch 529, BestLoss: 0.15949449755309578, Temperature 0.029035463617657853, step_size 0.9773590957567164, test_acc: 0.092\n",
            "Epoch 530, BestLoss: 0.14278056557428048, Temperature 0.030563645913324056, step_size 0.9772613598471408, test_acc: 0.0428\n",
            "Epoch 531, BestLoss: 0.14278056557428048, Temperature 0.029035463617657853, step_size 0.9771636337111561, test_acc: 0.0564\n",
            "Epoch 532, BestLoss: 0.14566761822053953, Temperature 0.030563645913324056, step_size 0.9771636337111561, test_acc: 0.1021\n",
            "Epoch 533, BestLoss: 0.14524299127501722, Temperature 0.029035463617657853, step_size 0.977065917347785, test_acc: 0.1023\n",
            "Epoch 534, BestLoss: 0.14524299127501722, Temperature 0.027583690436774957, step_size 0.9769682107560502, test_acc: 0.1081\n",
            "Epoch 535, BestLoss: 0.13181363410126237, Temperature 0.029035463617657853, step_size 0.977065917347785, test_acc: 0.0787\n",
            "Epoch 536, BestLoss: 0.14024894625663845, Temperature 0.027583690436774957, step_size 0.9769682107560502, test_acc: 0.0919\n",
            "Epoch 537, BestLoss: 0.14024894625663845, Temperature 0.02620450591493621, step_size 0.9768705139349746, test_acc: 0.0843\n",
            "Epoch 538, BestLoss: 0.1522349008112303, Temperature 0.027583690436774957, step_size 0.9769682107560502, test_acc: 0.122\n",
            "Epoch 539, BestLoss: 0.1522349008112303, Temperature 0.02620450591493621, step_size 0.9768705139349746, test_acc: 0.1121\n",
            "Epoch 540, BestLoss: 0.15221666588598434, Temperature 0.027583690436774957, step_size 0.9768705139349746, test_acc: 0.2129\n",
            "Epoch 541, BestLoss: 0.15221666588598434, Temperature 0.02620450591493621, step_size 0.9767728268835811, test_acc: 0.2234\n",
            "Epoch 542, BestLoss: 0.15221666588598434, Temperature 0.027583690436774957, step_size 0.9767728268835811, test_acc: 0.2326\n",
            "Epoch 543, BestLoss: 0.17927636769099478, Temperature 0.029035463617657853, step_size 0.9766751496008927, test_acc: 0.1709\n",
            "Epoch 544, BestLoss: 0.15628884431609746, Temperature 0.027583690436774957, step_size 0.9765774820859326, test_acc: 0.162\n",
            "Epoch 545, BestLoss: 0.1383414225072127, Temperature 0.02620450591493621, step_size 0.9764798243377241, test_acc: 0.105\n",
            "Epoch 546, BestLoss: 0.1383414225072127, Temperature 0.0248942806191894, step_size 0.9763821763552903, test_acc: 0.1095\n",
            "Epoch 547, BestLoss: 0.1383414225072127, Temperature 0.02620450591493621, step_size 0.9765774820859326, test_acc: 0.119\n",
            "Epoch 548, BestLoss: 0.15349634114244726, Temperature 0.027583690436774957, step_size 0.9764798243377241, test_acc: 0.0875\n",
            "Epoch 549, BestLoss: 0.11967768558997756, Temperature 0.02620450591493621, step_size 0.9763821763552903, test_acc: 0.1092\n",
            "Epoch 550, BestLoss: 0.11967768558997756, Temperature 0.0248942806191894, step_size 0.9762845381376548, test_acc: 0.1094\n",
            "Epoch 551, BestLoss: 0.11967768558997756, Temperature 0.02620450591493621, step_size 0.9763821763552903, test_acc: 0.1064\n",
            "Epoch 552, BestLoss: 0.11967768558997756, Temperature 0.027583690436774957, step_size 0.9762845381376548, test_acc: 0.1074\n",
            "Epoch 553, BestLoss: 0.11967768558997756, Temperature 0.029035463617657853, step_size 0.976186909683841, test_acc: 0.1161\n",
            "Epoch 554, BestLoss: 0.14910273214361008, Temperature 0.030563645913324056, step_size 0.9760892909928727, test_acc: 0.0875\n",
            "Epoch 555, BestLoss: 0.16506480352123606, Temperature 0.029035463617657853, step_size 0.9759916820637734, test_acc: 0.1385\n",
            "Epoch 556, BestLoss: 0.1311049359308444, Temperature 0.027583690436774957, step_size 0.975894082895567, test_acc: 0.1055\n",
            "Epoch 557, BestLoss: 0.13094755001030495, Temperature 0.02620450591493621, step_size 0.9757964934872775, test_acc: 0.0673\n",
            "Epoch 558, BestLoss: 0.13094755001030495, Temperature 0.0248942806191894, step_size 0.9756989138379287, test_acc: 0.0676\n",
            "Epoch 559, BestLoss: 0.18162802010501913, Temperature 0.02620450591493621, step_size 0.9759916820637734, test_acc: 0.0709\n",
            "Epoch 560, BestLoss: 0.1633329438546974, Temperature 0.0248942806191894, step_size 0.975894082895567, test_acc: 0.0962\n",
            "Epoch 561, BestLoss: 0.16083232907227174, Temperature 0.023649566588229927, step_size 0.9757964934872775, test_acc: 0.1295\n",
            "Epoch 562, BestLoss: 0.16083232907227174, Temperature 0.022467088258818428, step_size 0.9756989138379287, test_acc: 0.1435\n",
            "Epoch 563, BestLoss: 0.16083232907227174, Temperature 0.023649566588229927, step_size 0.975894082895567, test_acc: 0.1706\n",
            "Epoch 564, BestLoss: 0.16083232907227174, Temperature 0.0248942806191894, step_size 0.9757964934872775, test_acc: 0.2026\n",
            "Epoch 565, BestLoss: 0.16356695500118368, Temperature 0.02620450591493621, step_size 0.9756989138379287, test_acc: 0.0862\n",
            "Epoch 566, BestLoss: 0.16356695500118368, Temperature 0.0248942806191894, step_size 0.975601343946545, test_acc: 0.0904\n",
            "Epoch 567, BestLoss: 0.15645439033423397, Temperature 0.02620450591493621, step_size 0.975601343946545, test_acc: 0.0852\n",
            "Epoch 568, BestLoss: 0.11073702689441726, Temperature 0.0248942806191894, step_size 0.9755037838121503, test_acc: 0.1211\n",
            "Epoch 569, BestLoss: 0.11073702689441726, Temperature 0.023649566588229927, step_size 0.9754062334337691, test_acc: 0.1201\n",
            "Epoch 570, BestLoss: 0.12082760396474368, Temperature 0.0248942806191894, step_size 0.9755037838121503, test_acc: 0.137\n",
            "Epoch 571, BestLoss: 0.14923019704963864, Temperature 0.023649566588229927, step_size 0.9754062334337691, test_acc: 0.0524\n",
            "Epoch 572, BestLoss: 0.14503089417833487, Temperature 0.022467088258818428, step_size 0.9753086928104258, test_acc: 0.0905\n",
            "Epoch 573, BestLoss: 0.14503089417833487, Temperature 0.021343733845877507, step_size 0.9752111619411448, test_acc: 0.086\n",
            "Epoch 574, BestLoss: 0.1524683335271988, Temperature 0.022467088258818428, step_size 0.9754062334337691, test_acc: 0.1144\n",
            "Epoch 575, BestLoss: 0.1524683335271988, Temperature 0.021343733845877507, step_size 0.9753086928104258, test_acc: 0.1096\n",
            "Epoch 576, BestLoss: 0.1524683335271988, Temperature 0.022467088258818428, step_size 0.9753086928104258, test_acc: 0.1014\n",
            "Epoch 577, BestLoss: 0.17796902705535517, Temperature 0.023649566588229927, step_size 0.9752111619411448, test_acc: 0.0409\n",
            "Epoch 578, BestLoss: 0.19157833512400288, Temperature 0.022467088258818428, step_size 0.9751136408249507, test_acc: 0.0674\n",
            "Epoch 579, BestLoss: 0.21608041850707596, Temperature 0.021343733845877507, step_size 0.9750161294608681, test_acc: 0.1069\n",
            "Epoch 580, BestLoss: 0.24535298455850235, Temperature 0.02027654715358363, step_size 0.974918627847922, test_acc: 0.1061\n",
            "Epoch 581, BestLoss: 0.24347165090716494, Temperature 0.019262719795904448, step_size 0.9748211359851373, test_acc: 0.1057\n",
            "Epoch 582, BestLoss: 0.222007513385765, Temperature 0.018299583806109226, step_size 0.9747236538715388, test_acc: 0.0653\n",
            "Epoch 583, BestLoss: 0.21505639306934443, Temperature 0.017384604615803764, step_size 0.9746261815061517, test_acc: 0.0499\n",
            "Epoch 584, BestLoss: 0.2129975580327442, Temperature 0.016515374385013576, step_size 0.9745287188880011, test_acc: 0.0715\n",
            "Epoch 585, BestLoss: 0.2129975580327442, Temperature 0.015689605665762895, step_size 0.9744312660161123, test_acc: 0.0659\n",
            "Epoch 586, BestLoss: 0.2129975580327442, Temperature 0.016515374385013576, step_size 0.9751136408249507, test_acc: 0.0651\n",
            "Epoch 587, BestLoss: 0.16825740922252344, Temperature 0.017384604615803764, step_size 0.9750161294608681, test_acc: 0.0221\n",
            "Epoch 588, BestLoss: 0.16825740922252344, Temperature 0.016515374385013576, step_size 0.974918627847922, test_acc: 0.0236\n",
            "Epoch 589, BestLoss: 0.1936470232376124, Temperature 0.017384604615803764, step_size 0.974918627847922, test_acc: 0.1134\n",
            "Epoch 590, BestLoss: 0.18908633966739025, Temperature 0.016515374385013576, step_size 0.9748211359851373, test_acc: 0.1338\n",
            "Epoch 591, BestLoss: 0.1937927603582422, Temperature 0.015689605665762895, step_size 0.9747236538715388, test_acc: 0.0767\n",
            "Epoch 592, BestLoss: 0.1552670058729707, Temperature 0.01490512538247475, step_size 0.9746261815061517, test_acc: 0.1224\n",
            "Epoch 593, BestLoss: 0.1552670058729707, Temperature 0.014159869113351011, step_size 0.9745287188880011, test_acc: 0.1148\n",
            "Epoch 594, BestLoss: 0.14552791726261444, Temperature 0.01490512538247475, step_size 0.9748211359851373, test_acc: 0.0832\n",
            "Epoch 595, BestLoss: 0.14552791726261444, Temperature 0.014159869113351011, step_size 0.9747236538715388, test_acc: 0.0876\n",
            "Epoch 596, BestLoss: 0.14552791726261444, Temperature 0.01490512538247475, step_size 0.9747236538715388, test_acc: 0.083\n",
            "Epoch 597, BestLoss: 0.14552791726261444, Temperature 0.015689605665762895, step_size 0.9746261815061517, test_acc: 0.0838\n",
            "Epoch 598, BestLoss: 0.14552791726261444, Temperature 0.016515374385013576, step_size 0.9745287188880011, test_acc: 0.0848\n",
            "Epoch 599, BestLoss: 0.14552791726261444, Temperature 0.017384604615803764, step_size 0.9744312660161123, test_acc: 0.0878\n",
            "Epoch 600, BestLoss: 0.14552791726261444, Temperature 0.018299583806109226, step_size 0.9743338228895106, test_acc: 0.0978\n",
            "Epoch 601, BestLoss: 0.15346876597392295, Temperature 0.019262719795904448, step_size 0.9742363895072217, test_acc: 0.1021\n",
            "Epoch 602, BestLoss: 0.15292060949382516, Temperature 0.018299583806109226, step_size 0.974138965868271, test_acc: 0.1591\n",
            "Epoch 603, BestLoss: 0.15292060949382516, Temperature 0.017384604615803764, step_size 0.9740415519716842, test_acc: 0.1594\n",
            "Epoch 604, BestLoss: 0.15292060949382516, Temperature 0.018299583806109226, step_size 0.974138965868271, test_acc: 0.1557\n",
            "Epoch 605, BestLoss: 0.22244827367744502, Temperature 0.019262719795904448, step_size 0.9740415519716842, test_acc: 0.109\n",
            "Epoch 606, BestLoss: 0.19255148881879433, Temperature 0.018299583806109226, step_size 0.973944147816487, test_acc: 0.1568\n",
            "Epoch 607, BestLoss: 0.1625838652739581, Temperature 0.017384604615803764, step_size 0.9738467534017053, test_acc: 0.1038\n",
            "Epoch 608, BestLoss: 0.16149297427147621, Temperature 0.016515374385013576, step_size 0.9737493687263652, test_acc: 0.132\n",
            "Epoch 609, BestLoss: 0.1473184861631178, Temperature 0.015689605665762895, step_size 0.9736519937894925, test_acc: 0.1498\n",
            "Epoch 610, BestLoss: 0.1473184861631178, Temperature 0.01490512538247475, step_size 0.9735546285901135, test_acc: 0.1506\n",
            "Epoch 611, BestLoss: 0.16645771425016967, Temperature 0.015689605665762895, step_size 0.973944147816487, test_acc: 0.1023\n",
            "Epoch 612, BestLoss: 0.1823805895440121, Temperature 0.01490512538247475, step_size 0.9738467534017053, test_acc: 0.0823\n",
            "Epoch 613, BestLoss: 0.1823805895440121, Temperature 0.014159869113351011, step_size 0.9737493687263652, test_acc: 0.0892\n",
            "Epoch 614, BestLoss: 0.17035409738558985, Temperature 0.01490512538247475, step_size 0.9738467534017053, test_acc: 0.1032\n",
            "Epoch 615, BestLoss: 0.17035409738558985, Temperature 0.014159869113351011, step_size 0.9737493687263652, test_acc: 0.0998\n",
            "Epoch 616, BestLoss: 0.17035409738558985, Temperature 0.01490512538247475, step_size 0.9737493687263652, test_acc: 0.0939\n",
            "Epoch 617, BestLoss: 0.17035409738558985, Temperature 0.015689605665762895, step_size 0.9736519937894925, test_acc: 0.0895\n",
            "Epoch 618, BestLoss: 0.13947885890731623, Temperature 0.016515374385013576, step_size 0.9735546285901135, test_acc: 0.161\n",
            "Epoch 619, BestLoss: 0.13947885890731623, Temperature 0.015689605665762895, step_size 0.9734572731272545, test_acc: 0.1645\n",
            "Epoch 620, BestLoss: 0.13947885890731623, Temperature 0.016515374385013576, step_size 0.9734572731272545, test_acc: 0.1699\n",
            "Epoch 621, BestLoss: 0.13947885890731623, Temperature 0.017384604615803764, step_size 0.9733599273999418, test_acc: 0.1776\n",
            "Epoch 622, BestLoss: 0.13209245812304113, Temperature 0.018299583806109226, step_size 0.9732625914072018, test_acc: 0.0696\n",
            "Epoch 623, BestLoss: 0.13209245812304113, Temperature 0.017384604615803764, step_size 0.9731652651480611, test_acc: 0.074\n",
            "Epoch 624, BestLoss: 0.14987896617968866, Temperature 0.018299583806109226, step_size 0.9731652651480611, test_acc: 0.095\n",
            "Epoch 625, BestLoss: 0.1527092125050241, Temperature 0.017384604615803764, step_size 0.9730679486215463, test_acc: 0.1036\n",
            "Epoch 626, BestLoss: 0.1527092125050241, Temperature 0.016515374385013576, step_size 0.9729706418266841, test_acc: 0.1044\n",
            "Epoch 627, BestLoss: 0.15201609304291017, Temperature 0.017384604615803764, step_size 0.9730679486215463, test_acc: 0.1224\n",
            "Epoch 628, BestLoss: 0.14903760835266608, Temperature 0.016515374385013576, step_size 0.9729706418266841, test_acc: 0.1569\n",
            "Epoch 629, BestLoss: 0.14903760835266608, Temperature 0.015689605665762895, step_size 0.9728733447625014, test_acc: 0.1587\n",
            "Epoch 630, BestLoss: 0.14903760835266608, Temperature 0.016515374385013576, step_size 0.9729706418266841, test_acc: 0.1587\n",
            "Epoch 631, BestLoss: 0.14903760835266608, Temperature 0.017384604615803764, step_size 0.9728733447625014, test_acc: 0.1508\n",
            "Epoch 632, BestLoss: 0.14903760835266608, Temperature 0.018299583806109226, step_size 0.9727760574280252, test_acc: 0.143\n",
            "Epoch 633, BestLoss: 0.1615786542442692, Temperature 0.019262719795904448, step_size 0.9726787798222825, test_acc: 0.0678\n",
            "Epoch 634, BestLoss: 0.12821604160552047, Temperature 0.018299583806109226, step_size 0.9725815119443002, test_acc: 0.0643\n",
            "Epoch 635, BestLoss: 0.1145312896032069, Temperature 0.017384604615803764, step_size 0.9724842537931058, test_acc: 0.098\n",
            "Epoch 636, BestLoss: 0.1145312896032069, Temperature 0.016515374385013576, step_size 0.9723870053677265, test_acc: 0.0961\n",
            "Epoch 637, BestLoss: 0.1145312896032069, Temperature 0.017384604615803764, step_size 0.9725815119443002, test_acc: 0.0936\n",
            "Epoch 638, BestLoss: 0.1145312896032069, Temperature 0.018299583806109226, step_size 0.9724842537931058, test_acc: 0.0892\n",
            "Epoch 639, BestLoss: 0.1145312896032069, Temperature 0.019262719795904448, step_size 0.9723870053677265, test_acc: 0.0884\n",
            "Epoch 640, BestLoss: 0.1145312896032069, Temperature 0.02027654715358363, step_size 0.9722897666671897, test_acc: 0.0944\n",
            "Epoch 641, BestLoss: 0.1145312896032069, Temperature 0.021343733845877507, step_size 0.9721925376905229, test_acc: 0.1031\n",
            "Epoch 642, BestLoss: 0.1145312896032069, Temperature 0.022467088258818428, step_size 0.9720953184367539, test_acc: 0.1178\n",
            "Epoch 643, BestLoss: 0.1145312896032069, Temperature 0.023649566588229927, step_size 0.9719981089049102, test_acc: 0.1424\n",
            "Epoch 644, BestLoss: 0.1145312896032069, Temperature 0.0248942806191894, step_size 0.9719009090940197, test_acc: 0.1909\n",
            "Epoch 645, BestLoss: 0.1145312896032069, Temperature 0.02620450591493621, step_size 0.9718037190031104, test_acc: 0.2974\n",
            "Epoch 646, BestLoss: 0.1145312896032069, Temperature 0.027583690436774957, step_size 0.97170653863121, test_acc: 0.4967\n",
            "Epoch 647, BestLoss: 0.1145312896032069, Temperature 0.029035463617657853, step_size 0.9716093679773469, test_acc: 0.7023\n",
            "Epoch 648, BestLoss: 0.2104939161521309, Temperature 0.030563645913324056, step_size 0.9715122070405492, test_acc: 0.1443\n",
            "Epoch 649, BestLoss: 0.20705453007148336, Temperature 0.029035463617657853, step_size 0.9714150558198451, test_acc: 0.1075\n",
            "Epoch 650, BestLoss: 0.19437761434574177, Temperature 0.027583690436774957, step_size 0.9713179143142632, test_acc: 0.1236\n",
            "Epoch 651, BestLoss: 0.14923149278276665, Temperature 0.02620450591493621, step_size 0.9712207825228317, test_acc: 0.1126\n",
            "Epoch 652, BestLoss: 0.17450382330352077, Temperature 0.0248942806191894, step_size 0.9711236604445794, test_acc: 0.1085\n",
            "Epoch 653, BestLoss: 0.14599402889768992, Temperature 0.023649566588229927, step_size 0.971026548078535, test_acc: 0.1103\n",
            "Epoch 654, BestLoss: 0.1337357793261692, Temperature 0.022467088258818428, step_size 0.9709294454237271, test_acc: 0.1204\n",
            "Epoch 655, BestLoss: 0.12261572063304205, Temperature 0.021343733845877507, step_size 0.9708323524791846, test_acc: 0.133\n",
            "Epoch 656, BestLoss: 0.12261572063304205, Temperature 0.02027654715358363, step_size 0.9707352692439367, test_acc: 0.1367\n",
            "Epoch 657, BestLoss: 0.12261572063304205, Temperature 0.021343733845877507, step_size 0.9714150558198451, test_acc: 0.1419\n",
            "Epoch 658, BestLoss: 0.12261572063304205, Temperature 0.022467088258818428, step_size 0.9713179143142632, test_acc: 0.1454\n",
            "Epoch 659, BestLoss: 0.1203640964062461, Temperature 0.023649566588229927, step_size 0.9712207825228317, test_acc: 0.1181\n",
            "Epoch 660, BestLoss: 0.1203640964062461, Temperature 0.022467088258818428, step_size 0.9711236604445794, test_acc: 0.1231\n",
            "Epoch 661, BestLoss: 0.1203640964062461, Temperature 0.023649566588229927, step_size 0.9711236604445794, test_acc: 0.1349\n",
            "Epoch 662, BestLoss: 0.1203640964062461, Temperature 0.0248942806191894, step_size 0.971026548078535, test_acc: 0.1527\n",
            "Epoch 663, BestLoss: 0.1203640964062461, Temperature 0.02620450591493621, step_size 0.9709294454237271, test_acc: 0.1604\n",
            "Epoch 664, BestLoss: 0.1203640964062461, Temperature 0.027583690436774957, step_size 0.9708323524791846, test_acc: 0.1643\n",
            "Epoch 665, BestLoss: 0.1203640964062461, Temperature 0.029035463617657853, step_size 0.9707352692439367, test_acc: 0.18\n",
            "Epoch 666, BestLoss: 0.1203640964062461, Temperature 0.030563645913324056, step_size 0.9706381957170124, test_acc: 0.2097\n",
            "Epoch 667, BestLoss: 0.13802012292257004, Temperature 0.032172258856130585, step_size 0.9705411318974407, test_acc: 0.103\n",
            "Epoch 668, BestLoss: 0.13802012292257004, Temperature 0.030563645913324056, step_size 0.970444077784251, test_acc: 0.1161\n",
            "Epoch 669, BestLoss: 0.16785757236508958, Temperature 0.032172258856130585, step_size 0.970444077784251, test_acc: 0.1067\n",
            "Epoch 670, BestLoss: 0.16785757236508958, Temperature 0.030563645913324056, step_size 0.9703470333764725, test_acc: 0.1098\n",
            "Epoch 671, BestLoss: 0.16785757236508958, Temperature 0.032172258856130585, step_size 0.9703470333764725, test_acc: 0.1233\n",
            "Epoch 672, BestLoss: 0.16785757236508958, Temperature 0.0338655356380322, step_size 0.9702499986731349, test_acc: 0.138\n",
            "Epoch 673, BestLoss: 0.16785757236508958, Temperature 0.03564793225056021, step_size 0.9701529736732676, test_acc: 0.1484\n",
            "Epoch 674, BestLoss: 0.16785757236508958, Temperature 0.03752413921111601, step_size 0.9700559583759003, test_acc: 0.1647\n",
            "Epoch 675, BestLoss: 0.1664568576350882, Temperature 0.03949909390643791, step_size 0.9699589527800627, test_acc: 0.0822\n",
            "Epoch 676, BestLoss: 0.1664568576350882, Temperature 0.03752413921111601, step_size 0.9698619568847847, test_acc: 0.0787\n",
            "Epoch 677, BestLoss: 0.19745764961279877, Temperature 0.03949909390643791, step_size 0.9698619568847847, test_acc: 0.0973\n",
            "Epoch 678, BestLoss: 0.20838236295349116, Temperature 0.03752413921111601, step_size 0.9697649706890963, test_acc: 0.0412\n",
            "Epoch 679, BestLoss: 0.23900585597969423, Temperature 0.03564793225056021, step_size 0.9696679941920274, test_acc: 0.0966\n",
            "Epoch 680, BestLoss: 0.20512133690874965, Temperature 0.0338655356380322, step_size 0.9695710273926083, test_acc: 0.111\n",
            "Epoch 681, BestLoss: 0.18460188712636025, Temperature 0.032172258856130585, step_size 0.969474070289869, test_acc: 0.071\n",
            "Epoch 682, BestLoss: 0.1328174502133102, Temperature 0.030563645913324056, step_size 0.9693771228828401, test_acc: 0.0296\n",
            "Epoch 683, BestLoss: 0.1328174502133102, Temperature 0.029035463617657853, step_size 0.9692801851705518, test_acc: 0.0298\n",
            "Epoch 684, BestLoss: 0.1328174502133102, Temperature 0.030563645913324056, step_size 0.9697649706890963, test_acc: 0.0295\n",
            "Epoch 685, BestLoss: 0.1328174502133102, Temperature 0.032172258856130585, step_size 0.9696679941920274, test_acc: 0.0283\n",
            "Epoch 686, BestLoss: 0.1328174502133102, Temperature 0.0338655356380322, step_size 0.9695710273926083, test_acc: 0.03\n",
            "Epoch 687, BestLoss: 0.1328174502133102, Temperature 0.03564793225056021, step_size 0.969474070289869, test_acc: 0.0357\n",
            "Epoch 688, BestLoss: 0.1328174502133102, Temperature 0.03752413921111601, step_size 0.9693771228828401, test_acc: 0.0424\n",
            "Epoch 689, BestLoss: 0.1328174502133102, Temperature 0.03949909390643791, step_size 0.9692801851705518, test_acc: 0.0537\n",
            "Epoch 690, BestLoss: 0.19713330287731126, Temperature 0.041577993585724116, step_size 0.9691832571520348, test_acc: 0.0871\n",
            "Epoch 691, BestLoss: 0.26001878270334855, Temperature 0.03949909390643791, step_size 0.9690863388263196, test_acc: 0.117\n",
            "Epoch 692, BestLoss: 0.21638465737988938, Temperature 0.03752413921111601, step_size 0.9689894301924369, test_acc: 0.1002\n",
            "Epoch 693, BestLoss: 0.2310214772513515, Temperature 0.03564793225056021, step_size 0.9688925312494177, test_acc: 0.1354\n",
            "Epoch 694, BestLoss: 0.17090312240334601, Temperature 0.0338655356380322, step_size 0.9687956419962928, test_acc: 0.1395\n",
            "Epoch 695, BestLoss: 0.17090312240334601, Temperature 0.032172258856130585, step_size 0.9686987624320932, test_acc: 0.1356\n",
            "Epoch 696, BestLoss: 0.21427678201449762, Temperature 0.0338655356380322, step_size 0.9690863388263196, test_acc: 0.1223\n",
            "Epoch 697, BestLoss: 0.19424614272844615, Temperature 0.032172258856130585, step_size 0.9689894301924369, test_acc: 0.0847\n",
            "Epoch 698, BestLoss: 0.20155130569304164, Temperature 0.030563645913324056, step_size 0.9688925312494177, test_acc: 0.0704\n",
            "Epoch 699, BestLoss: 0.20155130569304164, Temperature 0.029035463617657853, step_size 0.9687956419962928, test_acc: 0.0675\n",
            "Epoch 700, BestLoss: 0.19453854344321597, Temperature 0.030563645913324056, step_size 0.9689894301924369, test_acc: 0.0875\n",
            "Epoch 701, BestLoss: 0.21092566805967378, Temperature 0.029035463617657853, step_size 0.9688925312494177, test_acc: 0.0759\n",
            "Epoch 702, BestLoss: 0.1598422714574758, Temperature 0.027583690436774957, step_size 0.9687956419962928, test_acc: 0.0837\n",
            "Epoch 703, BestLoss: 0.18417596690131088, Temperature 0.02620450591493621, step_size 0.9686987624320932, test_acc: 0.0415\n",
            "Epoch 704, BestLoss: 0.17859740813474836, Temperature 0.0248942806191894, step_size 0.9686018925558499, test_acc: 0.0428\n",
            "Epoch 705, BestLoss: 0.17222441708777506, Temperature 0.023649566588229927, step_size 0.9685050323665944, test_acc: 0.0778\n",
            "Epoch 706, BestLoss: 0.14196924742280395, Temperature 0.022467088258818428, step_size 0.9684081818633578, test_acc: 0.0773\n",
            "Epoch 707, BestLoss: 0.12863312554457637, Temperature 0.021343733845877507, step_size 0.9683113410451715, test_acc: 0.0996\n",
            "Epoch 708, BestLoss: 0.15507466451305185, Temperature 0.02027654715358363, step_size 0.968214509911067, test_acc: 0.083\n",
            "Epoch 709, BestLoss: 0.1299347148991588, Temperature 0.019262719795904448, step_size 0.9681176884600758, test_acc: 0.0648\n",
            "Epoch 710, BestLoss: 0.1299347148991588, Temperature 0.018299583806109226, step_size 0.9680208766912298, test_acc: 0.0606\n",
            "Epoch 711, BestLoss: 0.1384642250558578, Temperature 0.019262719795904448, step_size 0.9688925312494177, test_acc: 0.0989\n",
            "Epoch 712, BestLoss: 0.1384642250558578, Temperature 0.018299583806109226, step_size 0.9687956419962928, test_acc: 0.1035\n",
            "Epoch 713, BestLoss: 0.18203397184690945, Temperature 0.019262719795904448, step_size 0.9687956419962928, test_acc: 0.1514\n",
            "Epoch 714, BestLoss: 0.18203397184690945, Temperature 0.018299583806109226, step_size 0.9686987624320932, test_acc: 0.1533\n",
            "Epoch 715, BestLoss: 0.18203397184690945, Temperature 0.019262719795904448, step_size 0.9686987624320932, test_acc: 0.1447\n",
            "Epoch 716, BestLoss: 0.14999945596134265, Temperature 0.02027654715358363, step_size 0.9686018925558499, test_acc: 0.0966\n",
            "Epoch 717, BestLoss: 0.1441970411070445, Temperature 0.019262719795904448, step_size 0.9685050323665944, test_acc: 0.1012\n",
            "Epoch 718, BestLoss: 0.1441970411070445, Temperature 0.018299583806109226, step_size 0.9684081818633578, test_acc: 0.0981\n",
            "Epoch 719, BestLoss: 0.1441970411070445, Temperature 0.019262719795904448, step_size 0.9685050323665944, test_acc: 0.0953\n",
            "Epoch 720, BestLoss: 0.1441970411070445, Temperature 0.02027654715358363, step_size 0.9684081818633578, test_acc: 0.0842\n",
            "Epoch 721, BestLoss: 0.1441970411070445, Temperature 0.021343733845877507, step_size 0.9683113410451715, test_acc: 0.0738\n",
            "Epoch 722, BestLoss: 0.1441970411070445, Temperature 0.022467088258818428, step_size 0.968214509911067, test_acc: 0.0701\n",
            "Epoch 723, BestLoss: 0.1756802837380674, Temperature 0.023649566588229927, step_size 0.9681176884600758, test_acc: 0.0846\n",
            "Epoch 724, BestLoss: 0.1756802837380674, Temperature 0.022467088258818428, step_size 0.9680208766912298, test_acc: 0.0926\n",
            "Epoch 725, BestLoss: 0.1756802837380674, Temperature 0.023649566588229927, step_size 0.9680208766912298, test_acc: 0.0963\n",
            "Epoch 726, BestLoss: 0.1756802837380674, Temperature 0.0248942806191894, step_size 0.9679240746035607, test_acc: 0.1026\n",
            "Epoch 727, BestLoss: 0.1756802837380674, Temperature 0.02620450591493621, step_size 0.9678272821961004, test_acc: 0.1285\n",
            "Epoch 728, BestLoss: 0.1756802837380674, Temperature 0.027583690436774957, step_size 0.9677304994678808, test_acc: 0.1421\n",
            "Epoch 729, BestLoss: 0.1778824133787168, Temperature 0.029035463617657853, step_size 0.967633726417934, test_acc: 0.1357\n",
            "Epoch 730, BestLoss: 0.1854728606732594, Temperature 0.027583690436774957, step_size 0.9675369630452922, test_acc: 0.2118\n",
            "Epoch 731, BestLoss: 0.1854728606732594, Temperature 0.02620450591493621, step_size 0.9674402093489878, test_acc: 0.2091\n",
            "Epoch 732, BestLoss: 0.21623837725035008, Temperature 0.027583690436774957, step_size 0.9675369630452922, test_acc: 0.141\n",
            "Epoch 733, BestLoss: 0.14882258748233457, Temperature 0.02620450591493621, step_size 0.9674402093489878, test_acc: 0.1065\n",
            "Epoch 734, BestLoss: 0.14882258748233457, Temperature 0.0248942806191894, step_size 0.9673434653280529, test_acc: 0.0963\n",
            "Epoch 735, BestLoss: 0.14882258748233457, Temperature 0.02620450591493621, step_size 0.9674402093489878, test_acc: 0.0837\n",
            "Epoch 736, BestLoss: 0.14882258748233457, Temperature 0.027583690436774957, step_size 0.9673434653280529, test_acc: 0.0722\n",
            "Epoch 737, BestLoss: 0.14882258748233457, Temperature 0.029035463617657853, step_size 0.9672467309815201, test_acc: 0.0796\n",
            "Epoch 738, BestLoss: 0.14882258748233457, Temperature 0.030563645913324056, step_size 0.967150006308422, test_acc: 0.0868\n",
            "Epoch 739, BestLoss: 0.14972753931813787, Temperature 0.032172258856130585, step_size 0.9670532913077912, test_acc: 0.1216\n",
            "Epoch 740, BestLoss: 0.14972753931813787, Temperature 0.030563645913324056, step_size 0.9669565859786604, test_acc: 0.118\n",
            "Epoch 741, BestLoss: 0.14972753931813787, Temperature 0.032172258856130585, step_size 0.9669565859786604, test_acc: 0.1118\n",
            "Epoch 742, BestLoss: 0.14972753931813787, Temperature 0.0338655356380322, step_size 0.9668598903200626, test_acc: 0.1087\n",
            "Epoch 743, BestLoss: 0.11283439329808323, Temperature 0.03564793225056021, step_size 0.9667632043310306, test_acc: 0.0561\n",
            "Epoch 744, BestLoss: 0.11283439329808323, Temperature 0.0338655356380322, step_size 0.9666665280105975, test_acc: 0.0771\n",
            "Epoch 745, BestLoss: 0.1245883937563598, Temperature 0.03564793225056021, step_size 0.9666665280105975, test_acc: 0.0865\n",
            "Epoch 746, BestLoss: 0.1245883937563598, Temperature 0.0338655356380322, step_size 0.9665698613577964, test_acc: 0.0837\n",
            "Epoch 747, BestLoss: 0.1245883937563598, Temperature 0.03564793225056021, step_size 0.9665698613577964, test_acc: 0.0862\n",
            "Epoch 748, BestLoss: 0.1245883937563598, Temperature 0.03752413921111601, step_size 0.9664732043716606, test_acc: 0.0963\n",
            "Epoch 749, BestLoss: 0.1137142177938656, Temperature 0.03949909390643791, step_size 0.9663765570512235, test_acc: 0.0928\n",
            "Epoch 750, BestLoss: 0.13052488328355638, Temperature 0.03752413921111601, step_size 0.9662799193955184, test_acc: 0.1326\n",
            "Epoch 751, BestLoss: 0.12063829979290117, Temperature 0.03564793225056021, step_size 0.9661832914035788, test_acc: 0.1173\n",
            "Epoch 752, BestLoss: 0.12063829979290117, Temperature 0.0338655356380322, step_size 0.9660866730744385, test_acc: 0.1167\n",
            "Epoch 753, BestLoss: 0.1158783286146587, Temperature 0.03564793225056021, step_size 0.9662799193955184, test_acc: 0.0902\n",
            "Epoch 754, BestLoss: 0.13842997116720415, Temperature 0.0338655356380322, step_size 0.9661832914035788, test_acc: 0.156\n",
            "Epoch 755, BestLoss: 0.16209193224725987, Temperature 0.032172258856130585, step_size 0.9660866730744385, test_acc: 0.1592\n",
            "Epoch 756, BestLoss: 0.16209193224725987, Temperature 0.030563645913324056, step_size 0.9659900644071311, test_acc: 0.156\n",
            "Epoch 757, BestLoss: 0.16602633768041603, Temperature 0.032172258856130585, step_size 0.9661832914035788, test_acc: 0.1224\n",
            "Epoch 758, BestLoss: 0.15217078855959973, Temperature 0.030563645913324056, step_size 0.9660866730744385, test_acc: 0.0765\n",
            "Epoch 759, BestLoss: 0.16807966756662526, Temperature 0.029035463617657853, step_size 0.9659900644071311, test_acc: 0.0693\n",
            "Epoch 760, BestLoss: 0.17187610684700616, Temperature 0.027583690436774957, step_size 0.9658934654006904, test_acc: 0.0667\n",
            "Epoch 761, BestLoss: 0.2140134654703478, Temperature 0.02620450591493621, step_size 0.9657968760541503, test_acc: 0.0676\n",
            "Epoch 762, BestLoss: 0.22069912702614417, Temperature 0.0248942806191894, step_size 0.9657002963665449, test_acc: 0.099\n",
            "Epoch 763, BestLoss: 0.21171971905858727, Temperature 0.023649566588229927, step_size 0.9656037263369083, test_acc: 0.056\n",
            "Epoch 764, BestLoss: 0.20682165818454507, Temperature 0.022467088258818428, step_size 0.9655071659642745, test_acc: 0.0528\n",
            "Epoch 765, BestLoss: 0.20312883454099717, Temperature 0.021343733845877507, step_size 0.9654106152476781, test_acc: 0.0482\n",
            "Epoch 766, BestLoss: 0.20312883454099717, Temperature 0.02027654715358363, step_size 0.9653140741861533, test_acc: 0.0527\n",
            "Epoch 767, BestLoss: 0.20655005134092397, Temperature 0.021343733845877507, step_size 0.9660866730744385, test_acc: 0.0779\n",
            "Epoch 768, BestLoss: 0.20217808998958503, Temperature 0.02027654715358363, step_size 0.9659900644071311, test_acc: 0.0902\n",
            "Epoch 769, BestLoss: 0.22332459759838316, Temperature 0.019262719795904448, step_size 0.9658934654006904, test_acc: 0.0858\n",
            "Epoch 770, BestLoss: 0.1975493084574801, Temperature 0.018299583806109226, step_size 0.9657968760541503, test_acc: 0.1185\n",
            "Epoch 771, BestLoss: 0.1975493084574801, Temperature 0.017384604615803764, step_size 0.9657002963665449, test_acc: 0.1188\n",
            "Epoch 772, BestLoss: 0.16750187251589896, Temperature 0.018299583806109226, step_size 0.9659900644071311, test_acc: 0.1001\n",
            "Epoch 773, BestLoss: 0.17065323993781256, Temperature 0.017384604615803764, step_size 0.9658934654006904, test_acc: 0.0959\n",
            "Epoch 774, BestLoss: 0.17239096204120472, Temperature 0.016515374385013576, step_size 0.9657968760541503, test_acc: 0.1228\n",
            "Epoch 775, BestLoss: 0.1746339751505959, Temperature 0.015689605665762895, step_size 0.9657002963665449, test_acc: 0.1017\n",
            "Epoch 776, BestLoss: 0.13293505499737177, Temperature 0.01490512538247475, step_size 0.9656037263369083, test_acc: 0.079\n",
            "Epoch 777, BestLoss: 0.13293505499737177, Temperature 0.014159869113351011, step_size 0.9655071659642745, test_acc: 0.088\n",
            "Epoch 778, BestLoss: 0.13293505499737177, Temperature 0.01490512538247475, step_size 0.9658934654006904, test_acc: 0.1023\n",
            "Epoch 779, BestLoss: 0.13293505499737177, Temperature 0.015689605665762895, step_size 0.9657968760541503, test_acc: 0.1121\n",
            "Epoch 780, BestLoss: 0.13293505499737177, Temperature 0.016515374385013576, step_size 0.9657002963665449, test_acc: 0.1124\n",
            "Epoch 781, BestLoss: 0.13293505499737177, Temperature 0.017384604615803764, step_size 0.9656037263369083, test_acc: 0.1221\n",
            "Epoch 782, BestLoss: 0.14351460715679687, Temperature 0.018299583806109226, step_size 0.9655071659642745, test_acc: 0.0523\n",
            "Epoch 783, BestLoss: 0.14351460715679687, Temperature 0.017384604615803764, step_size 0.9654106152476781, test_acc: 0.0538\n",
            "Epoch 784, BestLoss: 0.14351460715679687, Temperature 0.018299583806109226, step_size 0.9654106152476781, test_acc: 0.0566\n",
            "Epoch 785, BestLoss: 0.14351460715679687, Temperature 0.019262719795904448, step_size 0.9653140741861533, test_acc: 0.0649\n",
            "Epoch 786, BestLoss: 0.14351460715679687, Temperature 0.02027654715358363, step_size 0.9652175427787347, test_acc: 0.0749\n",
            "Epoch 787, BestLoss: 0.14351460715679687, Temperature 0.021343733845877507, step_size 0.9651210210244568, test_acc: 0.0828\n",
            "Epoch 788, BestLoss: 0.14180993776965328, Temperature 0.022467088258818428, step_size 0.9650245089223544, test_acc: 0.1479\n",
            "Epoch 789, BestLoss: 0.08144526930819729, Temperature 0.021343733845877507, step_size 0.9649280064714622, test_acc: 0.1778\n",
            "Epoch 790, BestLoss: 0.08779478830706135, Temperature 0.02027654715358363, step_size 0.964831513670815, test_acc: 0.1358\n",
            "Epoch 791, BestLoss: 0.07507342588568897, Temperature 0.019262719795904448, step_size 0.9647350305194479, test_acc: 0.1204\n",
            "Epoch 792, BestLoss: 0.07507342588568897, Temperature 0.018299583806109226, step_size 0.9646385570163959, test_acc: 0.1259\n",
            "Epoch 793, BestLoss: 0.12845944423455216, Temperature 0.019262719795904448, step_size 0.9649280064714622, test_acc: 0.1123\n",
            "Epoch 794, BestLoss: 0.12845944423455216, Temperature 0.018299583806109226, step_size 0.964831513670815, test_acc: 0.114\n",
            "Epoch 795, BestLoss: 0.12845944423455216, Temperature 0.019262719795904448, step_size 0.964831513670815, test_acc: 0.118\n",
            "Epoch 796, BestLoss: 0.12845944423455216, Temperature 0.02027654715358363, step_size 0.9647350305194479, test_acc: 0.1201\n",
            "Epoch 797, BestLoss: 0.12845944423455216, Temperature 0.021343733845877507, step_size 0.9646385570163959, test_acc: 0.1291\n",
            "Epoch 798, BestLoss: 0.12845944423455216, Temperature 0.022467088258818428, step_size 0.9645420931606943, test_acc: 0.1468\n",
            "Epoch 799, BestLoss: 0.1736232476667548, Temperature 0.023649566588229927, step_size 0.9644456389513782, test_acc: 0.1325\n",
            "Epoch 800, BestLoss: 0.1736232476667548, Temperature 0.022467088258818428, step_size 0.9643491943874831, test_acc: 0.1532\n",
            "Epoch 801, BestLoss: 0.1736232476667548, Temperature 0.023649566588229927, step_size 0.9643491943874831, test_acc: 0.1653\n",
            "Epoch 802, BestLoss: 0.1736232476667548, Temperature 0.0248942806191894, step_size 0.9642527594680443, test_acc: 0.1773\n",
            "Epoch 803, BestLoss: 0.1736232476667548, Temperature 0.02620450591493621, step_size 0.9641563341920976, test_acc: 0.1911\n",
            "Epoch 804, BestLoss: 0.1770131837926369, Temperature 0.027583690436774957, step_size 0.9640599185586783, test_acc: 0.0605\n",
            "Epoch 805, BestLoss: 0.15977290230123145, Temperature 0.02620450591493621, step_size 0.9639635125668224, test_acc: 0.1271\n",
            "Epoch 806, BestLoss: 0.1772834791058908, Temperature 0.0248942806191894, step_size 0.9638671162155658, test_acc: 0.119\n",
            "Epoch 807, BestLoss: 0.19158964591228558, Temperature 0.023649566588229927, step_size 0.9637707295039442, test_acc: 0.1332\n",
            "Epoch 808, BestLoss: 0.1985946102519352, Temperature 0.022467088258818428, step_size 0.9636743524309938, test_acc: 0.1205\n",
            "Epoch 809, BestLoss: 0.1985946102519352, Temperature 0.021343733845877507, step_size 0.9635779849957506, test_acc: 0.1264\n",
            "Epoch 810, BestLoss: 0.20561743936869895, Temperature 0.022467088258818428, step_size 0.9639635125668224, test_acc: 0.0826\n",
            "Epoch 811, BestLoss: 0.20561743936869895, Temperature 0.021343733845877507, step_size 0.9638671162155658, test_acc: 0.0809\n",
            "Epoch 812, BestLoss: 0.20561743936869895, Temperature 0.022467088258818428, step_size 0.9638671162155658, test_acc: 0.074\n",
            "Epoch 813, BestLoss: 0.20561743936869895, Temperature 0.023649566588229927, step_size 0.9637707295039442, test_acc: 0.07\n",
            "Epoch 814, BestLoss: 0.1870631389995133, Temperature 0.0248942806191894, step_size 0.9636743524309938, test_acc: 0.1144\n",
            "Epoch 815, BestLoss: 0.19141852972112794, Temperature 0.023649566588229927, step_size 0.9635779849957506, test_acc: 0.1145\n",
            "Epoch 816, BestLoss: 0.16734799638861192, Temperature 0.022467088258818428, step_size 0.9634816271972511, test_acc: 0.1115\n",
            "Epoch 817, BestLoss: 0.16734799638861192, Temperature 0.021343733845877507, step_size 0.9633852790345313, test_acc: 0.1196\n",
            "Epoch 818, BestLoss: 0.15253422698503116, Temperature 0.022467088258818428, step_size 0.9635779849957506, test_acc: 0.0394\n",
            "Epoch 819, BestLoss: 0.15253422698503116, Temperature 0.021343733845877507, step_size 0.9634816271972511, test_acc: 0.0426\n",
            "Epoch 820, BestLoss: 0.10853025045953868, Temperature 0.022467088258818428, step_size 0.9634816271972511, test_acc: 0.068\n",
            "Epoch 821, BestLoss: 0.10853025045953868, Temperature 0.021343733845877507, step_size 0.9633852790345313, test_acc: 0.0692\n",
            "Epoch 822, BestLoss: 0.11482926344933621, Temperature 0.022467088258818428, step_size 0.9633852790345313, test_acc: 0.1279\n",
            "Epoch 823, BestLoss: 0.10929930582128027, Temperature 0.021343733845877507, step_size 0.9632889405066278, test_acc: 0.1141\n",
            "Epoch 824, BestLoss: 0.10282337885038417, Temperature 0.02027654715358363, step_size 0.9631926116125772, test_acc: 0.1555\n",
            "Epoch 825, BestLoss: 0.10719590643544731, Temperature 0.019262719795904448, step_size 0.9630962923514159, test_acc: 0.0936\n",
            "Epoch 826, BestLoss: 0.11464384213833304, Temperature 0.018299583806109226, step_size 0.9629999827221808, test_acc: 0.0766\n",
            "Epoch 827, BestLoss: 0.11464384213833304, Temperature 0.017384604615803764, step_size 0.9629036827239086, test_acc: 0.0765\n",
            "Epoch 828, BestLoss: 0.1072112399824611, Temperature 0.018299583806109226, step_size 0.9632889405066278, test_acc: 0.0675\n",
            "Epoch 829, BestLoss: 0.1072112399824611, Temperature 0.017384604615803764, step_size 0.9631926116125772, test_acc: 0.0713\n",
            "Epoch 830, BestLoss: 0.1072112399824611, Temperature 0.018299583806109226, step_size 0.9631926116125772, test_acc: 0.0783\n",
            "Epoch 831, BestLoss: 0.1072112399824611, Temperature 0.019262719795904448, step_size 0.9630962923514159, test_acc: 0.091\n",
            "Epoch 832, BestLoss: 0.1072112399824611, Temperature 0.02027654715358363, step_size 0.9629999827221808, test_acc: 0.0998\n",
            "Epoch 833, BestLoss: 0.1072112399824611, Temperature 0.021343733845877507, step_size 0.9629036827239086, test_acc: 0.1086\n",
            "Epoch 834, BestLoss: 0.1072112399824611, Temperature 0.022467088258818428, step_size 0.9628073923556362, test_acc: 0.1244\n",
            "Epoch 835, BestLoss: 0.1072112399824611, Temperature 0.023649566588229927, step_size 0.9627111116164007, test_acc: 0.155\n",
            "Epoch 836, BestLoss: 0.1072112399824611, Temperature 0.0248942806191894, step_size 0.962614840505239, test_acc: 0.207\n",
            "Epoch 837, BestLoss: 0.1072112399824611, Temperature 0.02620450591493621, step_size 0.9625185790211885, test_acc: 0.3046\n",
            "Epoch 838, BestLoss: 0.1072112399824611, Temperature 0.027583690436774957, step_size 0.9624223271632865, test_acc: 0.4478\n",
            "Epoch 839, BestLoss: 0.1072112399824611, Temperature 0.029035463617657853, step_size 0.9623260849305701, test_acc: 0.6107\n",
            "Epoch 840, BestLoss: 0.1072112399824611, Temperature 0.030563645913324056, step_size 0.9622298523220771, test_acc: 0.7383\n",
            "Epoch 841, BestLoss: 0.1072112399824611, Temperature 0.032172258856130585, step_size 0.962133629336845, test_acc: 0.8097\n",
            "Epoch 842, BestLoss: 0.1072112399824611, Temperature 0.0338655356380322, step_size 0.9620374159739113, test_acc: 0.839\n",
            "Epoch 843, BestLoss: 0.18752446131455433, Temperature 0.03564793225056021, step_size 0.961941212232314, test_acc: 0.1086\n",
            "Epoch 844, BestLoss: 0.19096334637836185, Temperature 0.0338655356380322, step_size 0.9618450181110908, test_acc: 0.1219\n",
            "Epoch 845, BestLoss: 0.18898862931038696, Temperature 0.032172258856130585, step_size 0.9617488336092797, test_acc: 0.0942\n",
            "Epoch 846, BestLoss: 0.1820036777064342, Temperature 0.030563645913324056, step_size 0.9616526587259188, test_acc: 0.0959\n",
            "Epoch 847, BestLoss: 0.22407925920885735, Temperature 0.029035463617657853, step_size 0.9615564934600462, test_acc: 0.1653\n",
            "Epoch 848, BestLoss: 0.2378369225311741, Temperature 0.027583690436774957, step_size 0.9614603378107003, test_acc: 0.1146\n",
            "Epoch 849, BestLoss: 0.22778792434133216, Temperature 0.02620450591493621, step_size 0.9613641917769192, test_acc: 0.0994\n",
            "Epoch 850, BestLoss: 0.22662244200981738, Temperature 0.0248942806191894, step_size 0.9612680553577415, test_acc: 0.0492\n",
            "Epoch 851, BestLoss: 0.23985996974620508, Temperature 0.023649566588229927, step_size 0.9611719285522058, test_acc: 0.0646\n",
            "Epoch 852, BestLoss: 0.26468562590568695, Temperature 0.022467088258818428, step_size 0.9610758113593506, test_acc: 0.1022\n",
            "Epoch 853, BestLoss: 0.26468562590568695, Temperature 0.021343733845877507, step_size 0.9609797037782147, test_acc: 0.108\n",
            "Epoch 854, BestLoss: 0.26468562590568695, Temperature 0.022467088258818428, step_size 0.9618450181110908, test_acc: 0.1119\n",
            "Epoch 855, BestLoss: 0.21241553305155111, Temperature 0.023649566588229927, step_size 0.9617488336092797, test_acc: 0.0954\n",
            "Epoch 856, BestLoss: 0.21241553305155111, Temperature 0.022467088258818428, step_size 0.9616526587259188, test_acc: 0.1075\n",
            "Epoch 857, BestLoss: 0.21159709229671544, Temperature 0.023649566588229927, step_size 0.9616526587259188, test_acc: 0.0533\n",
            "Epoch 858, BestLoss: 0.17211542932542326, Temperature 0.022467088258818428, step_size 0.9615564934600462, test_acc: 0.0757\n",
            "Epoch 859, BestLoss: 0.17211542932542326, Temperature 0.021343733845877507, step_size 0.9614603378107003, test_acc: 0.0818\n",
            "Epoch 860, BestLoss: 0.19014729075672077, Temperature 0.022467088258818428, step_size 0.9615564934600462, test_acc: 0.1182\n",
            "Epoch 861, BestLoss: 0.19014729075672077, Temperature 0.021343733845877507, step_size 0.9614603378107003, test_acc: 0.1149\n",
            "Epoch 862, BestLoss: 0.17519692657259442, Temperature 0.022467088258818428, step_size 0.9614603378107003, test_acc: 0.0538\n",
            "Epoch 863, BestLoss: 0.19830807515859475, Temperature 0.021343733845877507, step_size 0.9613641917769192, test_acc: 0.0748\n",
            "Epoch 864, BestLoss: 0.19369381132576433, Temperature 0.02027654715358363, step_size 0.9612680553577415, test_acc: 0.0961\n",
            "Epoch 865, BestLoss: 0.17692863962888716, Temperature 0.019262719795904448, step_size 0.9611719285522058, test_acc: 0.0834\n",
            "Epoch 866, BestLoss: 0.17692863962888716, Temperature 0.018299583806109226, step_size 0.9610758113593506, test_acc: 0.0813\n",
            "Epoch 867, BestLoss: 0.13276768738107095, Temperature 0.019262719795904448, step_size 0.9613641917769192, test_acc: 0.0949\n",
            "Epoch 868, BestLoss: 0.13276768738107095, Temperature 0.018299583806109226, step_size 0.9612680553577415, test_acc: 0.0955\n",
            "Epoch 869, BestLoss: 0.13276768738107095, Temperature 0.019262719795904448, step_size 0.9612680553577415, test_acc: 0.0939\n",
            "Epoch 870, BestLoss: 0.13276768738107095, Temperature 0.02027654715358363, step_size 0.9611719285522058, test_acc: 0.0873\n",
            "Epoch 871, BestLoss: 0.13276768738107095, Temperature 0.021343733845877507, step_size 0.9610758113593506, test_acc: 0.0913\n",
            "Epoch 872, BestLoss: 0.12925837993485664, Temperature 0.022467088258818428, step_size 0.9609797037782147, test_acc: 0.0778\n",
            "Epoch 873, BestLoss: 0.12925837993485664, Temperature 0.021343733845877507, step_size 0.9608836058078369, test_acc: 0.0854\n",
            "Epoch 874, BestLoss: 0.13122439234880978, Temperature 0.022467088258818428, step_size 0.9608836058078369, test_acc: 0.0564\n",
            "Epoch 875, BestLoss: 0.12594526673558312, Temperature 0.021343733845877507, step_size 0.9607875174472562, test_acc: 0.1297\n",
            "Epoch 876, BestLoss: 0.12555685065930652, Temperature 0.02027654715358363, step_size 0.9606914386955115, test_acc: 0.1338\n",
            "Epoch 877, BestLoss: 0.12555685065930652, Temperature 0.019262719795904448, step_size 0.960595369551642, test_acc: 0.1258\n",
            "Epoch 878, BestLoss: 0.12555685065930652, Temperature 0.02027654715358363, step_size 0.9607875174472562, test_acc: 0.1112\n",
            "Epoch 879, BestLoss: 0.10055020640427273, Temperature 0.021343733845877507, step_size 0.9606914386955115, test_acc: 0.1215\n",
            "Epoch 880, BestLoss: 0.10055020640427273, Temperature 0.02027654715358363, step_size 0.960595369551642, test_acc: 0.1257\n",
            "Epoch 881, BestLoss: 0.10094146733756433, Temperature 0.021343733845877507, step_size 0.960595369551642, test_acc: 0.1014\n",
            "Epoch 882, BestLoss: 0.1177593044883744, Temperature 0.02027654715358363, step_size 0.9604993100146869, test_acc: 0.0883\n",
            "Epoch 883, BestLoss: 0.11383341362160179, Temperature 0.019262719795904448, step_size 0.9604032600836855, test_acc: 0.0962\n",
            "Epoch 884, BestLoss: 0.12605726706384823, Temperature 0.018299583806109226, step_size 0.9603072197576771, test_acc: 0.0742\n",
            "Epoch 885, BestLoss: 0.12605726706384823, Temperature 0.017384604615803764, step_size 0.9602111890357014, test_acc: 0.0728\n",
            "Epoch 886, BestLoss: 0.12605726706384823, Temperature 0.018299583806109226, step_size 0.9604993100146869, test_acc: 0.0681\n",
            "Epoch 887, BestLoss: 0.12605726706384823, Temperature 0.019262719795904448, step_size 0.9604032600836855, test_acc: 0.0618\n",
            "Epoch 888, BestLoss: 0.12605726706384823, Temperature 0.02027654715358363, step_size 0.9603072197576771, test_acc: 0.0626\n",
            "Epoch 889, BestLoss: 0.17623951340095811, Temperature 0.021343733845877507, step_size 0.9602111890357014, test_acc: 0.0726\n",
            "Epoch 890, BestLoss: 0.10449776944857465, Temperature 0.02027654715358363, step_size 0.9601151679167977, test_acc: 0.0431\n",
            "Epoch 891, BestLoss: 0.10449776944857465, Temperature 0.019262719795904448, step_size 0.9600191564000061, test_acc: 0.0455\n",
            "Epoch 892, BestLoss: 0.10449776944857465, Temperature 0.02027654715358363, step_size 0.9601151679167977, test_acc: 0.0531\n",
            "Epoch 893, BestLoss: 0.12787347766551424, Temperature 0.021343733845877507, step_size 0.9600191564000061, test_acc: 0.0921\n",
            "Epoch 894, BestLoss: 0.10831113258170658, Temperature 0.02027654715358363, step_size 0.9599231544843662, test_acc: 0.1691\n",
            "Epoch 895, BestLoss: 0.11174706503086818, Temperature 0.019262719795904448, step_size 0.9598271621689177, test_acc: 0.1021\n",
            "Epoch 896, BestLoss: 0.11174706503086818, Temperature 0.018299583806109226, step_size 0.9597311794527008, test_acc: 0.1076\n",
            "Epoch 897, BestLoss: 0.11174706503086818, Temperature 0.019262719795904448, step_size 0.9599231544843662, test_acc: 0.1135\n",
            "Epoch 898, BestLoss: 0.11174706503086818, Temperature 0.02027654715358363, step_size 0.9598271621689177, test_acc: 0.1295\n",
            "Epoch 899, BestLoss: 0.11174706503086818, Temperature 0.021343733845877507, step_size 0.9597311794527008, test_acc: 0.1453\n",
            "Epoch 900, BestLoss: 0.11174706503086818, Temperature 0.022467088258818428, step_size 0.9596352063347555, test_acc: 0.1647\n",
            "Epoch 901, BestLoss: 0.11174706503086818, Temperature 0.023649566588229927, step_size 0.9595392428141221, test_acc: 0.1755\n",
            "Epoch 902, BestLoss: 0.11174706503086818, Temperature 0.0248942806191894, step_size 0.9594432888898407, test_acc: 0.19\n",
            "Epoch 903, BestLoss: 0.11174706503086818, Temperature 0.02620450591493621, step_size 0.9593473445609517, test_acc: 0.233\n",
            "Epoch 904, BestLoss: 0.11174706503086818, Temperature 0.027583690436774957, step_size 0.9592514098264956, test_acc: 0.2998\n",
            "Epoch 905, BestLoss: 0.11174706503086818, Temperature 0.029035463617657853, step_size 0.9591554846855129, test_acc: 0.3924\n",
            "Epoch 906, BestLoss: 0.11174706503086818, Temperature 0.030563645913324056, step_size 0.9590595691370444, test_acc: 0.5252\n",
            "Epoch 907, BestLoss: 0.11174706503086818, Temperature 0.032172258856130585, step_size 0.9589636631801307, test_acc: 0.6833\n",
            "Epoch 908, BestLoss: 0.11174706503086818, Temperature 0.0338655356380322, step_size 0.9588677668138127, test_acc: 0.7795\n",
            "Epoch 909, BestLoss: 0.11174706503086818, Temperature 0.03564793225056021, step_size 0.9587718800371313, test_acc: 0.8277\n",
            "Epoch 910, BestLoss: 0.11174706503086818, Temperature 0.03752413921111601, step_size 0.9586760028491276, test_acc: 0.8454\n",
            "Epoch 911, BestLoss: 0.11174706503086818, Temperature 0.03949909390643791, step_size 0.9585801352488427, test_acc: 0.8511\n",
            "Epoch 912, BestLoss: 0.11174706503086818, Temperature 0.041577993585724116, step_size 0.9584842772353178, test_acc: 0.8554\n",
            "Epoch 913, BestLoss: 0.11174706503086818, Temperature 0.04376630903760433, step_size 0.9583884288075943, test_acc: 0.8567\n",
            "Epoch 914, BestLoss: 0.11174706503086818, Temperature 0.04606979898695193, step_size 0.9582925899647136, test_acc: 0.8562\n",
            "Epoch 915, BestLoss: 0.17079760995353668, Temperature 0.04849452524942309, step_size 0.9581967607057171, test_acc: 0.0593\n",
            "Epoch 916, BestLoss: 0.17079760995353668, Temperature 0.04606979898695193, step_size 0.9581009410296466, test_acc: 0.0702\n",
            "Epoch 917, BestLoss: 0.23038952278044986, Temperature 0.04849452524942309, step_size 0.9581009410296466, test_acc: 0.101\n",
            "Epoch 918, BestLoss: 0.2120008880930165, Temperature 0.04606979898695193, step_size 0.9580051309355436, test_acc: 0.0986\n",
            "Epoch 919, BestLoss: 0.17888750669378162, Temperature 0.04376630903760433, step_size 0.9579093304224501, test_acc: 0.0946\n",
            "Epoch 920, BestLoss: 0.200210700801388, Temperature 0.041577993585724116, step_size 0.9578135394894078, test_acc: 0.0945\n",
            "Epoch 921, BestLoss: 0.19147299060397646, Temperature 0.03949909390643791, step_size 0.9577177581354589, test_acc: 0.0876\n",
            "Epoch 922, BestLoss: 0.1770451696725633, Temperature 0.03752413921111601, step_size 0.9576219863596453, test_acc: 0.0966\n",
            "Epoch 923, BestLoss: 0.1770451696725633, Temperature 0.03564793225056021, step_size 0.9575262241610094, test_acc: 0.0971\n",
            "Epoch 924, BestLoss: 0.20501961723747725, Temperature 0.03752413921111601, step_size 0.9580051309355436, test_acc: 0.1073\n",
            "Epoch 925, BestLoss: 0.16847342800005657, Temperature 0.03564793225056021, step_size 0.9579093304224501, test_acc: 0.1148\n",
            "Epoch 926, BestLoss: 0.14353488528182465, Temperature 0.0338655356380322, step_size 0.9578135394894078, test_acc: 0.1305\n",
            "Epoch 927, BestLoss: 0.16167500143059205, Temperature 0.032172258856130585, step_size 0.9577177581354589, test_acc: 0.0757\n",
            "Epoch 928, BestLoss: 0.1405350460533514, Temperature 0.030563645913324056, step_size 0.9576219863596453, test_acc: 0.0738\n",
            "Epoch 929, BestLoss: 0.1619378750572133, Temperature 0.029035463617657853, step_size 0.9575262241610094, test_acc: 0.0855\n",
            "Epoch 930, BestLoss: 0.1619378750572133, Temperature 0.027583690436774957, step_size 0.9574304715385933, test_acc: 0.0811\n",
            "Epoch 931, BestLoss: 0.1619378750572133, Temperature 0.029035463617657853, step_size 0.9579093304224501, test_acc: 0.0741\n",
            "Epoch 932, BestLoss: 0.1619378750572133, Temperature 0.030563645913324056, step_size 0.9578135394894078, test_acc: 0.0672\n",
            "Epoch 933, BestLoss: 0.16502741044368732, Temperature 0.032172258856130585, step_size 0.9577177581354589, test_acc: 0.1276\n",
            "Epoch 934, BestLoss: 0.15864153307333695, Temperature 0.030563645913324056, step_size 0.9576219863596453, test_acc: 0.1128\n",
            "Epoch 935, BestLoss: 0.1630665476578801, Temperature 0.029035463617657853, step_size 0.9575262241610094, test_acc: 0.1463\n",
            "Epoch 936, BestLoss: 0.1630665476578801, Temperature 0.027583690436774957, step_size 0.9574304715385933, test_acc: 0.152\n",
            "Epoch 937, BestLoss: 0.16694368584418762, Temperature 0.029035463617657853, step_size 0.9576219863596453, test_acc: 0.0784\n",
            "Epoch 938, BestLoss: 0.16694368584418762, Temperature 0.027583690436774957, step_size 0.9575262241610094, test_acc: 0.0847\n",
            "Epoch 939, BestLoss: 0.11877051620087177, Temperature 0.029035463617657853, step_size 0.9575262241610094, test_acc: 0.1164\n",
            "Epoch 940, BestLoss: 0.11877051620087177, Temperature 0.027583690436774957, step_size 0.9574304715385933, test_acc: 0.1182\n",
            "Epoch 941, BestLoss: 0.11241413985106267, Temperature 0.029035463617657853, step_size 0.9574304715385933, test_acc: 0.1334\n",
            "Epoch 942, BestLoss: 0.11241413985106267, Temperature 0.027583690436774957, step_size 0.9573347284914394, test_acc: 0.1164\n",
            "Epoch 943, BestLoss: 0.11241413985106267, Temperature 0.029035463617657853, step_size 0.9573347284914394, test_acc: 0.0991\n",
            "Epoch 944, BestLoss: 0.11241413985106267, Temperature 0.030563645913324056, step_size 0.9572389950185903, test_acc: 0.0799\n",
            "Epoch 945, BestLoss: 0.16374830158813294, Temperature 0.032172258856130585, step_size 0.9571432711190885, test_acc: 0.0865\n",
            "Epoch 946, BestLoss: 0.16374830158813294, Temperature 0.030563645913324056, step_size 0.9570475567919766, test_acc: 0.0908\n",
            "Epoch 947, BestLoss: 0.16374830158813294, Temperature 0.032172258856130585, step_size 0.9570475567919766, test_acc: 0.0951\n",
            "Epoch 948, BestLoss: 0.16374830158813294, Temperature 0.0338655356380322, step_size 0.9569518520362974, test_acc: 0.1034\n",
            "Epoch 949, BestLoss: 0.14928233841392516, Temperature 0.03564793225056021, step_size 0.9568561568510938, test_acc: 0.1455\n",
            "Epoch 950, BestLoss: 0.14928233841392516, Temperature 0.0338655356380322, step_size 0.9567604712354087, test_acc: 0.1466\n",
            "Epoch 951, BestLoss: 0.14928233841392516, Temperature 0.03564793225056021, step_size 0.9567604712354087, test_acc: 0.1458\n",
            "Epoch 952, BestLoss: 0.14928233841392516, Temperature 0.03752413921111601, step_size 0.9566647951882852, test_acc: 0.1318\n",
            "Epoch 953, BestLoss: 0.14928233841392516, Temperature 0.03949909390643791, step_size 0.9565691287087663, test_acc: 0.138\n",
            "Epoch 954, BestLoss: 0.14928233841392516, Temperature 0.041577993585724116, step_size 0.9564734717958955, test_acc: 0.1624\n",
            "Epoch 955, BestLoss: 0.14793766208798226, Temperature 0.04376630903760433, step_size 0.9563778244487159, test_acc: 0.0716\n",
            "Epoch 956, BestLoss: 0.14793766208798226, Temperature 0.041577993585724116, step_size 0.9562821866662711, test_acc: 0.0806\n",
            "Epoch 957, BestLoss: 0.16751522400475408, Temperature 0.04376630903760433, step_size 0.9562821866662711, test_acc: 0.1267\n",
            "Epoch 958, BestLoss: 0.16751522400475408, Temperature 0.041577993585724116, step_size 0.9561865584476045, test_acc: 0.1243\n",
            "Epoch 959, BestLoss: 0.16751522400475408, Temperature 0.04376630903760433, step_size 0.9561865584476045, test_acc: 0.1192\n",
            "Epoch 960, BestLoss: 0.16751522400475408, Temperature 0.04606979898695193, step_size 0.9560909397917597, test_acc: 0.1232\n",
            "Epoch 961, BestLoss: 0.16751522400475408, Temperature 0.04849452524942309, step_size 0.9559953306977805, test_acc: 0.1266\n",
            "Epoch 962, BestLoss: 0.20251258489757074, Temperature 0.05104686868360325, step_size 0.9558997311647107, test_acc: 0.1367\n",
            "Epoch 963, BestLoss: 0.19550713751494045, Temperature 0.04849452524942309, step_size 0.9558041411915943, test_acc: 0.1464\n",
            "Epoch 964, BestLoss: 0.2273519771261457, Temperature 0.04606979898695193, step_size 0.9557085607774751, test_acc: 0.1323\n",
            "Epoch 965, BestLoss: 0.2163711810457178, Temperature 0.04376630903760433, step_size 0.9556129899213974, test_acc: 0.1404\n",
            "Epoch 966, BestLoss: 0.21841299569738812, Temperature 0.041577993585724116, step_size 0.9555174286224053, test_acc: 0.123\n",
            "Epoch 967, BestLoss: 0.2266330375313983, Temperature 0.03949909390643791, step_size 0.955421876879543, test_acc: 0.1325\n",
            "Epoch 968, BestLoss: 0.20592549434366647, Temperature 0.03752413921111601, step_size 0.9553263346918551, test_acc: 0.1323\n",
            "Epoch 969, BestLoss: 0.20592549434366647, Temperature 0.03564793225056021, step_size 0.955230802058386, test_acc: 0.1357\n",
            "Epoch 970, BestLoss: 0.19848795651787185, Temperature 0.03752413921111601, step_size 0.9558041411915943, test_acc: 0.1493\n",
            "Epoch 971, BestLoss: 0.18358411359435783, Temperature 0.03564793225056021, step_size 0.9557085607774751, test_acc: 0.1336\n",
            "Epoch 972, BestLoss: 0.150727052772428, Temperature 0.0338655356380322, step_size 0.9556129899213974, test_acc: 0.1218\n",
            "Epoch 973, BestLoss: 0.10202231772443493, Temperature 0.032172258856130585, step_size 0.9555174286224053, test_acc: 0.1172\n",
            "Epoch 974, BestLoss: 0.10202231772443493, Temperature 0.030563645913324056, step_size 0.955421876879543, test_acc: 0.1272\n",
            "Epoch 975, BestLoss: 0.10202231772443493, Temperature 0.032172258856130585, step_size 0.9557085607774751, test_acc: 0.1408\n",
            "Epoch 976, BestLoss: 0.11322269554999966, Temperature 0.0338655356380322, step_size 0.9556129899213974, test_acc: 0.099\n",
            "Epoch 977, BestLoss: 0.11322269554999966, Temperature 0.032172258856130585, step_size 0.9555174286224053, test_acc: 0.096\n",
            "Epoch 978, BestLoss: 0.11322269554999966, Temperature 0.0338655356380322, step_size 0.9555174286224053, test_acc: 0.0888\n",
            "Epoch 979, BestLoss: 0.1007550494079157, Temperature 0.03564793225056021, step_size 0.955421876879543, test_acc: 0.1053\n",
            "Epoch 980, BestLoss: 0.1007550494079157, Temperature 0.0338655356380322, step_size 0.9553263346918551, test_acc: 0.1004\n",
            "Epoch 981, BestLoss: 0.1007550494079157, Temperature 0.03564793225056021, step_size 0.9553263346918551, test_acc: 0.0928\n",
            "Epoch 982, BestLoss: 0.18914939133551478, Temperature 0.03752413921111601, step_size 0.955230802058386, test_acc: 0.0965\n",
            "Epoch 983, BestLoss: 0.182530663220882, Temperature 0.03564793225056021, step_size 0.9551352789781802, test_acc: 0.1059\n",
            "Epoch 984, BestLoss: 0.182530663220882, Temperature 0.0338655356380322, step_size 0.9550397654502824, test_acc: 0.1078\n",
            "Epoch 985, BestLoss: 0.15220634127993135, Temperature 0.03564793225056021, step_size 0.9551352789781802, test_acc: 0.1028\n",
            "Epoch 986, BestLoss: 0.18404827462618528, Temperature 0.0338655356380322, step_size 0.9550397654502824, test_acc: 0.085\n",
            "Epoch 987, BestLoss: 0.17105615248975833, Temperature 0.032172258856130585, step_size 0.9549442614737373, test_acc: 0.1712\n",
            "Epoch 988, BestLoss: 0.22999175367735467, Temperature 0.030563645913324056, step_size 0.95484876704759, test_acc: 0.1539\n",
            "Epoch 989, BestLoss: 0.1996521179751356, Temperature 0.029035463617657853, step_size 0.9547532821708853, test_acc: 0.1084\n",
            "Epoch 990, BestLoss: 0.18741447197915811, Temperature 0.027583690436774957, step_size 0.9546578068426682, test_acc: 0.0759\n",
            "Epoch 991, BestLoss: 0.18741447197915811, Temperature 0.02620450591493621, step_size 0.9545623410619839, test_acc: 0.0673\n",
            "Epoch 992, BestLoss: 0.16773889330171438, Temperature 0.027583690436774957, step_size 0.9550397654502824, test_acc: 0.1199\n",
            "Epoch 993, BestLoss: 0.14697578825968646, Temperature 0.02620450591493621, step_size 0.9549442614737373, test_acc: 0.1511\n",
            "Epoch 994, BestLoss: 0.14697578825968646, Temperature 0.0248942806191894, step_size 0.95484876704759, test_acc: 0.1482\n",
            "Epoch 995, BestLoss: 0.14697578825968646, Temperature 0.02620450591493621, step_size 0.9549442614737373, test_acc: 0.1471\n",
            "Epoch 996, BestLoss: 0.12305344389390824, Temperature 0.027583690436774957, step_size 0.95484876704759, test_acc: 0.1531\n",
            "Epoch 997, BestLoss: 0.16221779998860344, Temperature 0.02620450591493621, step_size 0.9547532821708853, test_acc: 0.114\n",
            "Epoch 998, BestLoss: 0.16869212175941747, Temperature 0.0248942806191894, step_size 0.9546578068426682, test_acc: 0.1404\n",
            "Epoch 999, BestLoss: 0.1508708953425174, Temperature 0.023649566588229927, step_size 0.9545623410619839, test_acc: 0.1147\n",
            "Epoch 1000, BestLoss: 0.1508708953425174, Temperature 0.022467088258818428, step_size 0.9544668848278777, test_acc: 0.1082\n",
            "Epoch 1001, BestLoss: 0.1536169369595893, Temperature 0.023649566588229927, step_size 0.9547532821708853, test_acc: 0.1347\n",
            "Epoch 1002, BestLoss: 0.1536169369595893, Temperature 0.022467088258818428, step_size 0.9546578068426682, test_acc: 0.1253\n",
            "Epoch 1003, BestLoss: 0.1536169369595893, Temperature 0.023649566588229927, step_size 0.9546578068426682, test_acc: 0.1112\n",
            "Epoch 1004, BestLoss: 0.1536169369595893, Temperature 0.0248942806191894, step_size 0.9545623410619839, test_acc: 0.1159\n",
            "Epoch 1005, BestLoss: 0.1536169369595893, Temperature 0.02620450591493621, step_size 0.9544668848278777, test_acc: 0.1179\n",
            "Epoch 1006, BestLoss: 0.1536169369595893, Temperature 0.027583690436774957, step_size 0.9543714381393948, test_acc: 0.1184\n",
            "Epoch 1007, BestLoss: 0.13769707781415336, Temperature 0.029035463617657853, step_size 0.954276000995581, test_acc: 0.0847\n",
            "Epoch 1008, BestLoss: 0.13769707781415336, Temperature 0.027583690436774957, step_size 0.9541805733954815, test_acc: 0.0851\n",
            "Epoch 1009, BestLoss: 0.13769707781415336, Temperature 0.029035463617657853, step_size 0.9541805733954815, test_acc: 0.0921\n",
            "Epoch 1010, BestLoss: 0.154035314749281, Temperature 0.030563645913324056, step_size 0.9540851553381419, test_acc: 0.0726\n",
            "Epoch 1011, BestLoss: 0.154035314749281, Temperature 0.029035463617657853, step_size 0.9539897468226081, test_acc: 0.0729\n",
            "Epoch 1012, BestLoss: 0.154035314749281, Temperature 0.030563645913324056, step_size 0.9539897468226081, test_acc: 0.0787\n",
            "Epoch 1013, BestLoss: 0.154035314749281, Temperature 0.032172258856130585, step_size 0.9538943478479258, test_acc: 0.0915\n",
            "Epoch 1014, BestLoss: 0.1511965545938814, Temperature 0.0338655356380322, step_size 0.953798958413141, test_acc: 0.0906\n",
            "Epoch 1015, BestLoss: 0.1511965545938814, Temperature 0.032172258856130585, step_size 0.9537035785172997, test_acc: 0.0986\n",
            "Epoch 1016, BestLoss: 0.17126630698707745, Temperature 0.0338655356380322, step_size 0.9537035785172997, test_acc: 0.0795\n",
            "Epoch 1017, BestLoss: 0.17511116422353765, Temperature 0.032172258856130585, step_size 0.953608208159448, test_acc: 0.1101\n",
            "Epoch 1018, BestLoss: 0.17511116422353765, Temperature 0.030563645913324056, step_size 0.9535128473386321, test_acc: 0.1138\n",
            "Epoch 1019, BestLoss: 0.17511116422353765, Temperature 0.032172258856130585, step_size 0.953608208159448, test_acc: 0.1216\n",
            "Epoch 1020, BestLoss: 0.1878956647150495, Temperature 0.0338655356380322, step_size 0.9535128473386321, test_acc: 0.1246\n",
            "Epoch 1021, BestLoss: 0.1878956647150495, Temperature 0.032172258856130585, step_size 0.9534174960538983, test_acc: 0.1252\n",
            "Epoch 1022, BestLoss: 0.19999307458396445, Temperature 0.0338655356380322, step_size 0.9534174960538983, test_acc: 0.1272\n",
            "Epoch 1023, BestLoss: 0.19999307458396445, Temperature 0.032172258856130585, step_size 0.9533221543042929, test_acc: 0.1277\n",
            "Epoch 1024, BestLoss: 0.14921628158615627, Temperature 0.0338655356380322, step_size 0.9533221543042929, test_acc: 0.1374\n",
            "Epoch 1025, BestLoss: 0.11587090174701625, Temperature 0.032172258856130585, step_size 0.9532268220888624, test_acc: 0.1277\n",
            "Epoch 1026, BestLoss: 0.11587090174701625, Temperature 0.030563645913324056, step_size 0.9531314994066535, test_acc: 0.13\n",
            "Epoch 1027, BestLoss: 0.11175767864458261, Temperature 0.032172258856130585, step_size 0.9532268220888624, test_acc: 0.0584\n",
            "Epoch 1028, BestLoss: 0.11175767864458261, Temperature 0.030563645913324056, step_size 0.9531314994066535, test_acc: 0.059\n",
            "Epoch 1029, BestLoss: 0.11175767864458261, Temperature 0.032172258856130585, step_size 0.9531314994066535, test_acc: 0.069\n",
            "Epoch 1030, BestLoss: 0.11980716329524534, Temperature 0.0338655356380322, step_size 0.9530361862567129, test_acc: 0.0966\n",
            "Epoch 1031, BestLoss: 0.15062255002921046, Temperature 0.032172258856130585, step_size 0.9529408826380873, test_acc: 0.1288\n",
            "Epoch 1032, BestLoss: 0.155403074227001, Temperature 0.030563645913324056, step_size 0.9528455885498235, test_acc: 0.1124\n",
            "Epoch 1033, BestLoss: 0.15095139628258908, Temperature 0.029035463617657853, step_size 0.9527503039909685, test_acc: 0.0913\n",
            "Epoch 1034, BestLoss: 0.11710139377962453, Temperature 0.027583690436774957, step_size 0.9526550289605694, test_acc: 0.1106\n",
            "Epoch 1035, BestLoss: 0.1472669211731353, Temperature 0.02620450591493621, step_size 0.9525597634576733, test_acc: 0.0761\n",
            "Epoch 1036, BestLoss: 0.1472669211731353, Temperature 0.0248942806191894, step_size 0.9524645074813276, test_acc: 0.08\n",
            "Epoch 1037, BestLoss: 0.1472669211731353, Temperature 0.02620450591493621, step_size 0.9529408826380873, test_acc: 0.0876\n",
            "Epoch 1038, BestLoss: 0.1875658223090267, Temperature 0.027583690436774957, step_size 0.9528455885498235, test_acc: 0.1115\n",
            "Epoch 1039, BestLoss: 0.1875658223090267, Temperature 0.02620450591493621, step_size 0.9527503039909685, test_acc: 0.116\n",
            "Epoch 1040, BestLoss: 0.18124567650457746, Temperature 0.027583690436774957, step_size 0.9527503039909685, test_acc: 0.0586\n",
            "Epoch 1041, BestLoss: 0.18124567650457746, Temperature 0.02620450591493621, step_size 0.9526550289605694, test_acc: 0.0569\n",
            "Epoch 1042, BestLoss: 0.250300733268375, Temperature 0.027583690436774957, step_size 0.9526550289605694, test_acc: 0.1396\n",
            "Epoch 1043, BestLoss: 0.16925765644468022, Temperature 0.02620450591493621, step_size 0.9525597634576733, test_acc: 0.102\n",
            "Epoch 1044, BestLoss: 0.18121851462102045, Temperature 0.0248942806191894, step_size 0.9524645074813276, test_acc: 0.1446\n",
            "Epoch 1045, BestLoss: 0.18235590669096757, Temperature 0.023649566588229927, step_size 0.9523692610305795, test_acc: 0.1034\n",
            "Epoch 1046, BestLoss: 0.16669438536979897, Temperature 0.022467088258818428, step_size 0.9522740241044765, test_acc: 0.13\n",
            "Epoch 1047, BestLoss: 0.16075836167861887, Temperature 0.021343733845877507, step_size 0.9521787967020661, test_acc: 0.0783\n",
            "Epoch 1048, BestLoss: 0.1932638454145881, Temperature 0.02027654715358363, step_size 0.9520835788223959, test_acc: 0.1028\n",
            "Epoch 1049, BestLoss: 0.1851902569367045, Temperature 0.019262719795904448, step_size 0.9519883704645137, test_acc: 0.1468\n",
            "Epoch 1050, BestLoss: 0.1676281812381429, Temperature 0.018299583806109226, step_size 0.9518931716274672, test_acc: 0.1176\n",
            "Epoch 1051, BestLoss: 0.1676281812381429, Temperature 0.017384604615803764, step_size 0.9517979823103044, test_acc: 0.1252\n",
            "Epoch 1052, BestLoss: 0.1676281812381429, Temperature 0.018299583806109226, step_size 0.9525597634576733, test_acc: 0.1381\n",
            "Epoch 1053, BestLoss: 0.17928677512796856, Temperature 0.019262719795904448, step_size 0.9524645074813276, test_acc: 0.1116\n",
            "Epoch 1054, BestLoss: 0.17928677512796856, Temperature 0.018299583806109226, step_size 0.9523692610305795, test_acc: 0.1172\n",
            "Epoch 1055, BestLoss: 0.23606068475701866, Temperature 0.019262719795904448, step_size 0.9523692610305795, test_acc: 0.0992\n",
            "Epoch 1056, BestLoss: 0.16002160348767114, Temperature 0.018299583806109226, step_size 0.9522740241044765, test_acc: 0.1411\n",
            "Epoch 1057, BestLoss: 0.16002160348767114, Temperature 0.017384604615803764, step_size 0.9521787967020661, test_acc: 0.1341\n",
            "Epoch 1058, BestLoss: 0.16002160348767114, Temperature 0.018299583806109226, step_size 0.9522740241044765, test_acc: 0.1202\n",
            "Epoch 1059, BestLoss: 0.16002160348767114, Temperature 0.019262719795904448, step_size 0.9521787967020661, test_acc: 0.093\n",
            "Epoch 1060, BestLoss: 0.1673802095037456, Temperature 0.02027654715358363, step_size 0.9520835788223959, test_acc: 0.1533\n",
            "Epoch 1061, BestLoss: 0.14540256802420998, Temperature 0.019262719795904448, step_size 0.9519883704645137, test_acc: 0.1125\n",
            "Epoch 1062, BestLoss: 0.14540256802420998, Temperature 0.018299583806109226, step_size 0.9518931716274672, test_acc: 0.1195\n",
            "Epoch 1063, BestLoss: 0.14540256802420998, Temperature 0.019262719795904448, step_size 0.9519883704645137, test_acc: 0.13\n",
            "Epoch 1064, BestLoss: 0.14540256802420998, Temperature 0.02027654715358363, step_size 0.9518931716274672, test_acc: 0.153\n",
            "Epoch 1065, BestLoss: 0.1373069127072245, Temperature 0.021343733845877507, step_size 0.9517979823103044, test_acc: 0.1766\n",
            "Epoch 1066, BestLoss: 0.1373069127072245, Temperature 0.02027654715358363, step_size 0.9517028025120734, test_acc: 0.1665\n",
            "Epoch 1067, BestLoss: 0.13823936940034215, Temperature 0.021343733845877507, step_size 0.9517028025120734, test_acc: 0.1065\n",
            "Epoch 1068, BestLoss: 0.13823936940034215, Temperature 0.02027654715358363, step_size 0.9516076322318222, test_acc: 0.1096\n",
            "Epoch 1069, BestLoss: 0.11704786513519314, Temperature 0.021343733845877507, step_size 0.9516076322318222, test_acc: 0.1454\n",
            "Epoch 1070, BestLoss: 0.16344990992528913, Temperature 0.02027654715358363, step_size 0.9515124714685991, test_acc: 0.1478\n",
            "Epoch 1071, BestLoss: 0.16344990992528913, Temperature 0.019262719795904448, step_size 0.9514173202214522, test_acc: 0.1666\n",
            "Epoch 1072, BestLoss: 0.19104426074072026, Temperature 0.02027654715358363, step_size 0.9515124714685991, test_acc: 0.1089\n",
            "Epoch 1073, BestLoss: 0.16242870668713152, Temperature 0.019262719795904448, step_size 0.9514173202214522, test_acc: 0.1519\n",
            "Epoch 1074, BestLoss: 0.16242870668713152, Temperature 0.018299583806109226, step_size 0.95132217848943, test_acc: 0.1495\n",
            "Epoch 1075, BestLoss: 0.16242870668713152, Temperature 0.019262719795904448, step_size 0.9514173202214522, test_acc: 0.1528\n",
            "Epoch 1076, BestLoss: 0.16242870668713152, Temperature 0.02027654715358363, step_size 0.95132217848943, test_acc: 0.1466\n",
            "Epoch 1077, BestLoss: 0.14884708355407886, Temperature 0.021343733845877507, step_size 0.9512270462715811, test_acc: 0.1291\n",
            "Epoch 1078, BestLoss: 0.14884708355407886, Temperature 0.02027654715358363, step_size 0.9511319235669539, test_acc: 0.1308\n",
            "Epoch 1079, BestLoss: 0.14884708355407886, Temperature 0.021343733845877507, step_size 0.9511319235669539, test_acc: 0.1353\n",
            "Epoch 1080, BestLoss: 0.14884708355407886, Temperature 0.022467088258818428, step_size 0.9510368103745972, test_acc: 0.1567\n",
            "Epoch 1081, BestLoss: 0.10126603717478408, Temperature 0.023649566588229927, step_size 0.9509417066935597, test_acc: 0.0561\n",
            "Epoch 1082, BestLoss: 0.10126603717478408, Temperature 0.022467088258818428, step_size 0.9508466125228904, test_acc: 0.0627\n",
            "Epoch 1083, BestLoss: 0.0959067298216002, Temperature 0.023649566588229927, step_size 0.9508466125228904, test_acc: 0.1297\n",
            "Epoch 1084, BestLoss: 0.09145654861593998, Temperature 0.022467088258818428, step_size 0.9507515278616381, test_acc: 0.112\n",
            "Epoch 1085, BestLoss: 0.09145654861593998, Temperature 0.021343733845877507, step_size 0.9506564527088519, test_acc: 0.1118\n",
            "Epoch 1086, BestLoss: 0.09145654861593998, Temperature 0.022467088258818428, step_size 0.9507515278616381, test_acc: 0.1176\n",
            "Epoch 1087, BestLoss: 0.09145654861593998, Temperature 0.023649566588229927, step_size 0.9506564527088519, test_acc: 0.1269\n",
            "Epoch 1088, BestLoss: 0.09145654861593998, Temperature 0.0248942806191894, step_size 0.9505613870635811, test_acc: 0.1384\n",
            "Epoch 1089, BestLoss: 0.09145654861593998, Temperature 0.02620450591493621, step_size 0.9504663309248748, test_acc: 0.1475\n",
            "Epoch 1090, BestLoss: 0.09145654861593998, Temperature 0.027583690436774957, step_size 0.9503712842917823, test_acc: 0.1636\n",
            "Epoch 1091, BestLoss: 0.09145654861593998, Temperature 0.029035463617657853, step_size 0.9502762471633531, test_acc: 0.1893\n",
            "Epoch 1092, BestLoss: 0.09145654861593998, Temperature 0.030563645913324056, step_size 0.9501812195386368, test_acc: 0.2283\n",
            "Epoch 1093, BestLoss: 0.09145654861593998, Temperature 0.032172258856130585, step_size 0.9500862014166829, test_acc: 0.3077\n",
            "Epoch 1094, BestLoss: 0.09288567449862964, Temperature 0.0338655356380322, step_size 0.9499911927965412, test_acc: 0.1113\n",
            "Epoch 1095, BestLoss: 0.09288567449862964, Temperature 0.032172258856130585, step_size 0.9498961936772615, test_acc: 0.1155\n",
            "Epoch 1096, BestLoss: 0.09288567449862964, Temperature 0.0338655356380322, step_size 0.9498961936772615, test_acc: 0.1281\n",
            "Epoch 1097, BestLoss: 0.19750143463892808, Temperature 0.03564793225056021, step_size 0.9498012040578938, test_acc: 0.1245\n",
            "Epoch 1098, BestLoss: 0.2113389369545419, Temperature 0.0338655356380322, step_size 0.949706223937488, test_acc: 0.0669\n",
            "Epoch 1099, BestLoss: 0.1863816022284789, Temperature 0.032172258856130585, step_size 0.9496112533150943, test_acc: 0.147\n",
            "Epoch 1100, BestLoss: 0.1863816022284789, Temperature 0.030563645913324056, step_size 0.9495162921897629, test_acc: 0.1451\n",
            "Epoch 1101, BestLoss: 0.20109775932192972, Temperature 0.032172258856130585, step_size 0.949706223937488, test_acc: 0.1648\n",
            "Epoch 1102, BestLoss: 0.20025245857552693, Temperature 0.030563645913324056, step_size 0.9496112533150943, test_acc: 0.243\n",
            "Epoch 1103, BestLoss: 0.1769774967181974, Temperature 0.029035463617657853, step_size 0.9495162921897629, test_acc: 0.1862\n",
            "Epoch 1104, BestLoss: 0.14534131550463927, Temperature 0.027583690436774957, step_size 0.9494213405605438, test_acc: 0.1615\n",
            "Epoch 1105, BestLoss: 0.14534131550463927, Temperature 0.02620450591493621, step_size 0.9493263984264878, test_acc: 0.1615\n",
            "Epoch 1106, BestLoss: 0.14647446325245908, Temperature 0.027583690436774957, step_size 0.9496112533150943, test_acc: 0.1122\n",
            "Epoch 1107, BestLoss: 0.14647446325245908, Temperature 0.02620450591493621, step_size 0.9495162921897629, test_acc: 0.1087\n",
            "Epoch 1108, BestLoss: 0.14647446325245908, Temperature 0.027583690436774957, step_size 0.9495162921897629, test_acc: 0.1075\n",
            "Epoch 1109, BestLoss: 0.14647446325245908, Temperature 0.029035463617657853, step_size 0.9494213405605438, test_acc: 0.1119\n",
            "Epoch 1110, BestLoss: 0.15900049063514926, Temperature 0.030563645913324056, step_size 0.9493263984264878, test_acc: 0.1903\n",
            "Epoch 1111, BestLoss: 0.18016290199191573, Temperature 0.029035463617657853, step_size 0.9492314657866452, test_acc: 0.1338\n",
            "Epoch 1112, BestLoss: 0.18016290199191573, Temperature 0.027583690436774957, step_size 0.9491365426400665, test_acc: 0.1385\n",
            "Epoch 1113, BestLoss: 0.18016290199191573, Temperature 0.029035463617657853, step_size 0.9492314657866452, test_acc: 0.1451\n",
            "Epoch 1114, BestLoss: 0.18016290199191573, Temperature 0.030563645913324056, step_size 0.9491365426400665, test_acc: 0.1474\n",
            "Epoch 1115, BestLoss: 0.17431203743018533, Temperature 0.032172258856130585, step_size 0.9490416289858026, test_acc: 0.0925\n",
            "Epoch 1116, BestLoss: 0.17431203743018533, Temperature 0.030563645913324056, step_size 0.948946724822904, test_acc: 0.0959\n",
            "Epoch 1117, BestLoss: 0.13141086566052904, Temperature 0.032172258856130585, step_size 0.948946724822904, test_acc: 0.0894\n",
            "Epoch 1118, BestLoss: 0.14436289098403784, Temperature 0.030563645913324056, step_size 0.9488518301504217, test_acc: 0.1316\n",
            "Epoch 1119, BestLoss: 0.16883585896987754, Temperature 0.029035463617657853, step_size 0.9487569449674067, test_acc: 0.0846\n",
            "Epoch 1120, BestLoss: 0.1810458811576731, Temperature 0.027583690436774957, step_size 0.94866206927291, test_acc: 0.0805\n",
            "Epoch 1121, BestLoss: 0.15581927468603612, Temperature 0.02620450591493621, step_size 0.9485672030659826, test_acc: 0.0927\n",
            "Epoch 1122, BestLoss: 0.1919037240672265, Temperature 0.0248942806191894, step_size 0.948472346345676, test_acc: 0.0892\n",
            "Epoch 1123, BestLoss: 0.1919037240672265, Temperature 0.023649566588229927, step_size 0.9483774991110414, test_acc: 0.0849\n",
            "Epoch 1124, BestLoss: 0.13491854468242806, Temperature 0.0248942806191894, step_size 0.9488518301504217, test_acc: 0.076\n",
            "Epoch 1125, BestLoss: 0.11319760435631553, Temperature 0.023649566588229927, step_size 0.9487569449674067, test_acc: 0.0427\n",
            "Epoch 1126, BestLoss: 0.11319760435631553, Temperature 0.022467088258818428, step_size 0.94866206927291, test_acc: 0.0422\n",
            "Epoch 1127, BestLoss: 0.11319760435631553, Temperature 0.023649566588229927, step_size 0.9487569449674067, test_acc: 0.048\n",
            "Epoch 1128, BestLoss: 0.11319760435631553, Temperature 0.0248942806191894, step_size 0.94866206927291, test_acc: 0.0648\n",
            "Epoch 1129, BestLoss: 0.11319760435631553, Temperature 0.02620450591493621, step_size 0.9485672030659826, test_acc: 0.0967\n",
            "Epoch 1130, BestLoss: 0.10452327435492191, Temperature 0.027583690436774957, step_size 0.948472346345676, test_acc: 0.1018\n",
            "Epoch 1131, BestLoss: 0.10452327435492191, Temperature 0.02620450591493621, step_size 0.9483774991110414, test_acc: 0.1048\n",
            "Epoch 1132, BestLoss: 0.10452327435492191, Temperature 0.027583690436774957, step_size 0.9483774991110414, test_acc: 0.106\n",
            "Epoch 1133, BestLoss: 0.11565602079404183, Temperature 0.029035463617657853, step_size 0.9482826613611303, test_acc: 0.0863\n",
            "Epoch 1134, BestLoss: 0.11565602079404183, Temperature 0.027583690436774957, step_size 0.9481878330949942, test_acc: 0.0869\n",
            "Epoch 1135, BestLoss: 0.10590108819608648, Temperature 0.029035463617657853, step_size 0.9481878330949942, test_acc: 0.0579\n",
            "Epoch 1136, BestLoss: 0.10590108819608648, Temperature 0.027583690436774957, step_size 0.9480930143116847, test_acc: 0.0634\n",
            "Epoch 1137, BestLoss: 0.09322066175882109, Temperature 0.029035463617657853, step_size 0.9480930143116847, test_acc: 0.085\n",
            "Epoch 1138, BestLoss: 0.09322066175882109, Temperature 0.027583690436774957, step_size 0.9479982050102536, test_acc: 0.0927\n",
            "Epoch 1139, BestLoss: 0.09322066175882109, Temperature 0.029035463617657853, step_size 0.9479982050102536, test_acc: 0.0961\n",
            "Epoch 1140, BestLoss: 0.09322066175882109, Temperature 0.030563645913324056, step_size 0.9479034051897526, test_acc: 0.1108\n",
            "Epoch 1141, BestLoss: 0.09322066175882109, Temperature 0.032172258856130585, step_size 0.9478086148492336, test_acc: 0.1306\n",
            "Epoch 1142, BestLoss: 0.09322066175882109, Temperature 0.0338655356380322, step_size 0.9477138339877487, test_acc: 0.1469\n",
            "Epoch 1143, BestLoss: 0.09322066175882109, Temperature 0.03564793225056021, step_size 0.9476190626043499, test_acc: 0.1736\n",
            "Epoch 1144, BestLoss: 0.1732972047779937, Temperature 0.03752413921111601, step_size 0.9475243006980895, test_acc: 0.0911\n",
            "Epoch 1145, BestLoss: 0.1732972047779937, Temperature 0.03564793225056021, step_size 0.9474295482680196, test_acc: 0.097\n",
            "Epoch 1146, BestLoss: 0.22689729705152345, Temperature 0.03752413921111601, step_size 0.9474295482680196, test_acc: 0.1158\n",
            "Epoch 1147, BestLoss: 0.22689729705152345, Temperature 0.03564793225056021, step_size 0.9473348053131928, test_acc: 0.1133\n",
            "Epoch 1148, BestLoss: 0.20789565713386415, Temperature 0.03752413921111601, step_size 0.9473348053131928, test_acc: 0.1325\n",
            "Epoch 1149, BestLoss: 0.21411158414498233, Temperature 0.03564793225056021, step_size 0.9472400718326615, test_acc: 0.1625\n",
            "Epoch 1150, BestLoss: 0.207765493435622, Temperature 0.0338655356380322, step_size 0.9471453478254782, test_acc: 0.1106\n",
            "Epoch 1151, BestLoss: 0.19875411060668757, Temperature 0.032172258856130585, step_size 0.9470506332906957, test_acc: 0.1384\n",
            "Epoch 1152, BestLoss: 0.1598072406643246, Temperature 0.030563645913324056, step_size 0.9469559282273666, test_acc: 0.0683\n",
            "Epoch 1153, BestLoss: 0.1615164329418289, Temperature 0.029035463617657853, step_size 0.9468612326345439, test_acc: 0.0683\n",
            "Epoch 1154, BestLoss: 0.1832493212070126, Temperature 0.027583690436774957, step_size 0.9467665465112804, test_acc: 0.1107\n",
            "Epoch 1155, BestLoss: 0.1832493212070126, Temperature 0.02620450591493621, step_size 0.9466718698566293, test_acc: 0.1088\n",
            "Epoch 1156, BestLoss: 0.2006074174222867, Temperature 0.027583690436774957, step_size 0.9472400718326615, test_acc: 0.0822\n",
            "Epoch 1157, BestLoss: 0.11271712015756573, Temperature 0.02620450591493621, step_size 0.9471453478254782, test_acc: 0.078\n",
            "Epoch 1158, BestLoss: 0.11271712015756573, Temperature 0.0248942806191894, step_size 0.9470506332906957, test_acc: 0.0821\n",
            "Epoch 1159, BestLoss: 0.1301469224776117, Temperature 0.02620450591493621, step_size 0.9471453478254782, test_acc: 0.061\n",
            "Epoch 1160, BestLoss: 0.12805413506325825, Temperature 0.0248942806191894, step_size 0.9470506332906957, test_acc: 0.1438\n",
            "Epoch 1161, BestLoss: 0.12436451323299828, Temperature 0.023649566588229927, step_size 0.9469559282273666, test_acc: 0.1232\n",
            "Epoch 1162, BestLoss: 0.12436451323299828, Temperature 0.022467088258818428, step_size 0.9468612326345439, test_acc: 0.1287\n",
            "Epoch 1163, BestLoss: 0.12436451323299828, Temperature 0.023649566588229927, step_size 0.9470506332906957, test_acc: 0.1368\n",
            "Epoch 1164, BestLoss: 0.1191104691918242, Temperature 0.0248942806191894, step_size 0.9469559282273666, test_acc: 0.114\n",
            "Epoch 1165, BestLoss: 0.1191104691918242, Temperature 0.023649566588229927, step_size 0.9468612326345439, test_acc: 0.1121\n",
            "Epoch 1166, BestLoss: 0.1191104691918242, Temperature 0.0248942806191894, step_size 0.9468612326345439, test_acc: 0.117\n",
            "Epoch 1167, BestLoss: 0.1191104691918242, Temperature 0.02620450591493621, step_size 0.9467665465112804, test_acc: 0.1203\n",
            "Epoch 1168, BestLoss: 0.1191104691918242, Temperature 0.027583690436774957, step_size 0.9466718698566293, test_acc: 0.1161\n",
            "Epoch 1169, BestLoss: 0.12844391711365552, Temperature 0.029035463617657853, step_size 0.9465772026696436, test_acc: 0.1226\n",
            "Epoch 1170, BestLoss: 0.13565517003755664, Temperature 0.027583690436774957, step_size 0.9464825449493767, test_acc: 0.0915\n",
            "Epoch 1171, BestLoss: 0.13565517003755664, Temperature 0.02620450591493621, step_size 0.9463878966948818, test_acc: 0.0919\n",
            "Epoch 1172, BestLoss: 0.13565517003755664, Temperature 0.027583690436774957, step_size 0.9464825449493767, test_acc: 0.0927\n",
            "Epoch 1173, BestLoss: 0.13565517003755664, Temperature 0.029035463617657853, step_size 0.9463878966948818, test_acc: 0.0926\n",
            "Epoch 1174, BestLoss: 0.13565517003755664, Temperature 0.030563645913324056, step_size 0.9462932579052122, test_acc: 0.0958\n",
            "Epoch 1175, BestLoss: 0.15410479726909035, Temperature 0.032172258856130585, step_size 0.9461986285794217, test_acc: 0.0976\n",
            "Epoch 1176, BestLoss: 0.15410479726909035, Temperature 0.030563645913324056, step_size 0.9461040087165637, test_acc: 0.0906\n",
            "Epoch 1177, BestLoss: 0.15410479726909035, Temperature 0.032172258856130585, step_size 0.9461040087165637, test_acc: 0.0924\n",
            "Epoch 1178, BestLoss: 0.14662516740259748, Temperature 0.0338655356380322, step_size 0.9460093983156921, test_acc: 0.0749\n",
            "Epoch 1179, BestLoss: 0.13765947945508342, Temperature 0.032172258856130585, step_size 0.9459147973758605, test_acc: 0.0978\n",
            "Epoch 1180, BestLoss: 0.13765947945508342, Temperature 0.030563645913324056, step_size 0.9458202058961229, test_acc: 0.1066\n",
            "Epoch 1181, BestLoss: 0.15483414108294463, Temperature 0.032172258856130585, step_size 0.9459147973758605, test_acc: 0.061\n",
            "Epoch 1182, BestLoss: 0.15483414108294463, Temperature 0.030563645913324056, step_size 0.9458202058961229, test_acc: 0.0587\n",
            "Epoch 1183, BestLoss: 0.15239493857607372, Temperature 0.032172258856130585, step_size 0.9458202058961229, test_acc: 0.0546\n",
            "Epoch 1184, BestLoss: 0.15239493857607372, Temperature 0.030563645913324056, step_size 0.9457256238755333, test_acc: 0.0566\n",
            "Epoch 1185, BestLoss: 0.17604918394447547, Temperature 0.032172258856130585, step_size 0.9457256238755333, test_acc: 0.0444\n",
            "Epoch 1186, BestLoss: 0.20425111796587483, Temperature 0.030563645913324056, step_size 0.9456310513131457, test_acc: 0.083\n",
            "Epoch 1187, BestLoss: 0.20425111796587483, Temperature 0.029035463617657853, step_size 0.9455364882080144, test_acc: 0.077\n",
            "Epoch 1188, BestLoss: 0.20425111796587483, Temperature 0.030563645913324056, step_size 0.9456310513131457, test_acc: 0.0702\n",
            "Epoch 1189, BestLoss: 0.19232946501469886, Temperature 0.032172258856130585, step_size 0.9455364882080144, test_acc: 0.0854\n",
            "Epoch 1190, BestLoss: 0.13818753968182737, Temperature 0.030563645913324056, step_size 0.9454419345591936, test_acc: 0.1166\n",
            "Epoch 1191, BestLoss: 0.13818753968182737, Temperature 0.029035463617657853, step_size 0.9453473903657377, test_acc: 0.1162\n",
            "Epoch 1192, BestLoss: 0.13818753968182737, Temperature 0.030563645913324056, step_size 0.9454419345591936, test_acc: 0.1146\n",
            "Epoch 1193, BestLoss: 0.16963072050765654, Temperature 0.032172258856130585, step_size 0.9453473903657377, test_acc: 0.0412\n",
            "Epoch 1194, BestLoss: 0.141008178514245, Temperature 0.030563645913324056, step_size 0.9452528556267011, test_acc: 0.0442\n",
            "Epoch 1195, BestLoss: 0.141008178514245, Temperature 0.029035463617657853, step_size 0.9451583303411385, test_acc: 0.0527\n",
            "Epoch 1196, BestLoss: 0.141008178514245, Temperature 0.030563645913324056, step_size 0.9452528556267011, test_acc: 0.0639\n",
            "Epoch 1197, BestLoss: 0.1462631310330775, Temperature 0.032172258856130585, step_size 0.9451583303411385, test_acc: 0.0491\n",
            "Epoch 1198, BestLoss: 0.11638777756153815, Temperature 0.030563645913324056, step_size 0.9450638145081044, test_acc: 0.1057\n",
            "Epoch 1199, BestLoss: 0.11001863748785326, Temperature 0.029035463617657853, step_size 0.9449693081266536, test_acc: 0.1033\n",
            "Epoch 1200, BestLoss: 0.11001863748785326, Temperature 0.027583690436774957, step_size 0.9448748111958409, test_acc: 0.0998\n",
            "Epoch 1201, BestLoss: 0.11001863748785326, Temperature 0.029035463617657853, step_size 0.9450638145081044, test_acc: 0.0931\n",
            "Epoch 1202, BestLoss: 0.10990587341522694, Temperature 0.030563645913324056, step_size 0.9449693081266536, test_acc: 0.1193\n",
            "Epoch 1203, BestLoss: 0.13012630947161083, Temperature 0.029035463617657853, step_size 0.9448748111958409, test_acc: 0.1142\n",
            "Epoch 1204, BestLoss: 0.13012630947161083, Temperature 0.027583690436774957, step_size 0.9447803237147213, test_acc: 0.1165\n",
            "Epoch 1205, BestLoss: 0.12279856955127315, Temperature 0.029035463617657853, step_size 0.9448748111958409, test_acc: 0.086\n",
            "Epoch 1206, BestLoss: 0.12279856955127315, Temperature 0.027583690436774957, step_size 0.9447803237147213, test_acc: 0.0815\n",
            "Epoch 1207, BestLoss: 0.0922986376288066, Temperature 0.029035463617657853, step_size 0.9447803237147213, test_acc: 0.1457\n",
            "Epoch 1208, BestLoss: 0.0922986376288066, Temperature 0.027583690436774957, step_size 0.9446858456823498, test_acc: 0.1522\n",
            "Epoch 1209, BestLoss: 0.0922986376288066, Temperature 0.029035463617657853, step_size 0.9446858456823498, test_acc: 0.1635\n",
            "Epoch 1210, BestLoss: 0.0922986376288066, Temperature 0.030563645913324056, step_size 0.9445913770977816, test_acc: 0.1842\n",
            "Epoch 1211, BestLoss: 0.0922986376288066, Temperature 0.032172258856130585, step_size 0.9444969179600718, test_acc: 0.2094\n",
            "Epoch 1212, BestLoss: 0.0922986376288066, Temperature 0.0338655356380322, step_size 0.9444024682682759, test_acc: 0.2267\n",
            "Epoch 1213, BestLoss: 0.0922986376288066, Temperature 0.03564793225056021, step_size 0.9443080280214491, test_acc: 0.2515\n",
            "Epoch 1214, BestLoss: 0.0922986376288066, Temperature 0.03752413921111601, step_size 0.944213597218647, test_acc: 0.2878\n",
            "Epoch 1215, BestLoss: 0.0922986376288066, Temperature 0.03949909390643791, step_size 0.9441191758589251, test_acc: 0.3469\n",
            "Epoch 1216, BestLoss: 0.0922986376288066, Temperature 0.041577993585724116, step_size 0.9440247639413393, test_acc: 0.4315\n",
            "Epoch 1217, BestLoss: 0.0922986376288066, Temperature 0.04376630903760433, step_size 0.9439303614649451, test_acc: 0.5398\n",
            "Epoch 1218, BestLoss: 0.0922986376288066, Temperature 0.04606979898695193, step_size 0.9438359684287987, test_acc: 0.6472\n",
            "Epoch 1219, BestLoss: 0.0922986376288066, Temperature 0.04849452524942309, step_size 0.9437415848319558, test_acc: 0.7387\n",
            "Epoch 1220, BestLoss: 0.0922986376288066, Temperature 0.05104686868360325, step_size 0.9436472106734726, test_acc: 0.7917\n",
            "Epoch 1221, BestLoss: 0.0922986376288066, Temperature 0.05373354598274027, step_size 0.9435528459524052, test_acc: 0.8245\n",
            "Epoch 1222, BestLoss: 0.17790117251838947, Temperature 0.05656162735025292, step_size 0.94345849066781, test_acc: 0.1436\n",
            "Epoch 1223, BestLoss: 0.1799816146074515, Temperature 0.05373354598274027, step_size 0.9433641448187432, test_acc: 0.1874\n",
            "Epoch 1224, BestLoss: 0.16981895355220325, Temperature 0.05104686868360325, step_size 0.9432698084042613, test_acc: 0.1187\n",
            "Epoch 1225, BestLoss: 0.16763661806612012, Temperature 0.04849452524942309, step_size 0.9431754814234209, test_acc: 0.0938\n",
            "Epoch 1226, BestLoss: 0.15607749276645158, Temperature 0.04606979898695193, step_size 0.9430811638752785, test_acc: 0.0765\n",
            "Epoch 1227, BestLoss: 0.11728708206030065, Temperature 0.04376630903760433, step_size 0.942986855758891, test_acc: 0.1217\n",
            "Epoch 1228, BestLoss: 0.11728708206030065, Temperature 0.041577993585724116, step_size 0.9428925570733151, test_acc: 0.1307\n",
            "Epoch 1229, BestLoss: 0.18426148874706982, Temperature 0.04376630903760433, step_size 0.9433641448187432, test_acc: 0.0595\n",
            "Epoch 1230, BestLoss: 0.19222085070526235, Temperature 0.041577993585724116, step_size 0.9432698084042613, test_acc: 0.0337\n",
            "Epoch 1231, BestLoss: 0.24744219217298902, Temperature 0.03949909390643791, step_size 0.9431754814234209, test_acc: 0.096\n",
            "Epoch 1232, BestLoss: 0.19364193084217657, Temperature 0.03752413921111601, step_size 0.9430811638752785, test_acc: 0.1122\n",
            "Epoch 1233, BestLoss: 0.19364193084217657, Temperature 0.03564793225056021, step_size 0.942986855758891, test_acc: 0.1027\n",
            "Epoch 1234, BestLoss: 0.19364193084217657, Temperature 0.03752413921111601, step_size 0.9432698084042613, test_acc: 0.0871\n",
            "Epoch 1235, BestLoss: 0.2627674098285313, Temperature 0.03949909390643791, step_size 0.9431754814234209, test_acc: 0.0805\n",
            "Epoch 1236, BestLoss: 0.18493032362974976, Temperature 0.03752413921111601, step_size 0.9430811638752785, test_acc: 0.132\n",
            "Epoch 1237, BestLoss: 0.18493032362974976, Temperature 0.03564793225056021, step_size 0.942986855758891, test_acc: 0.1237\n",
            "Epoch 1238, BestLoss: 0.22793270664241475, Temperature 0.03752413921111601, step_size 0.9430811638752785, test_acc: 0.1458\n",
            "Epoch 1239, BestLoss: 0.2802236937057169, Temperature 0.03564793225056021, step_size 0.942986855758891, test_acc: 0.1344\n",
            "Epoch 1240, BestLoss: 0.26010794971106965, Temperature 0.0338655356380322, step_size 0.9428925570733151, test_acc: 0.1281\n",
            "Epoch 1241, BestLoss: 0.24875958426436365, Temperature 0.032172258856130585, step_size 0.9427982678176078, test_acc: 0.1182\n",
            "Epoch 1242, BestLoss: 0.24875958426436365, Temperature 0.030563645913324056, step_size 0.942703987990826, test_acc: 0.1184\n",
            "Epoch 1243, BestLoss: 0.24875958426436365, Temperature 0.032172258856130585, step_size 0.942986855758891, test_acc: 0.1212\n",
            "Epoch 1244, BestLoss: 0.19874777565969082, Temperature 0.0338655356380322, step_size 0.9428925570733151, test_acc: 0.139\n",
            "Epoch 1245, BestLoss: 0.16771052999088532, Temperature 0.032172258856130585, step_size 0.9427982678176078, test_acc: 0.1487\n",
            "Epoch 1246, BestLoss: 0.16771052999088532, Temperature 0.030563645913324056, step_size 0.942703987990826, test_acc: 0.1467\n",
            "Epoch 1247, BestLoss: 0.13471848495809163, Temperature 0.032172258856130585, step_size 0.9427982678176078, test_acc: 0.0718\n",
            "Epoch 1248, BestLoss: 0.1557071727654378, Temperature 0.030563645913324056, step_size 0.942703987990826, test_acc: 0.1035\n",
            "Epoch 1249, BestLoss: 0.1943019002143429, Temperature 0.029035463617657853, step_size 0.9426097175920269, test_acc: 0.1015\n",
            "Epoch 1250, BestLoss: 0.20939317816207043, Temperature 0.027583690436774957, step_size 0.9425154566202677, test_acc: 0.1326\n",
            "Epoch 1251, BestLoss: 0.16288702434233018, Temperature 0.02620450591493621, step_size 0.9424212050746057, test_acc: 0.1352\n",
            "Epoch 1252, BestLoss: 0.16288702434233018, Temperature 0.0248942806191894, step_size 0.9423269629540982, test_acc: 0.1269\n",
            "Epoch 1253, BestLoss: 0.1877395186340313, Temperature 0.02620450591493621, step_size 0.942703987990826, test_acc: 0.1668\n",
            "Epoch 1254, BestLoss: 0.17752956622575963, Temperature 0.0248942806191894, step_size 0.9426097175920269, test_acc: 0.2031\n",
            "Epoch 1255, BestLoss: 0.17752956622575963, Temperature 0.023649566588229927, step_size 0.9425154566202677, test_acc: 0.2001\n",
            "Epoch 1256, BestLoss: 0.17752956622575963, Temperature 0.0248942806191894, step_size 0.9426097175920269, test_acc: 0.1909\n",
            "Epoch 1257, BestLoss: 0.1503381435704945, Temperature 0.02620450591493621, step_size 0.9425154566202677, test_acc: 0.1029\n",
            "Epoch 1258, BestLoss: 0.1503381435704945, Temperature 0.0248942806191894, step_size 0.9424212050746057, test_acc: 0.1096\n",
            "Epoch 1259, BestLoss: 0.1711392888560921, Temperature 0.02620450591493621, step_size 0.9424212050746057, test_acc: 0.0893\n",
            "Epoch 1260, BestLoss: 0.18199275100046156, Temperature 0.0248942806191894, step_size 0.9423269629540982, test_acc: 0.0752\n",
            "Epoch 1261, BestLoss: 0.18199275100046156, Temperature 0.023649566588229927, step_size 0.9422327302578029, test_acc: 0.0752\n",
            "Epoch 1262, BestLoss: 0.18199275100046156, Temperature 0.0248942806191894, step_size 0.9423269629540982, test_acc: 0.0736\n",
            "Epoch 1263, BestLoss: 0.18199275100046156, Temperature 0.02620450591493621, step_size 0.9422327302578029, test_acc: 0.0726\n",
            "Epoch 1264, BestLoss: 0.20339933629338552, Temperature 0.027583690436774957, step_size 0.9421385069847771, test_acc: 0.2226\n",
            "Epoch 1265, BestLoss: 0.20339933629338552, Temperature 0.02620450591493621, step_size 0.9420442931340786, test_acc: 0.226\n",
            "Epoch 1266, BestLoss: 0.19698489604897496, Temperature 0.027583690436774957, step_size 0.9420442931340786, test_acc: 0.1041\n",
            "Epoch 1267, BestLoss: 0.19698489604897496, Temperature 0.02620450591493621, step_size 0.9419500887047652, test_acc: 0.1081\n",
            "Epoch 1268, BestLoss: 0.20152462931157075, Temperature 0.027583690436774957, step_size 0.9419500887047652, test_acc: 0.1684\n",
            "Epoch 1269, BestLoss: 0.20098762026043826, Temperature 0.02620450591493621, step_size 0.9418558936958947, test_acc: 0.1349\n",
            "Epoch 1270, BestLoss: 0.21290663003896226, Temperature 0.0248942806191894, step_size 0.9417617081065252, test_acc: 0.1471\n",
            "Epoch 1271, BestLoss: 0.21290663003896226, Temperature 0.023649566588229927, step_size 0.9416675319357145, test_acc: 0.1469\n",
            "Epoch 1272, BestLoss: 0.21290663003896226, Temperature 0.0248942806191894, step_size 0.9418558936958947, test_acc: 0.1504\n",
            "Epoch 1273, BestLoss: 0.20447208784691828, Temperature 0.02620450591493621, step_size 0.9417617081065252, test_acc: 0.1243\n",
            "Epoch 1274, BestLoss: 0.15889238156233954, Temperature 0.0248942806191894, step_size 0.9416675319357145, test_acc: 0.1226\n",
            "Epoch 1275, BestLoss: 0.16736830067023606, Temperature 0.023649566588229927, step_size 0.941573365182521, test_acc: 0.1139\n",
            "Epoch 1276, BestLoss: 0.14062831981488602, Temperature 0.022467088258818428, step_size 0.9414792078460027, test_acc: 0.1146\n",
            "Epoch 1277, BestLoss: 0.14062831981488602, Temperature 0.021343733845877507, step_size 0.9413850599252181, test_acc: 0.119\n",
            "Epoch 1278, BestLoss: 0.14528756588364308, Temperature 0.022467088258818428, step_size 0.9416675319357145, test_acc: 0.0803\n",
            "Epoch 1279, BestLoss: 0.14501641814041483, Temperature 0.021343733845877507, step_size 0.941573365182521, test_acc: 0.1303\n",
            "Epoch 1280, BestLoss: 0.13383746446971975, Temperature 0.02027654715358363, step_size 0.9414792078460027, test_acc: 0.1748\n",
            "Epoch 1281, BestLoss: 0.13767689677717784, Temperature 0.019262719795904448, step_size 0.9413850599252181, test_acc: 0.1426\n",
            "Epoch 1282, BestLoss: 0.13767689677717784, Temperature 0.018299583806109226, step_size 0.9412909214192257, test_acc: 0.138\n",
            "Epoch 1283, BestLoss: 0.13767689677717784, Temperature 0.019262719795904448, step_size 0.941573365182521, test_acc: 0.1356\n",
            "Epoch 1284, BestLoss: 0.13767689677717784, Temperature 0.02027654715358363, step_size 0.9414792078460027, test_acc: 0.1316\n",
            "Epoch 1285, BestLoss: 0.10653644392631599, Temperature 0.021343733845877507, step_size 0.9413850599252181, test_acc: 0.0872\n",
            "Epoch 1286, BestLoss: 0.10122393921246704, Temperature 0.02027654715358363, step_size 0.9412909214192257, test_acc: 0.079\n",
            "Epoch 1287, BestLoss: 0.09193484679825457, Temperature 0.019262719795904448, step_size 0.9411967923270838, test_acc: 0.1294\n",
            "Epoch 1288, BestLoss: 0.09193484679825457, Temperature 0.018299583806109226, step_size 0.9411026726478511, test_acc: 0.1273\n",
            "Epoch 1289, BestLoss: 0.09193484679825457, Temperature 0.019262719795904448, step_size 0.9412909214192257, test_acc: 0.1273\n",
            "Epoch 1290, BestLoss: 0.08111631763723004, Temperature 0.02027654715358363, step_size 0.9411967923270838, test_acc: 0.104\n",
            "Epoch 1291, BestLoss: 0.08111631763723004, Temperature 0.019262719795904448, step_size 0.9411026726478511, test_acc: 0.1095\n",
            "Epoch 1292, BestLoss: 0.08111631763723004, Temperature 0.02027654715358363, step_size 0.9411026726478511, test_acc: 0.111\n",
            "Epoch 1293, BestLoss: 0.08111631763723004, Temperature 0.021343733845877507, step_size 0.9410085623805863, test_acc: 0.1096\n",
            "Epoch 1294, BestLoss: 0.08111631763723004, Temperature 0.022467088258818428, step_size 0.9409144615243482, test_acc: 0.1075\n",
            "Epoch 1295, BestLoss: 0.08111631763723004, Temperature 0.023649566588229927, step_size 0.9408203700781957, test_acc: 0.1124\n",
            "Epoch 1296, BestLoss: 0.08111631763723004, Temperature 0.0248942806191894, step_size 0.940726288041188, test_acc: 0.1248\n",
            "Epoch 1297, BestLoss: 0.06804882448368832, Temperature 0.02620450591493621, step_size 0.9406322154123838, test_acc: 0.0851\n",
            "Epoch 1298, BestLoss: 0.06804882448368832, Temperature 0.0248942806191894, step_size 0.9405381521908426, test_acc: 0.0869\n",
            "Epoch 1299, BestLoss: 0.06804882448368832, Temperature 0.02620450591493621, step_size 0.9405381521908426, test_acc: 0.0866\n",
            "Epoch 1300, BestLoss: 0.06804882448368832, Temperature 0.027583690436774957, step_size 0.9404440983756235, test_acc: 0.0859\n",
            "Epoch 1301, BestLoss: 0.06804882448368832, Temperature 0.029035463617657853, step_size 0.940350053965786, test_acc: 0.0905\n",
            "Epoch 1302, BestLoss: 0.06804882448368832, Temperature 0.030563645913324056, step_size 0.9402560189603894, test_acc: 0.103\n",
            "Epoch 1303, BestLoss: 0.06804882448368832, Temperature 0.032172258856130585, step_size 0.9401619933584934, test_acc: 0.1297\n",
            "Epoch 1304, BestLoss: 0.06804882448368832, Temperature 0.0338655356380322, step_size 0.9400679771591576, test_acc: 0.1732\n",
            "Epoch 1305, BestLoss: 0.06804882448368832, Temperature 0.03564793225056021, step_size 0.9399739703614417, test_acc: 0.2575\n",
            "Epoch 1306, BestLoss: 0.06804882448368832, Temperature 0.03752413921111601, step_size 0.9398799729644056, test_acc: 0.4033\n",
            "Epoch 1307, BestLoss: 0.06804882448368832, Temperature 0.03949909390643791, step_size 0.9397859849671092, test_acc: 0.5589\n",
            "Epoch 1308, BestLoss: 0.06804882448368832, Temperature 0.041577993585724116, step_size 0.9396920063686125, test_acc: 0.7033\n",
            "Epoch 1309, BestLoss: 0.06804882448368832, Temperature 0.04376630903760433, step_size 0.9395980371679756, test_acc: 0.7957\n",
            "Epoch 1310, BestLoss: 0.06804882448368832, Temperature 0.04606979898695193, step_size 0.9395040773642589, test_acc: 0.835\n",
            "Epoch 1311, BestLoss: 0.06804882448368832, Temperature 0.04849452524942309, step_size 0.9394101269565225, test_acc: 0.8481\n",
            "Epoch 1312, BestLoss: 0.06804882448368832, Temperature 0.05104686868360325, step_size 0.9393161859438268, test_acc: 0.8532\n",
            "Epoch 1313, BestLoss: 0.14880401139037683, Temperature 0.05373354598274027, step_size 0.9392222543252324, test_acc: 0.1018\n",
            "Epoch 1314, BestLoss: 0.14880401139037683, Temperature 0.05104686868360325, step_size 0.9391283320998, test_acc: 0.1011\n",
            "Epoch 1315, BestLoss: 0.14880401139037683, Temperature 0.05373354598274027, step_size 0.9391283320998, test_acc: 0.1144\n",
            "Epoch 1316, BestLoss: 0.14880401139037683, Temperature 0.05656162735025292, step_size 0.93903441926659, test_acc: 0.131\n",
            "Epoch 1317, BestLoss: 0.14880401139037683, Temperature 0.05953855510552939, step_size 0.9389405158246634, test_acc: 0.1399\n",
            "Epoch 1318, BestLoss: 0.21585687510078036, Temperature 0.06267216326897831, step_size 0.938846621773081, test_acc: 0.1447\n",
            "Epoch 1319, BestLoss: 0.2702167447873298, Temperature 0.0595385551055294, step_size 0.9387527371109037, test_acc: 0.103\n",
            "Epoch 1320, BestLoss: 0.27707610956459794, Temperature 0.05656162735025293, step_size 0.9386588618371926, test_acc: 0.0928\n",
            "Epoch 1321, BestLoss: 0.25081596985068494, Temperature 0.05373354598274028, step_size 0.9385649959510088, test_acc: 0.1102\n",
            "Epoch 1322, BestLoss: 0.20220109389078397, Temperature 0.05104686868360326, step_size 0.9384711394514138, test_acc: 0.1229\n",
            "Epoch 1323, BestLoss: 0.20220109389078397, Temperature 0.0484945252494231, step_size 0.9383772923374687, test_acc: 0.1239\n",
            "Epoch 1324, BestLoss: 0.15909410520230263, Temperature 0.05104686868360326, step_size 0.9387527371109037, test_acc: 0.1068\n",
            "Epoch 1325, BestLoss: 0.18793810556418947, Temperature 0.0484945252494231, step_size 0.9386588618371926, test_acc: 0.1049\n",
            "Epoch 1326, BestLoss: 0.1743822760009257, Temperature 0.04606979898695194, step_size 0.9385649959510088, test_acc: 0.1041\n",
            "Epoch 1327, BestLoss: 0.1743822760009257, Temperature 0.04376630903760434, step_size 0.9384711394514138, test_acc: 0.1048\n",
            "Epoch 1328, BestLoss: 0.1743822760009257, Temperature 0.04606979898695194, step_size 0.9386588618371926, test_acc: 0.1066\n",
            "Epoch 1329, BestLoss: 0.1464151286132525, Temperature 0.0484945252494231, step_size 0.9385649959510088, test_acc: 0.1335\n",
            "Epoch 1330, BestLoss: 0.14794187178970603, Temperature 0.04606979898695194, step_size 0.9384711394514138, test_acc: 0.1336\n",
            "Epoch 1331, BestLoss: 0.14794187178970603, Temperature 0.04376630903760434, step_size 0.9383772923374687, test_acc: 0.1375\n",
            "Epoch 1332, BestLoss: 0.15464724515384287, Temperature 0.04606979898695194, step_size 0.9384711394514138, test_acc: 0.1054\n",
            "Epoch 1333, BestLoss: 0.15464724515384287, Temperature 0.04376630903760434, step_size 0.9383772923374687, test_acc: 0.1019\n",
            "Epoch 1334, BestLoss: 0.15823443251282332, Temperature 0.04606979898695194, step_size 0.9383772923374687, test_acc: 0.0931\n",
            "Epoch 1335, BestLoss: 0.15270197767261787, Temperature 0.04376630903760434, step_size 0.9382834546082349, test_acc: 0.1102\n",
            "Epoch 1336, BestLoss: 0.12358353663997419, Temperature 0.04157799358572412, step_size 0.9381896262627741, test_acc: 0.1359\n",
            "Epoch 1337, BestLoss: 0.1375834796362014, Temperature 0.03949909390643792, step_size 0.9380958073001479, test_acc: 0.09\n",
            "Epoch 1338, BestLoss: 0.14421115526515152, Temperature 0.03752413921111602, step_size 0.9380019977194178, test_acc: 0.1219\n",
            "Epoch 1339, BestLoss: 0.14421115526515152, Temperature 0.03564793225056022, step_size 0.9379081975196459, test_acc: 0.125\n",
            "Epoch 1340, BestLoss: 0.1607413830833695, Temperature 0.03752413921111602, step_size 0.9382834546082349, test_acc: 0.1225\n",
            "Epoch 1341, BestLoss: 0.14834721211505983, Temperature 0.03564793225056022, step_size 0.9381896262627741, test_acc: 0.1105\n",
            "Epoch 1342, BestLoss: 0.14834721211505983, Temperature 0.033865535638032206, step_size 0.9380958073001479, test_acc: 0.1082\n",
            "Epoch 1343, BestLoss: 0.11361184213577219, Temperature 0.03564793225056022, step_size 0.9381896262627741, test_acc: 0.1041\n",
            "Epoch 1344, BestLoss: 0.11361184213577219, Temperature 0.033865535638032206, step_size 0.9380958073001479, test_acc: 0.1072\n",
            "Epoch 1345, BestLoss: 0.11361184213577219, Temperature 0.03564793225056022, step_size 0.9380958073001479, test_acc: 0.112\n",
            "Epoch 1346, BestLoss: 0.15805518818122724, Temperature 0.03752413921111602, step_size 0.9380019977194178, test_acc: 0.0835\n",
            "Epoch 1347, BestLoss: 0.15805518818122724, Temperature 0.03564793225056022, step_size 0.9379081975196459, test_acc: 0.0787\n",
            "Epoch 1348, BestLoss: 0.15805518818122724, Temperature 0.03752413921111602, step_size 0.9379081975196459, test_acc: 0.0752\n",
            "Epoch 1349, BestLoss: 0.15805518818122724, Temperature 0.03949909390643792, step_size 0.937814406699894, test_acc: 0.0825\n",
            "Epoch 1350, BestLoss: 0.17090381050171913, Temperature 0.04157799358572412, step_size 0.9377206252592241, test_acc: 0.1392\n",
            "Epoch 1351, BestLoss: 0.12236733236840902, Temperature 0.03949909390643792, step_size 0.9376268531966981, test_acc: 0.1654\n",
            "Epoch 1352, BestLoss: 0.12236733236840902, Temperature 0.03752413921111602, step_size 0.9375330905113785, test_acc: 0.1651\n",
            "Epoch 1353, BestLoss: 0.1196233462255644, Temperature 0.03949909390643792, step_size 0.9376268531966981, test_acc: 0.1242\n",
            "Epoch 1354, BestLoss: 0.12445352830195391, Temperature 0.03752413921111602, step_size 0.9375330905113785, test_acc: 0.0944\n",
            "Epoch 1355, BestLoss: 0.12445352830195391, Temperature 0.03564793225056022, step_size 0.9374393372023273, test_acc: 0.0979\n",
            "Epoch 1356, BestLoss: 0.12646900839766026, Temperature 0.03752413921111602, step_size 0.9375330905113785, test_acc: 0.0427\n",
            "Epoch 1357, BestLoss: 0.12646900839766026, Temperature 0.03564793225056022, step_size 0.9374393372023273, test_acc: 0.0422\n",
            "Epoch 1358, BestLoss: 0.12646900839766026, Temperature 0.03752413921111602, step_size 0.9374393372023273, test_acc: 0.0535\n",
            "Epoch 1359, BestLoss: 0.11212713084423459, Temperature 0.03949909390643792, step_size 0.9373455932686071, test_acc: 0.1153\n",
            "Epoch 1360, BestLoss: 0.13736864300290916, Temperature 0.03752413921111602, step_size 0.9372518587092803, test_acc: 0.0846\n",
            "Epoch 1361, BestLoss: 0.13736864300290916, Temperature 0.03564793225056022, step_size 0.9371581335234094, test_acc: 0.086\n",
            "Epoch 1362, BestLoss: 0.1110163584199926, Temperature 0.03752413921111602, step_size 0.9372518587092803, test_acc: 0.0942\n",
            "Epoch 1363, BestLoss: 0.1110163584199926, Temperature 0.03564793225056022, step_size 0.9371581335234094, test_acc: 0.0933\n",
            "Epoch 1364, BestLoss: 0.1110163584199926, Temperature 0.03752413921111602, step_size 0.9371581335234094, test_acc: 0.0967\n",
            "Epoch 1365, BestLoss: 0.15901087692349586, Temperature 0.03949909390643792, step_size 0.937064417710057, test_acc: 0.0797\n",
            "Epoch 1366, BestLoss: 0.15901087692349586, Temperature 0.03752413921111602, step_size 0.936970711268286, test_acc: 0.0898\n",
            "Epoch 1367, BestLoss: 0.17580229734662567, Temperature 0.03949909390643792, step_size 0.936970711268286, test_acc: 0.1008\n",
            "Epoch 1368, BestLoss: 0.17580229734662567, Temperature 0.03752413921111602, step_size 0.9368770141971592, test_acc: 0.1059\n",
            "Epoch 1369, BestLoss: 0.17580229734662567, Temperature 0.03949909390643792, step_size 0.9368770141971592, test_acc: 0.1091\n",
            "Epoch 1370, BestLoss: 0.17581934830234475, Temperature 0.04157799358572412, step_size 0.9367833264957395, test_acc: 0.0834\n",
            "Epoch 1371, BestLoss: 0.13916318940900313, Temperature 0.04376630903760434, step_size 0.9366896481630899, test_acc: 0.0803\n",
            "Epoch 1372, BestLoss: 0.15743564557811826, Temperature 0.04157799358572412, step_size 0.9365959791982735, test_acc: 0.116\n",
            "Epoch 1373, BestLoss: 0.1645162601860451, Temperature 0.03949909390643792, step_size 0.9365023196003537, test_acc: 0.0962\n",
            "Epoch 1374, BestLoss: 0.22978487852278098, Temperature 0.03752413921111602, step_size 0.9364086693683936, test_acc: 0.1152\n",
            "Epoch 1375, BestLoss: 0.23625110506388577, Temperature 0.03564793225056022, step_size 0.9363150285014568, test_acc: 0.102\n",
            "Epoch 1376, BestLoss: 0.22024238071563523, Temperature 0.033865535638032206, step_size 0.9362213969986066, test_acc: 0.0534\n",
            "Epoch 1377, BestLoss: 0.23923407537095995, Temperature 0.03217225885613059, step_size 0.9361277748589067, test_acc: 0.0956\n",
            "Epoch 1378, BestLoss: 0.20145246857549404, Temperature 0.030563645913324063, step_size 0.9360341620814209, test_acc: 0.0993\n",
            "Epoch 1379, BestLoss: 0.18664000704152467, Temperature 0.02903546361765786, step_size 0.9359405586652128, test_acc: 0.1834\n",
            "Epoch 1380, BestLoss: 0.18664000704152467, Temperature 0.027583690436774964, step_size 0.9358469646093462, test_acc: 0.1809\n",
            "Epoch 1381, BestLoss: 0.2102072696098165, Temperature 0.02903546361765786, step_size 0.9365959791982735, test_acc: 0.1565\n",
            "Epoch 1382, BestLoss: 0.2102072696098165, Temperature 0.027583690436774964, step_size 0.9365023196003537, test_acc: 0.159\n",
            "Epoch 1383, BestLoss: 0.1879580762754901, Temperature 0.02903546361765786, step_size 0.9365023196003537, test_acc: 0.1371\n",
            "Epoch 1384, BestLoss: 0.1879580762754901, Temperature 0.027583690436774964, step_size 0.9364086693683936, test_acc: 0.1369\n",
            "Epoch 1385, BestLoss: 0.2404589300152497, Temperature 0.02903546361765786, step_size 0.9364086693683936, test_acc: 0.0868\n",
            "Epoch 1386, BestLoss: 0.26298331035661465, Temperature 0.027583690436774964, step_size 0.9363150285014568, test_acc: 0.1085\n",
            "Epoch 1387, BestLoss: 0.2796980154408404, Temperature 0.026204505914936217, step_size 0.9362213969986066, test_acc: 0.0734\n",
            "Epoch 1388, BestLoss: 0.22135985241461123, Temperature 0.024894280619189406, step_size 0.9361277748589067, test_acc: 0.0671\n",
            "Epoch 1389, BestLoss: 0.20064918726383812, Temperature 0.023649566588229934, step_size 0.9360341620814209, test_acc: 0.0757\n",
            "Epoch 1390, BestLoss: 0.15298400810374047, Temperature 0.022467088258818435, step_size 0.9359405586652128, test_acc: 0.0934\n",
            "Epoch 1391, BestLoss: 0.15298400810374047, Temperature 0.021343733845877514, step_size 0.9358469646093462, test_acc: 0.0911\n",
            "Epoch 1392, BestLoss: 0.16048614673684017, Temperature 0.02246708825881844, step_size 0.9363150285014568, test_acc: 0.0793\n",
            "Epoch 1393, BestLoss: 0.16795756473178713, Temperature 0.021343733845877514, step_size 0.9362213969986066, test_acc: 0.1277\n",
            "Epoch 1394, BestLoss: 0.14189339910651697, Temperature 0.020276547153583638, step_size 0.9361277748589067, test_acc: 0.1117\n",
            "Epoch 1395, BestLoss: 0.14189339910651697, Temperature 0.019262719795904455, step_size 0.9360341620814209, test_acc: 0.1151\n",
            "Epoch 1396, BestLoss: 0.14773736932304699, Temperature 0.020276547153583638, step_size 0.9362213969986066, test_acc: 0.095\n",
            "Epoch 1397, BestLoss: 0.1428914864572855, Temperature 0.019262719795904455, step_size 0.9361277748589067, test_acc: 0.1513\n",
            "Epoch 1398, BestLoss: 0.1428914864572855, Temperature 0.018299583806109233, step_size 0.9360341620814209, test_acc: 0.1556\n",
            "Epoch 1399, BestLoss: 0.1428914864572855, Temperature 0.019262719795904458, step_size 0.9361277748589067, test_acc: 0.1508\n",
            "Epoch 1400, BestLoss: 0.1428914864572855, Temperature 0.02027654715358364, step_size 0.9360341620814209, test_acc: 0.1514\n",
            "Epoch 1401, BestLoss: 0.1428914864572855, Temperature 0.021343733845877517, step_size 0.9359405586652128, test_acc: 0.1506\n",
            "Epoch 1402, BestLoss: 0.1428914864572855, Temperature 0.022467088258818442, step_size 0.9358469646093462, test_acc: 0.1571\n",
            "Epoch 1403, BestLoss: 0.1428914864572855, Temperature 0.02364956658822994, step_size 0.9357533799128853, test_acc: 0.1718\n",
            "Epoch 1404, BestLoss: 0.1428914864572855, Temperature 0.024894280619189413, step_size 0.9356598045748941, test_acc: 0.1995\n",
            "Epoch 1405, BestLoss: 0.1428914864572855, Temperature 0.026204505914936227, step_size 0.9355662385944366, test_acc: 0.2493\n",
            "Epoch 1406, BestLoss: 0.1428914864572855, Temperature 0.02758369043677498, step_size 0.9354726819705771, test_acc: 0.3365\n",
            "Epoch 1407, BestLoss: 0.18650074325492405, Temperature 0.029035463617657874, step_size 0.93537913470238, test_acc: 0.1361\n",
            "Epoch 1408, BestLoss: 0.18650074325492405, Temperature 0.02758369043677498, step_size 0.9352855967889098, test_acc: 0.1271\n",
            "Epoch 1409, BestLoss: 0.18049221809133112, Temperature 0.029035463617657874, step_size 0.9352855967889098, test_acc: 0.1116\n",
            "Epoch 1410, BestLoss: 0.11395859615144863, Temperature 0.02758369043677498, step_size 0.9351920682292308, test_acc: 0.107\n",
            "Epoch 1411, BestLoss: 0.11395859615144863, Temperature 0.026204505914936227, step_size 0.935098549022408, test_acc: 0.116\n",
            "Epoch 1412, BestLoss: 0.11395859615144863, Temperature 0.02758369043677498, step_size 0.9351920682292308, test_acc: 0.1338\n",
            "Epoch 1413, BestLoss: 0.11395859615144863, Temperature 0.029035463617657874, step_size 0.935098549022408, test_acc: 0.1502\n",
            "Epoch 1414, BestLoss: 0.11395859615144863, Temperature 0.03056364591332408, step_size 0.9350050391675058, test_acc: 0.156\n",
            "Epoch 1415, BestLoss: 0.11395859615144863, Temperature 0.03217225885613061, step_size 0.934911538663589, test_acc: 0.1574\n",
            "Epoch 1416, BestLoss: 0.11395859615144863, Temperature 0.033865535638032226, step_size 0.9348180475097226, test_acc: 0.168\n",
            "Epoch 1417, BestLoss: 0.11395859615144863, Temperature 0.03564793225056024, step_size 0.9347245657049716, test_acc: 0.1879\n",
            "Epoch 1418, BestLoss: 0.14352026485269526, Temperature 0.037524139211116046, step_size 0.9346310932484012, test_acc: 0.1286\n",
            "Epoch 1419, BestLoss: 0.22986027409681847, Temperature 0.03564793225056024, step_size 0.9345376301390764, test_acc: 0.1032\n",
            "Epoch 1420, BestLoss: 0.24398056036057755, Temperature 0.033865535638032226, step_size 0.9344441763760625, test_acc: 0.1253\n",
            "Epoch 1421, BestLoss: 0.22103157411432636, Temperature 0.03217225885613061, step_size 0.9343507319584249, test_acc: 0.1153\n",
            "Epoch 1422, BestLoss: 0.22103157411432636, Temperature 0.03056364591332408, step_size 0.9342572968852291, test_acc: 0.1079\n",
            "Epoch 1423, BestLoss: 0.20269810376963818, Temperature 0.03217225885613061, step_size 0.9345376301390764, test_acc: 0.1184\n",
            "Epoch 1424, BestLoss: 0.20269810376963818, Temperature 0.03056364591332408, step_size 0.9344441763760625, test_acc: 0.115\n",
            "Epoch 1425, BestLoss: 0.234834343758238, Temperature 0.03217225885613061, step_size 0.9344441763760625, test_acc: 0.0848\n",
            "Epoch 1426, BestLoss: 0.22958633013894514, Temperature 0.03056364591332408, step_size 0.9343507319584249, test_acc: 0.1004\n",
            "Epoch 1427, BestLoss: 0.20934337941759537, Temperature 0.029035463617657874, step_size 0.9342572968852291, test_acc: 0.0837\n",
            "Epoch 1428, BestLoss: 0.21657111551276587, Temperature 0.02758369043677498, step_size 0.9341638711555406, test_acc: 0.1248\n",
            "Epoch 1429, BestLoss: 0.16988883223024187, Temperature 0.026204505914936227, step_size 0.934070454768425, test_acc: 0.1581\n",
            "Epoch 1430, BestLoss: 0.16988883223024187, Temperature 0.024894280619189413, step_size 0.9339770477229482, test_acc: 0.1574\n",
            "Epoch 1431, BestLoss: 0.14414350849718405, Temperature 0.026204505914936227, step_size 0.9343507319584249, test_acc: 0.0709\n",
            "Epoch 1432, BestLoss: 0.1491934560484782, Temperature 0.024894280619189413, step_size 0.9342572968852291, test_acc: 0.1145\n",
            "Epoch 1433, BestLoss: 0.09184961226599044, Temperature 0.02364956658822994, step_size 0.9341638711555406, test_acc: 0.1054\n",
            "Epoch 1434, BestLoss: 0.09184961226599044, Temperature 0.022467088258818442, step_size 0.934070454768425, test_acc: 0.1117\n",
            "Epoch 1435, BestLoss: 0.09184961226599044, Temperature 0.02364956658822994, step_size 0.9342572968852291, test_acc: 0.1172\n",
            "Epoch 1436, BestLoss: 0.09184961226599044, Temperature 0.024894280619189413, step_size 0.9341638711555406, test_acc: 0.1148\n",
            "Epoch 1437, BestLoss: 0.09184961226599044, Temperature 0.026204505914936227, step_size 0.934070454768425, test_acc: 0.0956\n",
            "Epoch 1438, BestLoss: 0.09184961226599044, Temperature 0.02758369043677498, step_size 0.9339770477229482, test_acc: 0.0978\n",
            "Epoch 1439, BestLoss: 0.09184961226599044, Temperature 0.029035463617657874, step_size 0.933883650018176, test_acc: 0.1093\n",
            "Epoch 1440, BestLoss: 0.11305933648190396, Temperature 0.03056364591332408, step_size 0.9337902616531741, test_acc: 0.0492\n",
            "Epoch 1441, BestLoss: 0.13498584446124004, Temperature 0.029035463617657874, step_size 0.9336968826270088, test_acc: 0.0629\n",
            "Epoch 1442, BestLoss: 0.13498584446124004, Temperature 0.02758369043677498, step_size 0.9336035129387461, test_acc: 0.0734\n",
            "Epoch 1443, BestLoss: 0.1745535963218593, Temperature 0.029035463617657874, step_size 0.9336968826270088, test_acc: 0.063\n",
            "Epoch 1444, BestLoss: 0.20209034573887805, Temperature 0.02758369043677498, step_size 0.9336035129387461, test_acc: 0.0485\n",
            "Epoch 1445, BestLoss: 0.18714174264506883, Temperature 0.026204505914936227, step_size 0.9335101525874523, test_acc: 0.1152\n",
            "Epoch 1446, BestLoss: 0.152491753555879, Temperature 0.024894280619189413, step_size 0.9334168015721935, test_acc: 0.0976\n",
            "Epoch 1447, BestLoss: 0.152491753555879, Temperature 0.02364956658822994, step_size 0.9333234598920364, test_acc: 0.0975\n",
            "Epoch 1448, BestLoss: 0.17863826207158054, Temperature 0.024894280619189413, step_size 0.9336035129387461, test_acc: 0.093\n",
            "Epoch 1449, BestLoss: 0.17863826207158054, Temperature 0.02364956658822994, step_size 0.9335101525874523, test_acc: 0.0884\n",
            "Epoch 1450, BestLoss: 0.17863826207158054, Temperature 0.024894280619189413, step_size 0.9335101525874523, test_acc: 0.0777\n",
            "Epoch 1451, BestLoss: 0.12886806338901538, Temperature 0.026204505914936227, step_size 0.9334168015721935, test_acc: 0.0911\n",
            "Epoch 1452, BestLoss: 0.12886806338901538, Temperature 0.024894280619189413, step_size 0.9333234598920364, test_acc: 0.0887\n",
            "Epoch 1453, BestLoss: 0.12886806338901538, Temperature 0.026204505914936227, step_size 0.9333234598920364, test_acc: 0.0853\n",
            "Epoch 1454, BestLoss: 0.12886806338901538, Temperature 0.02758369043677498, step_size 0.9332301275460472, test_acc: 0.08\n",
            "Epoch 1455, BestLoss: 0.12886806338901538, Temperature 0.029035463617657874, step_size 0.9331368045332926, test_acc: 0.0782\n",
            "Epoch 1456, BestLoss: 0.12886806338901538, Temperature 0.03056364591332408, step_size 0.9330434908528393, test_acc: 0.0802\n",
            "Epoch 1457, BestLoss: 0.12886806338901538, Temperature 0.03217225885613061, step_size 0.932950186503754, test_acc: 0.0852\n",
            "Epoch 1458, BestLoss: 0.12886806338901538, Temperature 0.033865535638032226, step_size 0.9328568914851036, test_acc: 0.0922\n",
            "Epoch 1459, BestLoss: 0.12886806338901538, Temperature 0.03564793225056024, step_size 0.9327636057959552, test_acc: 0.106\n",
            "Epoch 1460, BestLoss: 0.12886806338901538, Temperature 0.037524139211116046, step_size 0.9326703294353756, test_acc: 0.1466\n",
            "Epoch 1461, BestLoss: 0.12886806338901538, Temperature 0.039499093906437945, step_size 0.9325770624024321, test_acc: 0.2681\n",
            "Epoch 1462, BestLoss: 0.1795414417040549, Temperature 0.04157799358572416, step_size 0.9324838046961919, test_acc: 0.0567\n",
            "Epoch 1463, BestLoss: 0.13203692070995604, Temperature 0.039499093906437945, step_size 0.9323905563157222, test_acc: 0.0518\n",
            "Epoch 1464, BestLoss: 0.1830134458073474, Temperature 0.037524139211116046, step_size 0.9322973172600907, test_acc: 0.0388\n",
            "Epoch 1465, BestLoss: 0.15158846370412538, Temperature 0.03564793225056024, step_size 0.9322040875283647, test_acc: 0.1398\n",
            "Epoch 1466, BestLoss: 0.15416944225344634, Temperature 0.033865535638032226, step_size 0.9321108671196119, test_acc: 0.0874\n",
            "Epoch 1467, BestLoss: 0.15416944225344634, Temperature 0.03217225885613061, step_size 0.9320176560329, test_acc: 0.0888\n",
            "Epoch 1468, BestLoss: 0.14350938350870682, Temperature 0.033865535638032226, step_size 0.9323905563157222, test_acc: 0.1458\n",
            "Epoch 1469, BestLoss: 0.23933562057808522, Temperature 0.03217225885613061, step_size 0.9322973172600907, test_acc: 0.0524\n",
            "Epoch 1470, BestLoss: 0.19718766958354875, Temperature 0.03056364591332408, step_size 0.9322040875283647, test_acc: 0.1444\n",
            "Epoch 1471, BestLoss: 0.19718766958354875, Temperature 0.029035463617657874, step_size 0.9321108671196119, test_acc: 0.1346\n",
            "Epoch 1472, BestLoss: 0.19718766958354875, Temperature 0.03056364591332408, step_size 0.9322973172600907, test_acc: 0.1223\n",
            "Epoch 1473, BestLoss: 0.19289906100925283, Temperature 0.03217225885613061, step_size 0.9322040875283647, test_acc: 0.1232\n",
            "Epoch 1474, BestLoss: 0.19289906100925283, Temperature 0.03056364591332408, step_size 0.9321108671196119, test_acc: 0.1146\n",
            "Epoch 1475, BestLoss: 0.1476251137979482, Temperature 0.03217225885613061, step_size 0.9321108671196119, test_acc: 0.1458\n",
            "Epoch 1476, BestLoss: 0.1476251137979482, Temperature 0.03056364591332408, step_size 0.9320176560329, test_acc: 0.152\n",
            "Epoch 1477, BestLoss: 0.1476251137979482, Temperature 0.03217225885613061, step_size 0.9320176560329, test_acc: 0.1683\n",
            "Epoch 1478, BestLoss: 0.13017112116061608, Temperature 0.033865535638032226, step_size 0.9319244542672968, test_acc: 0.1136\n",
            "Epoch 1479, BestLoss: 0.13017112116061608, Temperature 0.03217225885613061, step_size 0.93183126182187, test_acc: 0.1187\n",
            "Epoch 1480, BestLoss: 0.11987847724342547, Temperature 0.033865535638032226, step_size 0.93183126182187, test_acc: 0.1095\n",
            "Epoch 1481, BestLoss: 0.11987847724342547, Temperature 0.03217225885613061, step_size 0.9317380786956878, test_acc: 0.1074\n",
            "Epoch 1482, BestLoss: 0.11987847724342547, Temperature 0.033865535638032226, step_size 0.9317380786956878, test_acc: 0.0971\n",
            "Epoch 1483, BestLoss: 0.18675234854733933, Temperature 0.03564793225056024, step_size 0.9316449048878183, test_acc: 0.1186\n",
            "Epoch 1484, BestLoss: 0.18675234854733933, Temperature 0.033865535638032226, step_size 0.9315517403973295, test_acc: 0.1317\n",
            "Epoch 1485, BestLoss: 0.19524353264557848, Temperature 0.03564793225056024, step_size 0.9315517403973295, test_acc: 0.1404\n",
            "Epoch 1486, BestLoss: 0.1718903489938272, Temperature 0.033865535638032226, step_size 0.9314585852232897, test_acc: 0.1165\n",
            "Epoch 1487, BestLoss: 0.2122150358040578, Temperature 0.03217225885613061, step_size 0.9313654393647675, test_acc: 0.1539\n",
            "Epoch 1488, BestLoss: 0.20020766099135312, Temperature 0.03056364591332408, step_size 0.931272302820831, test_acc: 0.1013\n",
            "Epoch 1489, BestLoss: 0.19156399021838136, Temperature 0.029035463617657874, step_size 0.9311791755905489, test_acc: 0.0666\n",
            "Epoch 1490, BestLoss: 0.19003317601255282, Temperature 0.02758369043677498, step_size 0.9310860576729898, test_acc: 0.0771\n",
            "Epoch 1491, BestLoss: 0.1897491572434416, Temperature 0.026204505914936227, step_size 0.9309929490672225, test_acc: 0.095\n",
            "Epoch 1492, BestLoss: 0.2198872519628641, Temperature 0.024894280619189413, step_size 0.9308998497723158, test_acc: 0.1124\n",
            "Epoch 1493, BestLoss: 0.2198872519628641, Temperature 0.02364956658822994, step_size 0.9308067597873385, test_acc: 0.1146\n",
            "Epoch 1494, BestLoss: 0.2198872519628641, Temperature 0.024894280619189413, step_size 0.9314585852232897, test_acc: 0.1169\n",
            "Epoch 1495, BestLoss: 0.2198872519628641, Temperature 0.026204505914936227, step_size 0.9313654393647675, test_acc: 0.1158\n",
            "Epoch 1496, BestLoss: 0.18663266180513424, Temperature 0.02758369043677498, step_size 0.931272302820831, test_acc: 0.1174\n",
            "Epoch 1497, BestLoss: 0.1364591885939356, Temperature 0.026204505914936227, step_size 0.9311791755905489, test_acc: 0.1208\n",
            "Epoch 1498, BestLoss: 0.1364591885939356, Temperature 0.024894280619189413, step_size 0.9310860576729898, test_acc: 0.1157\n",
            "Epoch 1499, BestLoss: 0.1364591885939356, Temperature 0.026204505914936227, step_size 0.9311791755905489, test_acc: 0.1114\n",
            "Epoch 1500, BestLoss: 0.15812599494186977, Temperature 0.02758369043677498, step_size 0.9310860576729898, test_acc: 0.0592\n",
            "Epoch 1501, BestLoss: 0.13550787294157227, Temperature 0.026204505914936227, step_size 0.9309929490672225, test_acc: 0.0608\n",
            "Epoch 1502, BestLoss: 0.13550787294157227, Temperature 0.024894280619189413, step_size 0.9308998497723158, test_acc: 0.0589\n",
            "Epoch 1503, BestLoss: 0.14583961375330032, Temperature 0.026204505914936227, step_size 0.9309929490672225, test_acc: 0.0803\n",
            "Epoch 1504, BestLoss: 0.14004510766875333, Temperature 0.024894280619189413, step_size 0.9308998497723158, test_acc: 0.1019\n",
            "Epoch 1505, BestLoss: 0.14004510766875333, Temperature 0.02364956658822994, step_size 0.9308067597873385, test_acc: 0.1005\n",
            "Epoch 1506, BestLoss: 0.12501890081674677, Temperature 0.024894280619189413, step_size 0.9308998497723158, test_acc: 0.1333\n",
            "Epoch 1507, BestLoss: 0.12501890081674677, Temperature 0.02364956658822994, step_size 0.9308067597873385, test_acc: 0.1396\n",
            "Epoch 1508, BestLoss: 0.12501890081674677, Temperature 0.024894280619189413, step_size 0.9308067597873385, test_acc: 0.1483\n",
            "Epoch 1509, BestLoss: 0.12501890081674677, Temperature 0.026204505914936227, step_size 0.9307136791113598, test_acc: 0.1495\n",
            "Epoch 1510, BestLoss: 0.12194004337950898, Temperature 0.02758369043677498, step_size 0.9306206077434487, test_acc: 0.1148\n",
            "Epoch 1511, BestLoss: 0.16515225909796669, Temperature 0.026204505914936227, step_size 0.9305275456826744, test_acc: 0.1144\n",
            "Epoch 1512, BestLoss: 0.1672980648307096, Temperature 0.024894280619189413, step_size 0.9304344929281061, test_acc: 0.0939\n",
            "Epoch 1513, BestLoss: 0.15864975694567632, Temperature 0.02364956658822994, step_size 0.9303414494788133, test_acc: 0.1052\n",
            "Epoch 1514, BestLoss: 0.15864975694567632, Temperature 0.022467088258818442, step_size 0.9302484153338654, test_acc: 0.1049\n",
            "Epoch 1515, BestLoss: 0.15864975694567632, Temperature 0.02364956658822994, step_size 0.9305275456826744, test_acc: 0.1021\n",
            "Epoch 1516, BestLoss: 0.15864975694567632, Temperature 0.024894280619189413, step_size 0.9304344929281061, test_acc: 0.0955\n",
            "Epoch 1517, BestLoss: 0.15864975694567632, Temperature 0.026204505914936227, step_size 0.9303414494788133, test_acc: 0.0917\n",
            "Epoch 1518, BestLoss: 0.1762587364389386, Temperature 0.02758369043677498, step_size 0.9302484153338654, test_acc: 0.1354\n",
            "Epoch 1519, BestLoss: 0.1762587364389386, Temperature 0.026204505914936227, step_size 0.930155390492332, test_acc: 0.1308\n",
            "Epoch 1520, BestLoss: 0.20144567099213284, Temperature 0.02758369043677498, step_size 0.930155390492332, test_acc: 0.1258\n",
            "Epoch 1521, BestLoss: 0.1908542992003725, Temperature 0.026204505914936227, step_size 0.9300623749532828, test_acc: 0.1279\n",
            "Epoch 1522, BestLoss: 0.20037292877377733, Temperature 0.024894280619189413, step_size 0.9299693687157875, test_acc: 0.0614\n",
            "Epoch 1523, BestLoss: 0.20037292877377733, Temperature 0.02364956658822994, step_size 0.9298763717789159, test_acc: 0.0606\n",
            "Epoch 1524, BestLoss: 0.18877378099886605, Temperature 0.024894280619189413, step_size 0.9300623749532828, test_acc: 0.0756\n",
            "Epoch 1525, BestLoss: 0.15852933793990864, Temperature 0.02364956658822994, step_size 0.9299693687157875, test_acc: 0.15\n",
            "Epoch 1526, BestLoss: 0.17421482795163357, Temperature 0.022467088258818442, step_size 0.9298763717789159, test_acc: 0.1668\n",
            "Epoch 1527, BestLoss: 0.19156198465936158, Temperature 0.021343733845877517, step_size 0.929783384141738, test_acc: 0.12\n",
            "Epoch 1528, BestLoss: 0.17014694767986982, Temperature 0.02027654715358364, step_size 0.9296904058033238, test_acc: 0.1454\n",
            "Epoch 1529, BestLoss: 0.15736630787620667, Temperature 0.019262719795904458, step_size 0.9295974367627435, test_acc: 0.1856\n",
            "Epoch 1530, BestLoss: 0.15736630787620667, Temperature 0.018299583806109233, step_size 0.9295044770190672, test_acc: 0.1791\n",
            "Epoch 1531, BestLoss: 0.13301898768444598, Temperature 0.019262719795904458, step_size 0.9299693687157875, test_acc: 0.0924\n",
            "Epoch 1532, BestLoss: 0.12319864049969381, Temperature 0.018299583806109233, step_size 0.9298763717789159, test_acc: 0.1491\n",
            "Epoch 1533, BestLoss: 0.12319864049969381, Temperature 0.01738460461580377, step_size 0.929783384141738, test_acc: 0.143\n",
            "Epoch 1534, BestLoss: 0.09506664885118993, Temperature 0.018299583806109233, step_size 0.9298763717789159, test_acc: 0.1608\n",
            "Epoch 1535, BestLoss: 0.09506664885118993, Temperature 0.01738460461580377, step_size 0.929783384141738, test_acc: 0.1552\n",
            "Epoch 1536, BestLoss: 0.09506664885118993, Temperature 0.018299583806109233, step_size 0.929783384141738, test_acc: 0.1417\n",
            "Epoch 1537, BestLoss: 0.09506664885118993, Temperature 0.019262719795904458, step_size 0.9296904058033238, test_acc: 0.1348\n",
            "Epoch 1538, BestLoss: 0.09506664885118993, Temperature 0.02027654715358364, step_size 0.9295974367627435, test_acc: 0.1442\n",
            "Epoch 1539, BestLoss: 0.09506664885118993, Temperature 0.021343733845877517, step_size 0.9295044770190672, test_acc: 0.1507\n",
            "Epoch 1540, BestLoss: 0.09506664885118993, Temperature 0.022467088258818442, step_size 0.9294115265713654, test_acc: 0.1618\n",
            "Epoch 1541, BestLoss: 0.09506664885118993, Temperature 0.02364956658822994, step_size 0.9293185854187083, test_acc: 0.1776\n",
            "Epoch 1542, BestLoss: 0.09506664885118993, Temperature 0.024894280619189413, step_size 0.9292256535601664, test_acc: 0.2145\n",
            "Epoch 1543, BestLoss: 0.09293746912900577, Temperature 0.026204505914936227, step_size 0.9291327309948104, test_acc: 0.085\n",
            "Epoch 1544, BestLoss: 0.09293746912900577, Temperature 0.024894280619189413, step_size 0.929039817721711, test_acc: 0.0872\n",
            "Epoch 1545, BestLoss: 0.09293746912900577, Temperature 0.026204505914936227, step_size 0.929039817721711, test_acc: 0.1\n",
            "Epoch 1546, BestLoss: 0.09293746912900577, Temperature 0.02758369043677498, step_size 0.9289469137399388, test_acc: 0.1111\n",
            "Epoch 1547, BestLoss: 0.09958396673307433, Temperature 0.029035463617657874, step_size 0.9288540190485648, test_acc: 0.1391\n",
            "Epoch 1548, BestLoss: 0.1502347391131093, Temperature 0.02758369043677498, step_size 0.9287611336466599, test_acc: 0.1129\n",
            "Epoch 1549, BestLoss: 0.1502347391131093, Temperature 0.026204505914936227, step_size 0.9286682575332953, test_acc: 0.1188\n",
            "Epoch 1550, BestLoss: 0.1502347391131093, Temperature 0.02758369043677498, step_size 0.9287611336466599, test_acc: 0.1259\n",
            "Epoch 1551, BestLoss: 0.1502347391131093, Temperature 0.029035463617657874, step_size 0.9286682575332953, test_acc: 0.1352\n",
            "Epoch 1552, BestLoss: 0.1502347391131093, Temperature 0.03056364591332408, step_size 0.928575390707542, test_acc: 0.1418\n",
            "Epoch 1553, BestLoss: 0.14118705113172944, Temperature 0.03217225885613061, step_size 0.9284825331684713, test_acc: 0.0643\n",
            "Epoch 1554, BestLoss: 0.16723227958041118, Temperature 0.03056364591332408, step_size 0.9283896849151545, test_acc: 0.1156\n",
            "Epoch 1555, BestLoss: 0.16723227958041118, Temperature 0.029035463617657874, step_size 0.928296845946663, test_acc: 0.104\n",
            "Epoch 1556, BestLoss: 0.16723227958041118, Temperature 0.03056364591332408, step_size 0.9283896849151545, test_acc: 0.0943\n",
            "Epoch 1557, BestLoss: 0.12109154787058989, Temperature 0.03217225885613061, step_size 0.928296845946663, test_acc: 0.2036\n",
            "Epoch 1558, BestLoss: 0.15567698476736502, Temperature 0.03056364591332408, step_size 0.9282040162620683, test_acc: 0.1356\n",
            "Epoch 1559, BestLoss: 0.14484103612787708, Temperature 0.029035463617657874, step_size 0.9281111958604421, test_acc: 0.0839\n",
            "Epoch 1560, BestLoss: 0.14484103612787708, Temperature 0.02758369043677498, step_size 0.9280183847408561, test_acc: 0.0832\n",
            "Epoch 1561, BestLoss: 0.14484103612787708, Temperature 0.029035463617657874, step_size 0.9282040162620683, test_acc: 0.0826\n",
            "Epoch 1562, BestLoss: 0.14484103612787708, Temperature 0.03056364591332408, step_size 0.9281111958604421, test_acc: 0.0822\n",
            "Epoch 1563, BestLoss: 0.14484103612787708, Temperature 0.03217225885613061, step_size 0.9280183847408561, test_acc: 0.0744\n",
            "Epoch 1564, BestLoss: 0.14484103612787708, Temperature 0.033865535638032226, step_size 0.927925582902382, test_acc: 0.0741\n",
            "Epoch 1565, BestLoss: 0.07609317218254553, Temperature 0.03564793225056024, step_size 0.9278327903440918, test_acc: 0.1044\n",
            "Epoch 1566, BestLoss: 0.07609317218254553, Temperature 0.033865535638032226, step_size 0.9277400070650574, test_acc: 0.0927\n",
            "Epoch 1567, BestLoss: 0.07609317218254553, Temperature 0.03564793225056024, step_size 0.9277400070650574, test_acc: 0.0885\n",
            "Epoch 1568, BestLoss: 0.07609317218254553, Temperature 0.037524139211116046, step_size 0.9276472330643509, test_acc: 0.1017\n",
            "Epoch 1569, BestLoss: 0.07609317218254553, Temperature 0.039499093906437945, step_size 0.9275544683410445, test_acc: 0.1156\n",
            "Epoch 1570, BestLoss: 0.07609317218254553, Temperature 0.04157799358572416, step_size 0.9274617128942104, test_acc: 0.1315\n",
            "Epoch 1571, BestLoss: 0.07609317218254553, Temperature 0.04376630903760438, step_size 0.927368966722921, test_acc: 0.1623\n",
            "Epoch 1572, BestLoss: 0.14048242382961684, Temperature 0.04606979898695198, step_size 0.9272762298262487, test_acc: 0.0432\n",
            "Epoch 1573, BestLoss: 0.14857102131746477, Temperature 0.04376630903760438, step_size 0.9271835022032661, test_acc: 0.0686\n",
            "Epoch 1574, BestLoss: 0.14857102131746477, Temperature 0.04157799358572416, step_size 0.9270907838530458, test_acc: 0.0739\n",
            "Epoch 1575, BestLoss: 0.13272143672860554, Temperature 0.04376630903760438, step_size 0.9271835022032661, test_acc: 0.0996\n",
            "Epoch 1576, BestLoss: 0.101099337520009, Temperature 0.04157799358572416, step_size 0.9270907838530458, test_acc: 0.105\n",
            "Epoch 1577, BestLoss: 0.1443243994016125, Temperature 0.039499093906437945, step_size 0.9269980747746605, test_acc: 0.1445\n",
            "Epoch 1578, BestLoss: 0.13008229199108534, Temperature 0.037524139211116046, step_size 0.926905374967183, test_acc: 0.0982\n",
            "Epoch 1579, BestLoss: 0.13008229199108534, Temperature 0.03564793225056024, step_size 0.9268126844296862, test_acc: 0.1037\n",
            "Epoch 1580, BestLoss: 0.13497912087319994, Temperature 0.037524139211116046, step_size 0.9270907838530458, test_acc: 0.0832\n",
            "Epoch 1581, BestLoss: 0.1389276073530661, Temperature 0.03564793225056024, step_size 0.9269980747746605, test_acc: 0.0769\n",
            "Epoch 1582, BestLoss: 0.13178211098436537, Temperature 0.033865535638032226, step_size 0.926905374967183, test_acc: 0.0553\n",
            "Epoch 1583, BestLoss: 0.13178211098436537, Temperature 0.03217225885613061, step_size 0.9268126844296862, test_acc: 0.0562\n",
            "Epoch 1584, BestLoss: 0.14065561941357815, Temperature 0.033865535638032226, step_size 0.9269980747746605, test_acc: 0.0746\n",
            "Epoch 1585, BestLoss: 0.14065561941357815, Temperature 0.03217225885613061, step_size 0.926905374967183, test_acc: 0.0788\n",
            "Epoch 1586, BestLoss: 0.12711424556715406, Temperature 0.033865535638032226, step_size 0.926905374967183, test_acc: 0.1688\n",
            "Epoch 1587, BestLoss: 0.12711424556715406, Temperature 0.03217225885613061, step_size 0.9268126844296862, test_acc: 0.1615\n",
            "Epoch 1588, BestLoss: 0.12711424556715406, Temperature 0.033865535638032226, step_size 0.9268126844296862, test_acc: 0.1558\n",
            "Epoch 1589, BestLoss: 0.12711424556715406, Temperature 0.03564793225056024, step_size 0.9267200031612433, test_acc: 0.1564\n",
            "Epoch 1590, BestLoss: 0.12711424556715406, Temperature 0.037524139211116046, step_size 0.9266273311609271, test_acc: 0.1645\n",
            "Epoch 1591, BestLoss: 0.12711424556715406, Temperature 0.039499093906437945, step_size 0.9265346684278111, test_acc: 0.1793\n",
            "Epoch 1592, BestLoss: 0.14662275814623865, Temperature 0.04157799358572416, step_size 0.9264420149609683, test_acc: 0.1532\n",
            "Epoch 1593, BestLoss: 0.18427456404093218, Temperature 0.039499093906437945, step_size 0.9263493707594722, test_acc: 0.1392\n",
            "Epoch 1594, BestLoss: 0.13470525148472348, Temperature 0.037524139211116046, step_size 0.9262567358223963, test_acc: 0.1027\n",
            "Epoch 1595, BestLoss: 0.13470525148472348, Temperature 0.03564793225056024, step_size 0.9261641101488141, test_acc: 0.1197\n",
            "Epoch 1596, BestLoss: 0.13470525148472348, Temperature 0.037524139211116046, step_size 0.9263493707594722, test_acc: 0.1471\n",
            "Epoch 1597, BestLoss: 0.13470525148472348, Temperature 0.039499093906437945, step_size 0.9262567358223963, test_acc: 0.1699\n",
            "Epoch 1598, BestLoss: 0.13470525148472348, Temperature 0.04157799358572416, step_size 0.9261641101488141, test_acc: 0.1884\n",
            "Epoch 1599, BestLoss: 0.11380109154133201, Temperature 0.04376630903760438, step_size 0.9260714937377992, test_acc: 0.1051\n",
            "Epoch 1600, BestLoss: 0.11380109154133201, Temperature 0.04157799358572416, step_size 0.9259788865884254, test_acc: 0.1082\n",
            "Epoch 1601, BestLoss: 0.11380109154133201, Temperature 0.04376630903760438, step_size 0.9259788865884254, test_acc: 0.1036\n",
            "Epoch 1602, BestLoss: 0.11380109154133201, Temperature 0.04606979898695198, step_size 0.9258862886997666, test_acc: 0.1021\n",
            "Epoch 1603, BestLoss: 0.11380109154133201, Temperature 0.04849452524942314, step_size 0.9257937000708967, test_acc: 0.117\n",
            "Epoch 1604, BestLoss: 0.11380109154133201, Temperature 0.05104686868360331, step_size 0.9257011207008896, test_acc: 0.1339\n",
            "Epoch 1605, BestLoss: 0.11380109154133201, Temperature 0.05373354598274033, step_size 0.9256085505888195, test_acc: 0.1526\n",
            "Epoch 1606, BestLoss: 0.11380109154133201, Temperature 0.056561627350252976, step_size 0.9255159897337607, test_acc: 0.1804\n",
            "Epoch 1607, BestLoss: 0.12930185168930125, Temperature 0.05953855510552945, step_size 0.9254234381347873, test_acc: 0.0971\n",
            "Epoch 1608, BestLoss: 0.12930185168930125, Temperature 0.056561627350252976, step_size 0.9253308957909738, test_acc: 0.1046\n",
            "Epoch 1609, BestLoss: 0.14225004659773097, Temperature 0.05953855510552945, step_size 0.9253308957909738, test_acc: 0.1448\n",
            "Epoch 1610, BestLoss: 0.17147533717892932, Temperature 0.056561627350252976, step_size 0.9252383627013947, test_acc: 0.0913\n",
            "Epoch 1611, BestLoss: 0.17049970727952815, Temperature 0.05373354598274033, step_size 0.9251458388651246, test_acc: 0.1031\n",
            "Epoch 1612, BestLoss: 0.14048371013863598, Temperature 0.05104686868360331, step_size 0.9250533242812381, test_acc: 0.0703\n",
            "Epoch 1613, BestLoss: 0.14048371013863598, Temperature 0.04849452524942314, step_size 0.92496081894881, test_acc: 0.0675\n",
            "Epoch 1614, BestLoss: 0.1117682854947504, Temperature 0.05104686868360331, step_size 0.9252383627013947, test_acc: 0.0617\n",
            "Epoch 1615, BestLoss: 0.1117682854947504, Temperature 0.04849452524942314, step_size 0.9251458388651246, test_acc: 0.0653\n",
            "Epoch 1616, BestLoss: 0.1117682854947504, Temperature 0.05104686868360331, step_size 0.9251458388651246, test_acc: 0.0706\n",
            "Epoch 1617, BestLoss: 0.1117682854947504, Temperature 0.05373354598274033, step_size 0.9250533242812381, test_acc: 0.0731\n",
            "Epoch 1618, BestLoss: 0.2023417198759565, Temperature 0.056561627350252976, step_size 0.92496081894881, test_acc: 0.0588\n",
            "Epoch 1619, BestLoss: 0.17138846893977236, Temperature 0.05373354598274033, step_size 0.9248683228669151, test_acc: 0.0549\n",
            "Epoch 1620, BestLoss: 0.14270784050886673, Temperature 0.05104686868360331, step_size 0.9247758360346284, test_acc: 0.0742\n",
            "Epoch 1621, BestLoss: 0.12108336177568332, Temperature 0.04849452524942314, step_size 0.924683358451025, test_acc: 0.0728\n",
            "Epoch 1622, BestLoss: 0.13386206932665895, Temperature 0.04606979898695198, step_size 0.9245908901151799, test_acc: 0.0634\n",
            "Epoch 1623, BestLoss: 0.13386206932665895, Temperature 0.04376630903760438, step_size 0.9244984310261685, test_acc: 0.0625\n",
            "Epoch 1624, BestLoss: 0.13386206932665895, Temperature 0.04606979898695198, step_size 0.9248683228669151, test_acc: 0.0645\n",
            "Epoch 1625, BestLoss: 0.13386206932665895, Temperature 0.04849452524942314, step_size 0.9247758360346284, test_acc: 0.068\n",
            "Epoch 1626, BestLoss: 0.13786914473784947, Temperature 0.05104686868360331, step_size 0.924683358451025, test_acc: 0.1029\n",
            "Epoch 1627, BestLoss: 0.13786914473784947, Temperature 0.04849452524942314, step_size 0.9245908901151799, test_acc: 0.1048\n",
            "Epoch 1628, BestLoss: 0.21371825016775675, Temperature 0.05104686868360331, step_size 0.9245908901151799, test_acc: 0.1397\n",
            "Epoch 1629, BestLoss: 0.21371825016775675, Temperature 0.04849452524942314, step_size 0.9244984310261685, test_acc: 0.1325\n",
            "Epoch 1630, BestLoss: 0.20287681067089366, Temperature 0.05104686868360331, step_size 0.9244984310261685, test_acc: 0.0944\n",
            "Epoch 1631, BestLoss: 0.1989077404823446, Temperature 0.04849452524942314, step_size 0.9244059811830658, test_acc: 0.0987\n",
            "Epoch 1632, BestLoss: 0.18136640832993242, Temperature 0.04606979898695198, step_size 0.9243135405849475, test_acc: 0.1188\n",
            "Epoch 1633, BestLoss: 0.21731097197868274, Temperature 0.04376630903760438, step_size 0.9242211092308891, test_acc: 0.0985\n",
            "Epoch 1634, BestLoss: 0.19847269348873617, Temperature 0.04157799358572416, step_size 0.924128687119966, test_acc: 0.1008\n",
            "Epoch 1635, BestLoss: 0.20899465733096265, Temperature 0.039499093906437945, step_size 0.924036274251254, test_acc: 0.0948\n",
            "Epoch 1636, BestLoss: 0.23638668910414307, Temperature 0.037524139211116046, step_size 0.9239438706238289, test_acc: 0.0497\n",
            "Epoch 1637, BestLoss: 0.24069644775700988, Temperature 0.03564793225056024, step_size 0.9238514762367666, test_acc: 0.1605\n",
            "Epoch 1638, BestLoss: 0.24069644775700988, Temperature 0.033865535638032226, step_size 0.9237590910891429, test_acc: 0.1583\n",
            "Epoch 1639, BestLoss: 0.21330245547668075, Temperature 0.03564793225056024, step_size 0.9244059811830658, test_acc: 0.1664\n",
            "Epoch 1640, BestLoss: 0.21330245547668075, Temperature 0.033865535638032226, step_size 0.9243135405849475, test_acc: 0.1598\n",
            "Epoch 1641, BestLoss: 0.1808243164404101, Temperature 0.03564793225056024, step_size 0.9243135405849475, test_acc: 0.0966\n",
            "Epoch 1642, BestLoss: 0.18141609958261085, Temperature 0.033865535638032226, step_size 0.9242211092308891, test_acc: 0.1173\n",
            "Epoch 1643, BestLoss: 0.1768706492528714, Temperature 0.03217225885613061, step_size 0.924128687119966, test_acc: 0.0833\n",
            "Epoch 1644, BestLoss: 0.20367133711641144, Temperature 0.03056364591332408, step_size 0.924036274251254, test_acc: 0.0502\n",
            "Epoch 1645, BestLoss: 0.20367133711641144, Temperature 0.029035463617657874, step_size 0.9239438706238289, test_acc: 0.0498\n",
            "Epoch 1646, BestLoss: 0.21708292605609433, Temperature 0.03056364591332408, step_size 0.9242211092308891, test_acc: 0.0411\n",
            "Epoch 1647, BestLoss: 0.230571631569887, Temperature 0.029035463617657874, step_size 0.924128687119966, test_acc: 0.0683\n",
            "Epoch 1648, BestLoss: 0.230571631569887, Temperature 0.02758369043677498, step_size 0.924036274251254, test_acc: 0.0746\n",
            "Epoch 1649, BestLoss: 0.230571631569887, Temperature 0.029035463617657874, step_size 0.924128687119966, test_acc: 0.0833\n",
            "Epoch 1650, BestLoss: 0.17851654135525444, Temperature 0.03056364591332408, step_size 0.924036274251254, test_acc: 0.1186\n",
            "Epoch 1651, BestLoss: 0.17851654135525444, Temperature 0.029035463617657874, step_size 0.9239438706238289, test_acc: 0.1206\n",
            "Epoch 1652, BestLoss: 0.17851654135525444, Temperature 0.03056364591332408, step_size 0.9239438706238289, test_acc: 0.116\n",
            "Epoch 1653, BestLoss: 0.17851654135525444, Temperature 0.03217225885613061, step_size 0.9238514762367666, test_acc: 0.1097\n",
            "Epoch 1654, BestLoss: 0.17851654135525444, Temperature 0.033865535638032226, step_size 0.9237590910891429, test_acc: 0.1031\n",
            "Epoch 1655, BestLoss: 0.17851654135525444, Temperature 0.03564793225056024, step_size 0.923666715180034, test_acc: 0.1055\n",
            "Epoch 1656, BestLoss: 0.18590717412084204, Temperature 0.037524139211116046, step_size 0.9235743485085159, test_acc: 0.1164\n",
            "Epoch 1657, BestLoss: 0.17119743014554833, Temperature 0.03564793225056024, step_size 0.9234819910736651, test_acc: 0.1226\n",
            "Epoch 1658, BestLoss: 0.17119743014554833, Temperature 0.033865535638032226, step_size 0.9233896428745578, test_acc: 0.1236\n",
            "Epoch 1659, BestLoss: 0.17119743014554833, Temperature 0.03564793225056024, step_size 0.9234819910736651, test_acc: 0.1192\n",
            "Epoch 1660, BestLoss: 0.17119743014554833, Temperature 0.037524139211116046, step_size 0.9233896428745578, test_acc: 0.1232\n",
            "Epoch 1661, BestLoss: 0.17119743014554833, Temperature 0.039499093906437945, step_size 0.9232973039102704, test_acc: 0.1181\n",
            "Epoch 1662, BestLoss: 0.216980339804157, Temperature 0.04157799358572416, step_size 0.9232049741798795, test_acc: 0.0793\n",
            "Epoch 1663, BestLoss: 0.216980339804157, Temperature 0.039499093906437945, step_size 0.9231126536824614, test_acc: 0.0856\n",
            "Epoch 1664, BestLoss: 0.2033524671527713, Temperature 0.04157799358572416, step_size 0.9231126536824614, test_acc: 0.1172\n",
            "Epoch 1665, BestLoss: 0.2122201607857515, Temperature 0.039499093906437945, step_size 0.9230203424170932, test_acc: 0.0818\n",
            "Epoch 1666, BestLoss: 0.20275159161849846, Temperature 0.037524139211116046, step_size 0.9229280403828515, test_acc: 0.0731\n",
            "Epoch 1667, BestLoss: 0.2091628449312942, Temperature 0.03564793225056024, step_size 0.9228357475788131, test_acc: 0.1097\n",
            "Epoch 1668, BestLoss: 0.22664322808672213, Temperature 0.033865535638032226, step_size 0.9227434640040553, test_acc: 0.0895\n",
            "Epoch 1669, BestLoss: 0.15447669598396388, Temperature 0.03217225885613061, step_size 0.9226511896576549, test_acc: 0.1015\n",
            "Epoch 1670, BestLoss: 0.15447669598396388, Temperature 0.03056364591332408, step_size 0.9225589245386892, test_acc: 0.0949\n",
            "Epoch 1671, BestLoss: 0.15447669598396388, Temperature 0.03217225885613061, step_size 0.9230203424170932, test_acc: 0.0899\n",
            "Epoch 1672, BestLoss: 0.15447669598396388, Temperature 0.033865535638032226, step_size 0.9229280403828515, test_acc: 0.0806\n",
            "Epoch 1673, BestLoss: 0.15447669598396388, Temperature 0.03564793225056024, step_size 0.9228357475788131, test_acc: 0.0823\n",
            "Epoch 1674, BestLoss: 0.15447669598396388, Temperature 0.037524139211116046, step_size 0.9227434640040553, test_acc: 0.085\n",
            "Epoch 1675, BestLoss: 0.15447669598396388, Temperature 0.039499093906437945, step_size 0.9226511896576549, test_acc: 0.0926\n",
            "Epoch 1676, BestLoss: 0.15447669598396388, Temperature 0.04157799358572416, step_size 0.9225589245386892, test_acc: 0.1055\n",
            "Epoch 1677, BestLoss: 0.15447669598396388, Temperature 0.04376630903760438, step_size 0.9224666686462353, test_acc: 0.1319\n",
            "Epoch 1678, BestLoss: 0.15447669598396388, Temperature 0.04606979898695198, step_size 0.9223744219793707, test_acc: 0.192\n",
            "Epoch 1679, BestLoss: 0.15447669598396388, Temperature 0.04849452524942314, step_size 0.9222821845371728, test_acc: 0.3206\n",
            "Epoch 1680, BestLoss: 0.16055287164135715, Temperature 0.05104686868360331, step_size 0.9221899563187191, test_acc: 0.1027\n",
            "Epoch 1681, BestLoss: 0.15019187703984702, Temperature 0.04849452524942314, step_size 0.9220977373230873, test_acc: 0.0653\n",
            "Epoch 1682, BestLoss: 0.1471833461850294, Temperature 0.04606979898695198, step_size 0.922005527549355, test_acc: 0.0761\n",
            "Epoch 1683, BestLoss: 0.13583473862069978, Temperature 0.04376630903760438, step_size 0.9219133269966001, test_acc: 0.0672\n",
            "Epoch 1684, BestLoss: 0.13049572275345364, Temperature 0.04157799358572416, step_size 0.9218211356639004, test_acc: 0.0478\n",
            "Epoch 1685, BestLoss: 0.12113665570180358, Temperature 0.039499093906437945, step_size 0.921728953550334, test_acc: 0.0383\n",
            "Epoch 1686, BestLoss: 0.12113665570180358, Temperature 0.037524139211116046, step_size 0.921636780654979, test_acc: 0.0391\n",
            "Epoch 1687, BestLoss: 0.12113665570180358, Temperature 0.039499093906437945, step_size 0.9220977373230873, test_acc: 0.0382\n",
            "Epoch 1688, BestLoss: 0.12113665570180358, Temperature 0.04157799358572416, step_size 0.922005527549355, test_acc: 0.0439\n",
            "Epoch 1689, BestLoss: 0.12113665570180358, Temperature 0.04376630903760438, step_size 0.9219133269966001, test_acc: 0.0537\n",
            "Epoch 1690, BestLoss: 0.12113665570180358, Temperature 0.04606979898695198, step_size 0.9218211356639004, test_acc: 0.0643\n",
            "Epoch 1691, BestLoss: 0.12113665570180358, Temperature 0.04849452524942314, step_size 0.921728953550334, test_acc: 0.0711\n",
            "Epoch 1692, BestLoss: 0.12113665570180358, Temperature 0.05104686868360331, step_size 0.921636780654979, test_acc: 0.0817\n",
            "Epoch 1693, BestLoss: 0.12113665570180358, Temperature 0.05373354598274033, step_size 0.9215446169769135, test_acc: 0.0985\n",
            "Epoch 1694, BestLoss: 0.19735658936593073, Temperature 0.056561627350252976, step_size 0.9214524625152158, test_acc: 0.1001\n",
            "Epoch 1695, BestLoss: 0.14614435456004, Temperature 0.05373354598274033, step_size 0.9213603172689643, test_acc: 0.1511\n",
            "Epoch 1696, BestLoss: 0.1759092888223712, Temperature 0.05104686868360331, step_size 0.9212681812372374, test_acc: 0.1418\n",
            "Epoch 1697, BestLoss: 0.15013639242448631, Temperature 0.04849452524942314, step_size 0.9211760544191137, test_acc: 0.14\n",
            "Epoch 1698, BestLoss: 0.1663399770666495, Temperature 0.04606979898695198, step_size 0.9210839368136717, test_acc: 0.1026\n",
            "Epoch 1699, BestLoss: 0.16273164286222747, Temperature 0.04376630903760438, step_size 0.9209918284199904, test_acc: 0.1317\n",
            "Epoch 1700, BestLoss: 0.15324776252816757, Temperature 0.04157799358572416, step_size 0.9208997292371484, test_acc: 0.1436\n",
            "Epoch 1701, BestLoss: 0.12307232499058254, Temperature 0.039499093906437945, step_size 0.9208076392642246, test_acc: 0.1016\n",
            "Epoch 1702, BestLoss: 0.12307232499058254, Temperature 0.037524139211116046, step_size 0.9207155585002982, test_acc: 0.097\n",
            "Epoch 1703, BestLoss: 0.17594218715305526, Temperature 0.039499093906437945, step_size 0.9213603172689643, test_acc: 0.1182\n",
            "Epoch 1704, BestLoss: 0.15181637399423967, Temperature 0.037524139211116046, step_size 0.9212681812372374, test_acc: 0.1477\n",
            "Epoch 1705, BestLoss: 0.15181637399423967, Temperature 0.03564793225056024, step_size 0.9211760544191137, test_acc: 0.1462\n",
            "Epoch 1706, BestLoss: 0.15181637399423967, Temperature 0.037524139211116046, step_size 0.9212681812372374, test_acc: 0.1369\n",
            "Epoch 1707, BestLoss: 0.15181637399423967, Temperature 0.039499093906437945, step_size 0.9211760544191137, test_acc: 0.1171\n",
            "Epoch 1708, BestLoss: 0.09133717510004542, Temperature 0.04157799358572416, step_size 0.9210839368136717, test_acc: 0.0754\n",
            "Epoch 1709, BestLoss: 0.14773379569978554, Temperature 0.039499093906437945, step_size 0.9209918284199904, test_acc: 0.1806\n",
            "Epoch 1710, BestLoss: 0.16215825310973658, Temperature 0.037524139211116046, step_size 0.9208997292371484, test_acc: 0.1335\n",
            "Epoch 1711, BestLoss: 0.1771583523809092, Temperature 0.03564793225056024, step_size 0.9208076392642246, test_acc: 0.0879\n",
            "Epoch 1712, BestLoss: 0.1558316783721491, Temperature 0.033865535638032226, step_size 0.9207155585002982, test_acc: 0.1023\n",
            "Epoch 1713, BestLoss: 0.17647209824215782, Temperature 0.03217225885613061, step_size 0.9206234869444482, test_acc: 0.1877\n",
            "Epoch 1714, BestLoss: 0.21312044693976875, Temperature 0.03056364591332408, step_size 0.9205314245957538, test_acc: 0.1121\n",
            "Epoch 1715, BestLoss: 0.21312044693976875, Temperature 0.029035463617657874, step_size 0.9204393714532942, test_acc: 0.1081\n",
            "Epoch 1716, BestLoss: 0.21312044693976875, Temperature 0.03056364591332408, step_size 0.9209918284199904, test_acc: 0.1018\n",
            "Epoch 1717, BestLoss: 0.15120869654762092, Temperature 0.03217225885613061, step_size 0.9208997292371484, test_acc: 0.0697\n",
            "Epoch 1718, BestLoss: 0.21387983120939147, Temperature 0.03056364591332408, step_size 0.9208076392642246, test_acc: 0.1005\n",
            "Epoch 1719, BestLoss: 0.19296324521002173, Temperature 0.029035463617657874, step_size 0.9207155585002982, test_acc: 0.072\n",
            "Epoch 1720, BestLoss: 0.18821619721090332, Temperature 0.02758369043677498, step_size 0.9206234869444482, test_acc: 0.082\n",
            "Epoch 1721, BestLoss: 0.18821619721090332, Temperature 0.026204505914936227, step_size 0.9205314245957538, test_acc: 0.0813\n",
            "Epoch 1722, BestLoss: 0.1729508857332074, Temperature 0.02758369043677498, step_size 0.9208076392642246, test_acc: 0.1026\n",
            "Epoch 1723, BestLoss: 0.1729508857332074, Temperature 0.026204505914936227, step_size 0.9207155585002982, test_acc: 0.105\n",
            "Epoch 1724, BestLoss: 0.13317450205210987, Temperature 0.02758369043677498, step_size 0.9207155585002982, test_acc: 0.0422\n",
            "Epoch 1725, BestLoss: 0.13317450205210987, Temperature 0.026204505914936227, step_size 0.9206234869444482, test_acc: 0.0419\n",
            "Epoch 1726, BestLoss: 0.13317450205210987, Temperature 0.02758369043677498, step_size 0.9206234869444482, test_acc: 0.0398\n",
            "Epoch 1727, BestLoss: 0.14632294988353442, Temperature 0.029035463617657874, step_size 0.9205314245957538, test_acc: 0.1309\n",
            "Epoch 1728, BestLoss: 0.14632294988353442, Temperature 0.02758369043677498, step_size 0.9204393714532942, test_acc: 0.1463\n",
            "Epoch 1729, BestLoss: 0.14632294988353442, Temperature 0.029035463617657874, step_size 0.9204393714532942, test_acc: 0.1515\n",
            "Epoch 1730, BestLoss: 0.1912360622978279, Temperature 0.03056364591332408, step_size 0.9203473275161489, test_acc: 0.0713\n",
            "Epoch 1731, BestLoss: 0.1912360622978279, Temperature 0.029035463617657874, step_size 0.9202552927833972, test_acc: 0.0821\n",
            "Epoch 1732, BestLoss: 0.21459548068693898, Temperature 0.03056364591332408, step_size 0.9202552927833972, test_acc: 0.1351\n",
            "Epoch 1733, BestLoss: 0.17419095794859843, Temperature 0.029035463617657874, step_size 0.9201632672541189, test_acc: 0.0931\n",
            "Epoch 1734, BestLoss: 0.1662820160259639, Temperature 0.02758369043677498, step_size 0.9200712509273935, test_acc: 0.1101\n",
            "Epoch 1735, BestLoss: 0.1391188113765349, Temperature 0.026204505914936227, step_size 0.9199792438023008, test_acc: 0.1269\n",
            "Epoch 1736, BestLoss: 0.1391188113765349, Temperature 0.024894280619189413, step_size 0.9198872458779206, test_acc: 0.1242\n",
            "Epoch 1737, BestLoss: 0.1391188113765349, Temperature 0.026204505914936227, step_size 0.9201632672541189, test_acc: 0.1143\n",
            "Epoch 1738, BestLoss: 0.16871367679193694, Temperature 0.02758369043677498, step_size 0.9200712509273935, test_acc: 0.1109\n",
            "Epoch 1739, BestLoss: 0.1685631899140547, Temperature 0.026204505914936227, step_size 0.9199792438023008, test_acc: 0.052\n",
            "Epoch 1740, BestLoss: 0.1685631899140547, Temperature 0.024894280619189413, step_size 0.9198872458779206, test_acc: 0.0522\n",
            "Epoch 1741, BestLoss: 0.1685631899140547, Temperature 0.026204505914936227, step_size 0.9199792438023008, test_acc: 0.0514\n",
            "Epoch 1742, BestLoss: 0.1550616734731626, Temperature 0.02758369043677498, step_size 0.9198872458779206, test_acc: 0.0474\n",
            "Epoch 1743, BestLoss: 0.1550616734731626, Temperature 0.026204505914936227, step_size 0.9197952571533328, test_acc: 0.0527\n",
            "Epoch 1744, BestLoss: 0.1550616734731626, Temperature 0.02758369043677498, step_size 0.9197952571533328, test_acc: 0.0635\n",
            "Epoch 1745, BestLoss: 0.1550616734731626, Temperature 0.029035463617657874, step_size 0.9197032776276175, test_acc: 0.0802\n",
            "Epoch 1746, BestLoss: 0.1550616734731626, Temperature 0.03056364591332408, step_size 0.9196113072998547, test_acc: 0.0927\n",
            "Epoch 1747, BestLoss: 0.1550616734731626, Temperature 0.03217225885613061, step_size 0.9195193461691248, test_acc: 0.1047\n",
            "Epoch 1748, BestLoss: 0.1550616734731626, Temperature 0.033865535638032226, step_size 0.9194273942345079, test_acc: 0.1211\n",
            "Epoch 1749, BestLoss: 0.1915642876930873, Temperature 0.03564793225056024, step_size 0.9193354514950844, test_acc: 0.1037\n",
            "Epoch 1750, BestLoss: 0.1915642876930873, Temperature 0.033865535638032226, step_size 0.919243517949935, test_acc: 0.1084\n",
            "Epoch 1751, BestLoss: 0.1915642876930873, Temperature 0.03564793225056024, step_size 0.919243517949935, test_acc: 0.1173\n",
            "Epoch 1752, BestLoss: 0.1915642876930873, Temperature 0.037524139211116046, step_size 0.91915159359814, test_acc: 0.1383\n",
            "Epoch 1753, BestLoss: 0.1304421572359841, Temperature 0.039499093906437945, step_size 0.9190596784387802, test_acc: 0.0863\n",
            "Epoch 1754, BestLoss: 0.12877767012233599, Temperature 0.037524139211116046, step_size 0.9189677724709363, test_acc: 0.0521\n",
            "Epoch 1755, BestLoss: 0.12877767012233599, Temperature 0.03564793225056024, step_size 0.9188758756936892, test_acc: 0.0525\n",
            "Epoch 1756, BestLoss: 0.12877767012233599, Temperature 0.037524139211116046, step_size 0.9189677724709363, test_acc: 0.0534\n",
            "Epoch 1757, BestLoss: 0.19302472499951442, Temperature 0.039499093906437945, step_size 0.9188758756936892, test_acc: 0.1944\n",
            "Epoch 1758, BestLoss: 0.27710747293828736, Temperature 0.037524139211116046, step_size 0.9187839881061198, test_acc: 0.1821\n",
            "Epoch 1759, BestLoss: 0.2804486177727935, Temperature 0.03564793225056024, step_size 0.9186921097073092, test_acc: 0.1697\n",
            "Epoch 1760, BestLoss: 0.30196148324446487, Temperature 0.033865535638032226, step_size 0.9186002404963385, test_acc: 0.1433\n",
            "Epoch 1761, BestLoss: 0.26074601067908665, Temperature 0.03217225885613061, step_size 0.9185083804722889, test_acc: 0.1724\n",
            "Epoch 1762, BestLoss: 0.23113375481627138, Temperature 0.03056364591332408, step_size 0.9184165296342417, test_acc: 0.0648\n",
            "Epoch 1763, BestLoss: 0.25625816324310763, Temperature 0.029035463617657874, step_size 0.9183246879812783, test_acc: 0.1059\n",
            "Epoch 1764, BestLoss: 0.25625816324310763, Temperature 0.02758369043677498, step_size 0.9182328555124802, test_acc: 0.1041\n",
            "Epoch 1765, BestLoss: 0.1992832232861036, Temperature 0.029035463617657874, step_size 0.9187839881061198, test_acc: 0.0813\n",
            "Epoch 1766, BestLoss: 0.20048485549539627, Temperature 0.02758369043677498, step_size 0.9186921097073092, test_acc: 0.0669\n",
            "Epoch 1767, BestLoss: 0.19002221301355507, Temperature 0.026204505914936227, step_size 0.9186002404963385, test_acc: 0.0842\n",
            "Epoch 1768, BestLoss: 0.18884220028612464, Temperature 0.024894280619189413, step_size 0.9185083804722889, test_acc: 0.0776\n",
            "Epoch 1769, BestLoss: 0.18884220028612464, Temperature 0.02364956658822994, step_size 0.9184165296342417, test_acc: 0.0777\n",
            "Epoch 1770, BestLoss: 0.18884220028612464, Temperature 0.024894280619189413, step_size 0.9186921097073092, test_acc: 0.0782\n",
            "Epoch 1771, BestLoss: 0.18884220028612464, Temperature 0.026204505914936227, step_size 0.9186002404963385, test_acc: 0.0794\n",
            "Epoch 1772, BestLoss: 0.18884220028612464, Temperature 0.02758369043677498, step_size 0.9185083804722889, test_acc: 0.0821\n",
            "Epoch 1773, BestLoss: 0.18884220028612464, Temperature 0.029035463617657874, step_size 0.9184165296342417, test_acc: 0.0817\n",
            "Epoch 1774, BestLoss: 0.18884220028612464, Temperature 0.03056364591332408, step_size 0.9183246879812783, test_acc: 0.0882\n",
            "Epoch 1775, BestLoss: 0.18884220028612464, Temperature 0.03217225885613061, step_size 0.9182328555124802, test_acc: 0.1042\n",
            "Epoch 1776, BestLoss: 0.22011218974537114, Temperature 0.033865535638032226, step_size 0.918141032226929, test_acc: 0.0688\n",
            "Epoch 1777, BestLoss: 0.14837723874927253, Temperature 0.03217225885613061, step_size 0.9180492181237063, test_acc: 0.156\n",
            "Epoch 1778, BestLoss: 0.16602246562855152, Temperature 0.03056364591332408, step_size 0.917957413201894, test_acc: 0.1013\n",
            "Epoch 1779, BestLoss: 0.16602246562855152, Temperature 0.029035463617657874, step_size 0.9178656174605737, test_acc: 0.1063\n",
            "Epoch 1780, BestLoss: 0.16602246562855152, Temperature 0.03056364591332408, step_size 0.9180492181237063, test_acc: 0.1099\n",
            "Epoch 1781, BestLoss: 0.16602246562855152, Temperature 0.03217225885613061, step_size 0.917957413201894, test_acc: 0.1127\n",
            "Epoch 1782, BestLoss: 0.14326533138084535, Temperature 0.033865535638032226, step_size 0.9178656174605737, test_acc: 0.1836\n",
            "Epoch 1783, BestLoss: 0.13314054768954484, Temperature 0.03217225885613061, step_size 0.9177738308988277, test_acc: 0.156\n",
            "Epoch 1784, BestLoss: 0.13314054768954484, Temperature 0.03056364591332408, step_size 0.9176820535157378, test_acc: 0.1531\n",
            "Epoch 1785, BestLoss: 0.16535186297478924, Temperature 0.03217225885613061, step_size 0.9177738308988277, test_acc: 0.1457\n",
            "Epoch 1786, BestLoss: 0.16535186297478924, Temperature 0.03056364591332408, step_size 0.9176820535157378, test_acc: 0.1462\n",
            "Epoch 1787, BestLoss: 0.1671536890363554, Temperature 0.03217225885613061, step_size 0.9176820535157378, test_acc: 0.1057\n",
            "Epoch 1788, BestLoss: 0.1671536890363554, Temperature 0.03056364591332408, step_size 0.9175902853103862, test_acc: 0.1093\n",
            "Epoch 1789, BestLoss: 0.1751598413987633, Temperature 0.03217225885613061, step_size 0.9175902853103862, test_acc: 0.0861\n",
            "Epoch 1790, BestLoss: 0.1346657661896815, Temperature 0.03056364591332408, step_size 0.9174985262818552, test_acc: 0.1099\n",
            "Epoch 1791, BestLoss: 0.1346657661896815, Temperature 0.029035463617657874, step_size 0.9174067764292271, test_acc: 0.1046\n",
            "Epoch 1792, BestLoss: 0.1346657661896815, Temperature 0.03056364591332408, step_size 0.9174985262818552, test_acc: 0.0997\n",
            "Epoch 1793, BestLoss: 0.13421198874928147, Temperature 0.03217225885613061, step_size 0.9174067764292271, test_acc: 0.1363\n",
            "Epoch 1794, BestLoss: 0.13882769955568458, Temperature 0.03056364591332408, step_size 0.9173150357515841, test_acc: 0.0858\n",
            "Epoch 1795, BestLoss: 0.13882769955568458, Temperature 0.029035463617657874, step_size 0.917223304248009, test_acc: 0.0861\n",
            "Epoch 1796, BestLoss: 0.1845253860994232, Temperature 0.03056364591332408, step_size 0.9173150357515841, test_acc: 0.1118\n",
            "Epoch 1797, BestLoss: 0.1845253860994232, Temperature 0.029035463617657874, step_size 0.917223304248009, test_acc: 0.1154\n",
            "Epoch 1798, BestLoss: 0.1604802469457023, Temperature 0.03056364591332408, step_size 0.917223304248009, test_acc: 0.1079\n",
            "Epoch 1799, BestLoss: 0.1630506041433013, Temperature 0.029035463617657874, step_size 0.9171315819175843, test_acc: 0.0917\n",
            "Epoch 1800, BestLoss: 0.1630506041433013, Temperature 0.02758369043677498, step_size 0.9170398687593925, test_acc: 0.0919\n",
            "Epoch 1801, BestLoss: 0.1630506041433013, Temperature 0.029035463617657874, step_size 0.9171315819175843, test_acc: 0.0922\n",
            "Epoch 1802, BestLoss: 0.1630506041433013, Temperature 0.03056364591332408, step_size 0.9170398687593925, test_acc: 0.0904\n",
            "Epoch 1803, BestLoss: 0.12836159111457512, Temperature 0.03217225885613061, step_size 0.9169481647725165, test_acc: 0.0727\n",
            "Epoch 1804, BestLoss: 0.16147499069314034, Temperature 0.03056364591332408, step_size 0.9168564699560393, test_acc: 0.1329\n",
            "Epoch 1805, BestLoss: 0.17573389028026298, Temperature 0.029035463617657874, step_size 0.9167647843090437, test_acc: 0.0262\n",
            "Epoch 1806, BestLoss: 0.17573389028026298, Temperature 0.02758369043677498, step_size 0.9166731078306128, test_acc: 0.0288\n",
            "Epoch 1807, BestLoss: 0.17573389028026298, Temperature 0.029035463617657874, step_size 0.9168564699560393, test_acc: 0.0331\n",
            "Epoch 1808, BestLoss: 0.17573389028026298, Temperature 0.03056364591332408, step_size 0.9167647843090437, test_acc: 0.0439\n",
            "Epoch 1809, BestLoss: 0.1872717035779158, Temperature 0.03217225885613061, step_size 0.9166731078306128, test_acc: 0.1153\n",
            "Epoch 1810, BestLoss: 0.187580531992224, Temperature 0.03056364591332408, step_size 0.9165814405198298, test_acc: 0.0956\n",
            "Epoch 1811, BestLoss: 0.187580531992224, Temperature 0.029035463617657874, step_size 0.9164897823757778, test_acc: 0.1016\n",
            "Epoch 1812, BestLoss: 0.20022310647976563, Temperature 0.03056364591332408, step_size 0.9165814405198298, test_acc: 0.157\n",
            "Epoch 1813, BestLoss: 0.2120484484225729, Temperature 0.029035463617657874, step_size 0.9164897823757778, test_acc: 0.081\n",
            "Epoch 1814, BestLoss: 0.14993888626053503, Temperature 0.02758369043677498, step_size 0.9163981333975403, test_acc: 0.1097\n",
            "Epoch 1815, BestLoss: 0.14993888626053503, Temperature 0.026204505914936227, step_size 0.9163064935842006, test_acc: 0.1094\n",
            "Epoch 1816, BestLoss: 0.1877256440817626, Temperature 0.02758369043677498, step_size 0.9164897823757778, test_acc: 0.1459\n",
            "Epoch 1817, BestLoss: 0.1877256440817626, Temperature 0.026204505914936227, step_size 0.9163981333975403, test_acc: 0.1396\n",
            "Epoch 1818, BestLoss: 0.1877256440817626, Temperature 0.02758369043677498, step_size 0.9163981333975403, test_acc: 0.1352\n",
            "Epoch 1819, BestLoss: 0.15254263975571614, Temperature 0.029035463617657874, step_size 0.9163064935842006, test_acc: 0.1311\n",
            "Epoch 1820, BestLoss: 0.15254263975571614, Temperature 0.02758369043677498, step_size 0.9162148629348421, test_acc: 0.1219\n",
            "Epoch 1821, BestLoss: 0.16099842265581796, Temperature 0.029035463617657874, step_size 0.9162148629348421, test_acc: 0.0737\n",
            "Epoch 1822, BestLoss: 0.16099842265581796, Temperature 0.02758369043677498, step_size 0.9161232414485486, test_acc: 0.0715\n",
            "Epoch 1823, BestLoss: 0.16183745878402267, Temperature 0.029035463617657874, step_size 0.9161232414485486, test_acc: 0.0556\n",
            "Epoch 1824, BestLoss: 0.155387079577548, Temperature 0.02758369043677498, step_size 0.9160316291244037, test_acc: 0.1294\n",
            "Epoch 1825, BestLoss: 0.13678782433779618, Temperature 0.026204505914936227, step_size 0.9159400259614913, test_acc: 0.0631\n",
            "Epoch 1826, BestLoss: 0.13678782433779618, Temperature 0.024894280619189413, step_size 0.9158484319588951, test_acc: 0.066\n",
            "Epoch 1827, BestLoss: 0.1248360308043922, Temperature 0.026204505914936227, step_size 0.9160316291244037, test_acc: 0.1167\n",
            "Epoch 1828, BestLoss: 0.1248360308043922, Temperature 0.024894280619189413, step_size 0.9159400259614913, test_acc: 0.1205\n",
            "Epoch 1829, BestLoss: 0.12844533836730967, Temperature 0.026204505914936227, step_size 0.9159400259614913, test_acc: 0.0982\n",
            "Epoch 1830, BestLoss: 0.17855628024957382, Temperature 0.024894280619189413, step_size 0.9158484319588951, test_acc: 0.1042\n",
            "Epoch 1831, BestLoss: 0.17489769678228492, Temperature 0.02364956658822994, step_size 0.9157568471156993, test_acc: 0.0994\n",
            "Epoch 1832, BestLoss: 0.16181194049925368, Temperature 0.022467088258818442, step_size 0.9156652714309877, test_acc: 0.0797\n",
            "Epoch 1833, BestLoss: 0.16181194049925368, Temperature 0.021343733845877517, step_size 0.9155737049038447, test_acc: 0.071\n",
            "Epoch 1834, BestLoss: 0.16181194049925368, Temperature 0.022467088258818442, step_size 0.9158484319588951, test_acc: 0.0574\n",
            "Epoch 1835, BestLoss: 0.11945431280747243, Temperature 0.02364956658822994, step_size 0.9157568471156993, test_acc: 0.0222\n",
            "Epoch 1836, BestLoss: 0.1189322141449469, Temperature 0.022467088258818442, step_size 0.9156652714309877, test_acc: 0.0599\n",
            "Epoch 1837, BestLoss: 0.06946144595914179, Temperature 0.021343733845877517, step_size 0.9155737049038447, test_acc: 0.092\n",
            "Epoch 1838, BestLoss: 0.06946144595914179, Temperature 0.02027654715358364, step_size 0.9154821475333543, test_acc: 0.0889\n",
            "Epoch 1839, BestLoss: 0.06946144595914179, Temperature 0.021343733845877517, step_size 0.9156652714309877, test_acc: 0.083\n",
            "Epoch 1840, BestLoss: 0.06946144595914179, Temperature 0.022467088258818442, step_size 0.9155737049038447, test_acc: 0.0765\n",
            "Epoch 1841, BestLoss: 0.06946144595914179, Temperature 0.02364956658822994, step_size 0.9154821475333543, test_acc: 0.0758\n",
            "Epoch 1842, BestLoss: 0.06946144595914179, Temperature 0.024894280619189413, step_size 0.915390599318601, test_acc: 0.1022\n",
            "Epoch 1843, BestLoss: 0.06946144595914179, Temperature 0.026204505914936227, step_size 0.9152990602586691, test_acc: 0.1216\n",
            "Epoch 1844, BestLoss: 0.06946144595914179, Temperature 0.02758369043677498, step_size 0.9152075303526432, test_acc: 0.142\n",
            "Epoch 1845, BestLoss: 0.06946144595914179, Temperature 0.029035463617657874, step_size 0.915116009599608, test_acc: 0.1784\n",
            "Epoch 1846, BestLoss: 0.06946144595914179, Temperature 0.03056364591332408, step_size 0.915024497998648, test_acc: 0.2531\n",
            "Epoch 1847, BestLoss: 0.12928811584987807, Temperature 0.03217225885613061, step_size 0.9149329955488482, test_acc: 0.0661\n",
            "Epoch 1848, BestLoss: 0.12928811584987807, Temperature 0.03056364591332408, step_size 0.9148415022492934, test_acc: 0.0652\n",
            "Epoch 1849, BestLoss: 0.12928811584987807, Temperature 0.03217225885613061, step_size 0.9148415022492934, test_acc: 0.075\n",
            "Epoch 1850, BestLoss: 0.12928811584987807, Temperature 0.033865535638032226, step_size 0.9147500180990684, test_acc: 0.0931\n",
            "Epoch 1851, BestLoss: 0.1388202765893332, Temperature 0.03564793225056024, step_size 0.9146585430972585, test_acc: 0.1714\n",
            "Epoch 1852, BestLoss: 0.1388202765893332, Temperature 0.033865535638032226, step_size 0.9145670772429487, test_acc: 0.1635\n",
            "Epoch 1853, BestLoss: 0.14802114210811482, Temperature 0.03564793225056024, step_size 0.9145670772429487, test_acc: 0.1149\n",
            "Epoch 1854, BestLoss: 0.14802114210811482, Temperature 0.033865535638032226, step_size 0.9144756205352245, test_acc: 0.1259\n",
            "Epoch 1855, BestLoss: 0.14802114210811482, Temperature 0.03564793225056024, step_size 0.9144756205352245, test_acc: 0.1406\n",
            "Epoch 1856, BestLoss: 0.14802114210811482, Temperature 0.037524139211116046, step_size 0.914384172973171, test_acc: 0.1404\n",
            "Epoch 1857, BestLoss: 0.14802114210811482, Temperature 0.039499093906437945, step_size 0.9142927345558737, test_acc: 0.1312\n",
            "Epoch 1858, BestLoss: 0.14802114210811482, Temperature 0.04157799358572416, step_size 0.914201305282418, test_acc: 0.1316\n",
            "Epoch 1859, BestLoss: 0.09420863733884985, Temperature 0.04376630903760438, step_size 0.9141098851518898, test_acc: 0.113\n",
            "Epoch 1860, BestLoss: 0.09420863733884985, Temperature 0.04157799358572416, step_size 0.9140184741633747, test_acc: 0.1143\n",
            "Epoch 1861, BestLoss: 0.09420863733884985, Temperature 0.04376630903760438, step_size 0.9140184741633747, test_acc: 0.1267\n",
            "Epoch 1862, BestLoss: 0.13980643080355518, Temperature 0.04606979898695198, step_size 0.9139270723159584, test_acc: 0.0531\n",
            "Epoch 1863, BestLoss: 0.11589501407723762, Temperature 0.04376630903760438, step_size 0.9138356796087268, test_acc: 0.1045\n",
            "Epoch 1864, BestLoss: 0.11589501407723762, Temperature 0.04157799358572416, step_size 0.9137442960407659, test_acc: 0.1084\n",
            "Epoch 1865, BestLoss: 0.16151686243431881, Temperature 0.04376630903760438, step_size 0.9138356796087268, test_acc: 0.0852\n",
            "Epoch 1866, BestLoss: 0.19771691935042837, Temperature 0.04157799358572416, step_size 0.9137442960407659, test_acc: 0.1621\n",
            "Epoch 1867, BestLoss: 0.19085029518600657, Temperature 0.039499093906437945, step_size 0.9136529216111619, test_acc: 0.1761\n",
            "Epoch 1868, BestLoss: 0.1657635700309167, Temperature 0.037524139211116046, step_size 0.9135615563190007, test_acc: 0.1585\n",
            "Epoch 1869, BestLoss: 0.16458464624125468, Temperature 0.03564793225056024, step_size 0.9134702001633688, test_acc: 0.1036\n",
            "Epoch 1870, BestLoss: 0.17694315871696942, Temperature 0.033865535638032226, step_size 0.9133788531433524, test_acc: 0.0852\n",
            "Epoch 1871, BestLoss: 0.19164614586730633, Temperature 0.03217225885613061, step_size 0.9132875152580381, test_acc: 0.1094\n",
            "Epoch 1872, BestLoss: 0.19164614586730633, Temperature 0.03056364591332408, step_size 0.9131961865065122, test_acc: 0.1137\n",
            "Epoch 1873, BestLoss: 0.15391623084746509, Temperature 0.03217225885613061, step_size 0.9137442960407659, test_acc: 0.0849\n",
            "Epoch 1874, BestLoss: 0.15391623084746509, Temperature 0.03056364591332408, step_size 0.9136529216111619, test_acc: 0.0925\n",
            "Epoch 1875, BestLoss: 0.19993435045958124, Temperature 0.03217225885613061, step_size 0.9136529216111619, test_acc: 0.0848\n",
            "Epoch 1876, BestLoss: 0.21152264841559248, Temperature 0.03056364591332408, step_size 0.9135615563190007, test_acc: 0.0734\n",
            "Epoch 1877, BestLoss: 0.23697290202967808, Temperature 0.029035463617657874, step_size 0.9134702001633688, test_acc: 0.0995\n",
            "Epoch 1878, BestLoss: 0.21975440650054093, Temperature 0.02758369043677498, step_size 0.9133788531433524, test_acc: 0.1655\n",
            "Epoch 1879, BestLoss: 0.21975440650054093, Temperature 0.026204505914936227, step_size 0.9132875152580381, test_acc: 0.1698\n",
            "Epoch 1880, BestLoss: 0.20555348899469555, Temperature 0.02758369043677498, step_size 0.9135615563190007, test_acc: 0.1621\n",
            "Epoch 1881, BestLoss: 0.22191277700178635, Temperature 0.026204505914936227, step_size 0.9134702001633688, test_acc: 0.0655\n",
            "Epoch 1882, BestLoss: 0.14528389722970508, Temperature 0.024894280619189413, step_size 0.9133788531433524, test_acc: 0.0524\n",
            "Epoch 1883, BestLoss: 0.14528389722970508, Temperature 0.02364956658822994, step_size 0.9132875152580381, test_acc: 0.0509\n",
            "Epoch 1884, BestLoss: 0.14528389722970508, Temperature 0.024894280619189413, step_size 0.9134702001633688, test_acc: 0.0499\n",
            "Epoch 1885, BestLoss: 0.14528389722970508, Temperature 0.026204505914936227, step_size 0.9133788531433524, test_acc: 0.0505\n",
            "Epoch 1886, BestLoss: 0.1552540631178594, Temperature 0.02758369043677498, step_size 0.9132875152580381, test_acc: 0.0707\n",
            "Epoch 1887, BestLoss: 0.14458884209451944, Temperature 0.026204505914936227, step_size 0.9131961865065122, test_acc: 0.0494\n",
            "Epoch 1888, BestLoss: 0.14458884209451944, Temperature 0.024894280619189413, step_size 0.9131048668878616, test_acc: 0.0599\n",
            "Epoch 1889, BestLoss: 0.08498964143051639, Temperature 0.026204505914936227, step_size 0.9131961865065122, test_acc: 0.0665\n",
            "Epoch 1890, BestLoss: 0.08498964143051639, Temperature 0.024894280619189413, step_size 0.9131048668878616, test_acc: 0.072\n",
            "Epoch 1891, BestLoss: 0.08498964143051639, Temperature 0.026204505914936227, step_size 0.9131048668878616, test_acc: 0.0861\n",
            "Epoch 1892, BestLoss: 0.14265463986662516, Temperature 0.02758369043677498, step_size 0.9130135564011729, test_acc: 0.0642\n",
            "Epoch 1893, BestLoss: 0.14265463986662516, Temperature 0.026204505914936227, step_size 0.9129222550455328, test_acc: 0.0714\n",
            "Epoch 1894, BestLoss: 0.14265463986662516, Temperature 0.02758369043677498, step_size 0.9129222550455328, test_acc: 0.0923\n",
            "Epoch 1895, BestLoss: 0.14265463986662516, Temperature 0.029035463617657874, step_size 0.9128309628200283, test_acc: 0.1277\n",
            "Epoch 1896, BestLoss: 0.12134813449045809, Temperature 0.03056364591332408, step_size 0.9127396797237463, test_acc: 0.0974\n",
            "Epoch 1897, BestLoss: 0.12134813449045809, Temperature 0.029035463617657874, step_size 0.9126484057557739, test_acc: 0.0994\n",
            "Epoch 1898, BestLoss: 0.1440389891852445, Temperature 0.03056364591332408, step_size 0.9126484057557739, test_acc: 0.1136\n",
            "Epoch 1899, BestLoss: 0.12194415362517223, Temperature 0.029035463617657874, step_size 0.9125571409151984, test_acc: 0.102\n",
            "Epoch 1900, BestLoss: 0.12194415362517223, Temperature 0.02758369043677498, step_size 0.9124658852011068, test_acc: 0.1077\n",
            "Epoch 1901, BestLoss: 0.1139947767040473, Temperature 0.029035463617657874, step_size 0.9125571409151984, test_acc: 0.1014\n",
            "Epoch 1902, BestLoss: 0.1139947767040473, Temperature 0.02758369043677498, step_size 0.9124658852011068, test_acc: 0.1049\n",
            "Epoch 1903, BestLoss: 0.1139947767040473, Temperature 0.029035463617657874, step_size 0.9124658852011068, test_acc: 0.1086\n",
            "Epoch 1904, BestLoss: 0.1139947767040473, Temperature 0.03056364591332408, step_size 0.9123746386125867, test_acc: 0.1121\n",
            "Epoch 1905, BestLoss: 0.1139947767040473, Temperature 0.03217225885613061, step_size 0.9122834011487255, test_acc: 0.1192\n",
            "Epoch 1906, BestLoss: 0.1139947767040473, Temperature 0.033865535638032226, step_size 0.9121921728086106, test_acc: 0.1323\n",
            "Epoch 1907, BestLoss: 0.1139947767040473, Temperature 0.03564793225056024, step_size 0.9121009535913298, test_acc: 0.1551\n",
            "Epoch 1908, BestLoss: 0.24492484515564797, Temperature 0.037524139211116046, step_size 0.9120097434959706, test_acc: 0.1732\n",
            "Epoch 1909, BestLoss: 0.23604800366415352, Temperature 0.03564793225056024, step_size 0.9119185425216211, test_acc: 0.0972\n",
            "Epoch 1910, BestLoss: 0.25497280090171565, Temperature 0.033865535638032226, step_size 0.911827350667369, test_acc: 0.0916\n",
            "Epoch 1911, BestLoss: 0.1850999609191795, Temperature 0.03217225885613061, step_size 0.9117361679323023, test_acc: 0.0806\n",
            "Epoch 1912, BestLoss: 0.1949227707073481, Temperature 0.03056364591332408, step_size 0.911644994315509, test_acc: 0.0615\n",
            "Epoch 1913, BestLoss: 0.17098458582950343, Temperature 0.029035463617657874, step_size 0.9115538298160775, test_acc: 0.0548\n",
            "Epoch 1914, BestLoss: 0.15751481881608412, Temperature 0.02758369043677498, step_size 0.9114626744330959, test_acc: 0.1035\n",
            "Epoch 1915, BestLoss: 0.1565445310653421, Temperature 0.026204505914936227, step_size 0.9113715281656526, test_acc: 0.0865\n",
            "Epoch 1916, BestLoss: 0.1565445310653421, Temperature 0.024894280619189413, step_size 0.9112803910128361, test_acc: 0.0864\n",
            "Epoch 1917, BestLoss: 0.1565445310653421, Temperature 0.026204505914936227, step_size 0.9119185425216211, test_acc: 0.0804\n",
            "Epoch 1918, BestLoss: 0.1565445310653421, Temperature 0.02758369043677498, step_size 0.911827350667369, test_acc: 0.0718\n",
            "Epoch 1919, BestLoss: 0.1565445310653421, Temperature 0.029035463617657874, step_size 0.9117361679323023, test_acc: 0.0855\n",
            "Epoch 1920, BestLoss: 0.14368709595741633, Temperature 0.03056364591332408, step_size 0.911644994315509, test_acc: 0.1654\n",
            "Epoch 1921, BestLoss: 0.14368709595741633, Temperature 0.029035463617657874, step_size 0.9115538298160775, test_acc: 0.1478\n",
            "Epoch 1922, BestLoss: 0.14368709595741633, Temperature 0.03056364591332408, step_size 0.9115538298160775, test_acc: 0.1257\n",
            "Epoch 1923, BestLoss: 0.19045908722043853, Temperature 0.03217225885613061, step_size 0.9114626744330959, test_acc: 0.1063\n",
            "Epoch 1924, BestLoss: 0.23029171949827315, Temperature 0.03056364591332408, step_size 0.9113715281656526, test_acc: 0.0833\n",
            "Epoch 1925, BestLoss: 0.23098857145776414, Temperature 0.029035463617657874, step_size 0.9112803910128361, test_acc: 0.1095\n",
            "Epoch 1926, BestLoss: 0.20554650794115456, Temperature 0.02758369043677498, step_size 0.9111892629737348, test_acc: 0.1214\n",
            "Epoch 1927, BestLoss: 0.17158259721669447, Temperature 0.026204505914936227, step_size 0.9110981440474375, test_acc: 0.1088\n",
            "Epoch 1928, BestLoss: 0.17158259721669447, Temperature 0.024894280619189413, step_size 0.9110070342330328, test_acc: 0.1114\n",
            "Epoch 1929, BestLoss: 0.17158259721669447, Temperature 0.026204505914936227, step_size 0.9113715281656526, test_acc: 0.1116\n",
            "Epoch 1930, BestLoss: 0.17158259721669447, Temperature 0.02758369043677498, step_size 0.9112803910128361, test_acc: 0.117\n",
            "Epoch 1931, BestLoss: 0.17158259721669447, Temperature 0.029035463617657874, step_size 0.9111892629737348, test_acc: 0.1283\n",
            "Epoch 1932, BestLoss: 0.17158259721669447, Temperature 0.03056364591332408, step_size 0.9110981440474375, test_acc: 0.1348\n",
            "Epoch 1933, BestLoss: 0.17158259721669447, Temperature 0.03217225885613061, step_size 0.9110070342330328, test_acc: 0.1398\n",
            "Epoch 1934, BestLoss: 0.1978653529357415, Temperature 0.033865535638032226, step_size 0.9109159335296094, test_acc: 0.1665\n",
            "Epoch 1935, BestLoss: 0.15988175910884495, Temperature 0.03217225885613061, step_size 0.9108248419362565, test_acc: 0.1136\n",
            "Epoch 1936, BestLoss: 0.16279199522742982, Temperature 0.03056364591332408, step_size 0.9107337594520628, test_acc: 0.1138\n",
            "Epoch 1937, BestLoss: 0.1126881718645809, Temperature 0.029035463617657874, step_size 0.9106426860761176, test_acc: 0.1591\n",
            "Epoch 1938, BestLoss: 0.1126881718645809, Temperature 0.02758369043677498, step_size 0.91055162180751, test_acc: 0.1496\n",
            "Epoch 1939, BestLoss: 0.11931870135504456, Temperature 0.029035463617657874, step_size 0.9108248419362565, test_acc: 0.0853\n",
            "Epoch 1940, BestLoss: 0.11931870135504456, Temperature 0.02758369043677498, step_size 0.9107337594520628, test_acc: 0.076\n",
            "Epoch 1941, BestLoss: 0.12057829506192136, Temperature 0.029035463617657874, step_size 0.9107337594520628, test_acc: 0.0755\n",
            "Epoch 1942, BestLoss: 0.12057829506192136, Temperature 0.02758369043677498, step_size 0.9106426860761176, test_acc: 0.0796\n",
            "Epoch 1943, BestLoss: 0.12916400796602528, Temperature 0.029035463617657874, step_size 0.9106426860761176, test_acc: 0.1165\n",
            "Epoch 1944, BestLoss: 0.16510507682521314, Temperature 0.02758369043677498, step_size 0.91055162180751, test_acc: 0.1335\n",
            "Epoch 1945, BestLoss: 0.16510507682521314, Temperature 0.026204505914936227, step_size 0.9104605666453293, test_acc: 0.1373\n",
            "Epoch 1946, BestLoss: 0.16510507682521314, Temperature 0.02758369043677498, step_size 0.91055162180751, test_acc: 0.1426\n",
            "Epoch 1947, BestLoss: 0.16510507682521314, Temperature 0.029035463617657874, step_size 0.9104605666453293, test_acc: 0.149\n",
            "Epoch 1948, BestLoss: 0.16510507682521314, Temperature 0.03056364591332408, step_size 0.9103695205886647, test_acc: 0.1503\n",
            "Epoch 1949, BestLoss: 0.11785368994332777, Temperature 0.03217225885613061, step_size 0.9102784836366059, test_acc: 0.1277\n",
            "Epoch 1950, BestLoss: 0.11785368994332777, Temperature 0.03056364591332408, step_size 0.9101874557882422, test_acc: 0.1263\n",
            "Epoch 1951, BestLoss: 0.11785368994332777, Temperature 0.03217225885613061, step_size 0.9101874557882422, test_acc: 0.129\n",
            "Epoch 1952, BestLoss: 0.11785368994332777, Temperature 0.033865535638032226, step_size 0.9100964370426634, test_acc: 0.1216\n",
            "Epoch 1953, BestLoss: 0.11615994849589445, Temperature 0.03564793225056024, step_size 0.9100054273989592, test_acc: 0.0627\n",
            "Epoch 1954, BestLoss: 0.16336951054277973, Temperature 0.033865535638032226, step_size 0.9099144268562193, test_acc: 0.0685\n",
            "Epoch 1955, BestLoss: 0.1414967139270526, Temperature 0.03217225885613061, step_size 0.9098234354135336, test_acc: 0.0527\n",
            "Epoch 1956, BestLoss: 0.14956947649213204, Temperature 0.03056364591332408, step_size 0.9097324530699923, test_acc: 0.06\n",
            "Epoch 1957, BestLoss: 0.12837060493228789, Temperature 0.029035463617657874, step_size 0.9096414798246854, test_acc: 0.0441\n",
            "Epoch 1958, BestLoss: 0.1109076841814782, Temperature 0.02758369043677498, step_size 0.9095505156767029, test_acc: 0.1042\n",
            "Epoch 1959, BestLoss: 0.1109076841814782, Temperature 0.026204505914936227, step_size 0.9094595606251352, test_acc: 0.1065\n",
            "Epoch 1960, BestLoss: 0.08561578143597705, Temperature 0.02758369043677498, step_size 0.9099144268562193, test_acc: 0.1073\n",
            "Epoch 1961, BestLoss: 0.08561578143597705, Temperature 0.026204505914936227, step_size 0.9098234354135336, test_acc: 0.1097\n",
            "Epoch 1962, BestLoss: 0.08561578143597705, Temperature 0.02758369043677498, step_size 0.9098234354135336, test_acc: 0.1068\n",
            "Epoch 1963, BestLoss: 0.08561578143597705, Temperature 0.029035463617657874, step_size 0.9097324530699923, test_acc: 0.1086\n",
            "Epoch 1964, BestLoss: 0.08561578143597705, Temperature 0.03056364591332408, step_size 0.9096414798246854, test_acc: 0.1073\n",
            "Epoch 1965, BestLoss: 0.08561578143597705, Temperature 0.03217225885613061, step_size 0.9095505156767029, test_acc: 0.1139\n",
            "Epoch 1966, BestLoss: 0.08561578143597705, Temperature 0.033865535638032226, step_size 0.9094595606251352, test_acc: 0.1277\n",
            "Epoch 1967, BestLoss: 0.13287676125187897, Temperature 0.03564793225056024, step_size 0.9093686146690727, test_acc: 0.1189\n",
            "Epoch 1968, BestLoss: 0.15142243501696587, Temperature 0.033865535638032226, step_size 0.9092776778076058, test_acc: 0.061\n",
            "Epoch 1969, BestLoss: 0.1966242774082862, Temperature 0.03217225885613061, step_size 0.9091867500398251, test_acc: 0.0931\n",
            "Epoch 1970, BestLoss: 0.21522049623266823, Temperature 0.03056364591332408, step_size 0.9090958313648211, test_acc: 0.1358\n",
            "Epoch 1971, BestLoss: 0.1645729685422261, Temperature 0.029035463617657874, step_size 0.9090049217816846, test_acc: 0.1237\n",
            "Epoch 1972, BestLoss: 0.1645729685422261, Temperature 0.02758369043677498, step_size 0.9089140212895064, test_acc: 0.1285\n",
            "Epoch 1973, BestLoss: 0.17168053217188278, Temperature 0.029035463617657874, step_size 0.9092776778076058, test_acc: 0.1666\n",
            "Epoch 1974, BestLoss: 0.14022986189870557, Temperature 0.02758369043677498, step_size 0.9091867500398251, test_acc: 0.0934\n",
            "Epoch 1975, BestLoss: 0.14022986189870557, Temperature 0.026204505914936227, step_size 0.9090958313648211, test_acc: 0.0931\n",
            "Epoch 1976, BestLoss: 0.14022986189870557, Temperature 0.02758369043677498, step_size 0.9091867500398251, test_acc: 0.0977\n",
            "Epoch 1977, BestLoss: 0.14022986189870557, Temperature 0.029035463617657874, step_size 0.9090958313648211, test_acc: 0.1094\n",
            "Epoch 1978, BestLoss: 0.14022986189870557, Temperature 0.03056364591332408, step_size 0.9090049217816846, test_acc: 0.1279\n",
            "Epoch 1979, BestLoss: 0.18634603180937118, Temperature 0.03217225885613061, step_size 0.9089140212895064, test_acc: 0.1147\n",
            "Epoch 1980, BestLoss: 0.14436387492097874, Temperature 0.03056364591332408, step_size 0.9088231298873775, test_acc: 0.1293\n",
            "Epoch 1981, BestLoss: 0.14436387492097874, Temperature 0.029035463617657874, step_size 0.9087322475743888, test_acc: 0.1311\n",
            "Epoch 1982, BestLoss: 0.14436387492097874, Temperature 0.03056364591332408, step_size 0.9088231298873775, test_acc: 0.1335\n",
            "Epoch 1983, BestLoss: 0.14436387492097874, Temperature 0.03217225885613061, step_size 0.9087322475743888, test_acc: 0.1364\n",
            "Epoch 1984, BestLoss: 0.14436387492097874, Temperature 0.033865535638032226, step_size 0.9086413743496313, test_acc: 0.1391\n",
            "Epoch 1985, BestLoss: 0.14436387492097874, Temperature 0.03564793225056024, step_size 0.9085505102121963, test_acc: 0.1415\n",
            "Epoch 1986, BestLoss: 0.14436387492097874, Temperature 0.037524139211116046, step_size 0.9084596551611751, test_acc: 0.1536\n",
            "Epoch 1987, BestLoss: 0.19955238975548295, Temperature 0.039499093906437945, step_size 0.908368809195659, test_acc: 0.0964\n",
            "Epoch 1988, BestLoss: 0.16427162898222805, Temperature 0.037524139211116046, step_size 0.9082779723147394, test_acc: 0.0972\n",
            "Epoch 1989, BestLoss: 0.16427162898222805, Temperature 0.03564793225056024, step_size 0.9081871445175079, test_acc: 0.0969\n",
            "Epoch 1990, BestLoss: 0.16427162898222805, Temperature 0.037524139211116046, step_size 0.9082779723147394, test_acc: 0.0963\n",
            "Epoch 1991, BestLoss: 0.1393383624843113, Temperature 0.039499093906437945, step_size 0.9081871445175079, test_acc: 0.1042\n",
            "Epoch 1992, BestLoss: 0.1393383624843113, Temperature 0.037524139211116046, step_size 0.9080963258030562, test_acc: 0.1106\n",
            "Epoch 1993, BestLoss: 0.15574005309716946, Temperature 0.039499093906437945, step_size 0.9080963258030562, test_acc: 0.1122\n",
            "Epoch 1994, BestLoss: 0.1794436723576145, Temperature 0.037524139211116046, step_size 0.9080055161704759, test_acc: 0.0909\n",
            "Epoch 1995, BestLoss: 0.20212961892451303, Temperature 0.03564793225056024, step_size 0.9079147156188588, test_acc: 0.1012\n",
            "Epoch 1996, BestLoss: 0.20212961892451303, Temperature 0.033865535638032226, step_size 0.907823924147297, test_acc: 0.1133\n",
            "Epoch 1997, BestLoss: 0.14412937475123347, Temperature 0.03564793225056024, step_size 0.9080055161704759, test_acc: 0.1347\n",
            "Epoch 1998, BestLoss: 0.18466960438523472, Temperature 0.033865535638032226, step_size 0.9079147156188588, test_acc: 0.1463\n",
            "Epoch 1999, BestLoss: 0.18818286736574744, Temperature 0.03217225885613061, step_size 0.907823924147297, test_acc: 0.1079\n",
            "Epoch 2000, BestLoss: 0.17458785837399943, Temperature 0.03056364591332408, step_size 0.9077331417548823, test_acc: 0.0823\n",
            "Epoch 2001, BestLoss: 0.16457420382215449, Temperature 0.029035463617657874, step_size 0.9076423684407068, test_acc: 0.0858\n",
            "Epoch 2002, BestLoss: 0.16457420382215449, Temperature 0.02758369043677498, step_size 0.9075516042038627, test_acc: 0.0801\n",
            "Epoch 2003, BestLoss: 0.1908581957053924, Temperature 0.029035463617657874, step_size 0.9079147156188588, test_acc: 0.0364\n",
            "Epoch 2004, BestLoss: 0.196490959073253, Temperature 0.02758369043677498, step_size 0.907823924147297, test_acc: 0.0346\n",
            "Epoch 2005, BestLoss: 0.196490959073253, Temperature 0.026204505914936227, step_size 0.9077331417548823, test_acc: 0.0368\n",
            "Epoch 2006, BestLoss: 0.196490959073253, Temperature 0.02758369043677498, step_size 0.907823924147297, test_acc: 0.0398\n",
            "Epoch 2007, BestLoss: 0.19924512863535748, Temperature 0.029035463617657874, step_size 0.9077331417548823, test_acc: 0.0874\n",
            "Epoch 2008, BestLoss: 0.19924512863535748, Temperature 0.02758369043677498, step_size 0.9076423684407068, test_acc: 0.0926\n",
            "Epoch 2009, BestLoss: 0.20233425638146155, Temperature 0.029035463617657874, step_size 0.9076423684407068, test_acc: 0.1186\n",
            "Epoch 2010, BestLoss: 0.1809796506136708, Temperature 0.02758369043677498, step_size 0.9075516042038627, test_acc: 0.1013\n",
            "Epoch 2011, BestLoss: 0.15131390381398235, Temperature 0.026204505914936227, step_size 0.9074608490434424, test_acc: 0.1194\n",
            "Epoch 2012, BestLoss: 0.16608217560960697, Temperature 0.024894280619189413, step_size 0.907370102958538, test_acc: 0.0721\n",
            "Epoch 2013, BestLoss: 0.16608217560960697, Temperature 0.02364956658822994, step_size 0.9072793659482422, test_acc: 0.0698\n",
            "Epoch 2014, BestLoss: 0.19280146789615182, Temperature 0.024894280619189413, step_size 0.9075516042038627, test_acc: 0.0907\n",
            "Epoch 2015, BestLoss: 0.22766537953918736, Temperature 0.02364956658822994, step_size 0.9074608490434424, test_acc: 0.0889\n",
            "Epoch 2016, BestLoss: 0.19363391625978546, Temperature 0.022467088258818442, step_size 0.907370102958538, test_acc: 0.1086\n",
            "Epoch 2017, BestLoss: 0.1918494911178955, Temperature 0.021343733845877517, step_size 0.9072793659482422, test_acc: 0.0776\n",
            "Epoch 2018, BestLoss: 0.1918494911178955, Temperature 0.02027654715358364, step_size 0.9071886380116474, test_acc: 0.075\n",
            "Epoch 2019, BestLoss: 0.17976364765695257, Temperature 0.021343733845877517, step_size 0.9074608490434424, test_acc: 0.1523\n",
            "Epoch 2020, BestLoss: 0.17976364765695257, Temperature 0.02027654715358364, step_size 0.907370102958538, test_acc: 0.143\n",
            "Epoch 2021, BestLoss: 0.19550111320352123, Temperature 0.021343733845877517, step_size 0.907370102958538, test_acc: 0.1201\n",
            "Epoch 2022, BestLoss: 0.19550111320352123, Temperature 0.02027654715358364, step_size 0.9072793659482422, test_acc: 0.1183\n",
            "Epoch 2023, BestLoss: 0.19550111320352123, Temperature 0.021343733845877517, step_size 0.9072793659482422, test_acc: 0.1161\n",
            "Epoch 2024, BestLoss: 0.19550111320352123, Temperature 0.022467088258818442, step_size 0.9071886380116474, test_acc: 0.1199\n",
            "Epoch 2025, BestLoss: 0.20589180300805457, Temperature 0.02364956658822994, step_size 0.9070979191478462, test_acc: 0.0684\n",
            "Epoch 2026, BestLoss: 0.1788658554236939, Temperature 0.022467088258818442, step_size 0.9070072093559315, test_acc: 0.1374\n",
            "Epoch 2027, BestLoss: 0.1788658554236939, Temperature 0.021343733845877517, step_size 0.906916508634996, test_acc: 0.1308\n",
            "Epoch 2028, BestLoss: 0.1788658554236939, Temperature 0.022467088258818442, step_size 0.9070072093559315, test_acc: 0.1264\n",
            "Epoch 2029, BestLoss: 0.18307290029069556, Temperature 0.02364956658822994, step_size 0.906916508634996, test_acc: 0.1174\n",
            "Epoch 2030, BestLoss: 0.18307290029069556, Temperature 0.022467088258818442, step_size 0.9068258169841324, test_acc: 0.1171\n",
            "Epoch 2031, BestLoss: 0.18474145314236814, Temperature 0.02364956658822994, step_size 0.9068258169841324, test_acc: 0.1505\n",
            "Epoch 2032, BestLoss: 0.1698271945295248, Temperature 0.022467088258818442, step_size 0.906735134402434, test_acc: 0.2017\n",
            "Epoch 2033, BestLoss: 0.17713264943888635, Temperature 0.021343733845877517, step_size 0.9066444608889938, test_acc: 0.1464\n",
            "Epoch 2034, BestLoss: 0.17713264943888635, Temperature 0.02027654715358364, step_size 0.9065537964429049, test_acc: 0.1426\n",
            "Epoch 2035, BestLoss: 0.17713264943888635, Temperature 0.021343733845877517, step_size 0.906735134402434, test_acc: 0.1417\n",
            "Epoch 2036, BestLoss: 0.17713264943888635, Temperature 0.022467088258818442, step_size 0.9066444608889938, test_acc: 0.1435\n",
            "Epoch 2037, BestLoss: 0.17713264943888635, Temperature 0.02364956658822994, step_size 0.9065537964429049, test_acc: 0.1469\n",
            "Epoch 2038, BestLoss: 0.17427286765365735, Temperature 0.024894280619189413, step_size 0.9064631410632606, test_acc: 0.1806\n",
            "Epoch 2039, BestLoss: 0.14499188425659665, Temperature 0.02364956658822994, step_size 0.9063724947491544, test_acc: 0.1189\n",
            "Epoch 2040, BestLoss: 0.14499188425659665, Temperature 0.022467088258818442, step_size 0.9062818574996795, test_acc: 0.1119\n",
            "Epoch 2041, BestLoss: 0.15160683119529655, Temperature 0.02364956658822994, step_size 0.9063724947491544, test_acc: 0.0664\n",
            "Epoch 2042, BestLoss: 0.15160683119529655, Temperature 0.022467088258818442, step_size 0.9062818574996795, test_acc: 0.0606\n",
            "Epoch 2043, BestLoss: 0.15160683119529655, Temperature 0.02364956658822994, step_size 0.9062818574996795, test_acc: 0.0618\n",
            "Epoch 2044, BestLoss: 0.18854188803985608, Temperature 0.024894280619189413, step_size 0.9061912293139295, test_acc: 0.0959\n",
            "Epoch 2045, BestLoss: 0.1890474866778894, Temperature 0.02364956658822994, step_size 0.9061006101909981, test_acc: 0.1132\n",
            "Epoch 2046, BestLoss: 0.23700989591797023, Temperature 0.022467088258818442, step_size 0.906010000129979, test_acc: 0.1309\n",
            "Epoch 2047, BestLoss: 0.23700989591797023, Temperature 0.021343733845877517, step_size 0.905919399129966, test_acc: 0.1301\n",
            "Epoch 2048, BestLoss: 0.22121127488626235, Temperature 0.022467088258818442, step_size 0.9061006101909981, test_acc: 0.0987\n",
            "Epoch 2049, BestLoss: 0.1741050275882429, Temperature 0.021343733845877517, step_size 0.906010000129979, test_acc: 0.0865\n",
            "Epoch 2050, BestLoss: 0.18006353768007452, Temperature 0.02027654715358364, step_size 0.905919399129966, test_acc: 0.066\n",
            "Epoch 2051, BestLoss: 0.15084438470486256, Temperature 0.019262719795904458, step_size 0.905828807190053, test_acc: 0.0562\n",
            "Epoch 2052, BestLoss: 0.13594545111597614, Temperature 0.018299583806109233, step_size 0.905738224309334, test_acc: 0.0796\n",
            "Epoch 2053, BestLoss: 0.17020694582158108, Temperature 0.01738460461580377, step_size 0.9056476504869031, test_acc: 0.0924\n",
            "Epoch 2054, BestLoss: 0.15664193593088815, Temperature 0.016515374385013583, step_size 0.9055570857218544, test_acc: 0.1896\n",
            "Epoch 2055, BestLoss: 0.15322780229851785, Temperature 0.015689605665762902, step_size 0.9054665300132823, test_acc: 0.1508\n",
            "Epoch 2056, BestLoss: 0.15983005739233747, Temperature 0.014905125382474757, step_size 0.9053759833602809, test_acc: 0.1419\n",
            "Epoch 2057, BestLoss: 0.15983005739233747, Temperature 0.014159869113351018, step_size 0.9052854457619449, test_acc: 0.1346\n",
            "Epoch 2058, BestLoss: 0.15983005739233747, Temperature 0.014905125382474757, step_size 0.906010000129979, test_acc: 0.1223\n",
            "Epoch 2059, BestLoss: 0.15983005739233747, Temperature 0.015689605665762902, step_size 0.905919399129966, test_acc: 0.0998\n",
            "Epoch 2060, BestLoss: 0.15681587995423316, Temperature 0.016515374385013583, step_size 0.905828807190053, test_acc: 0.1408\n",
            "Epoch 2061, BestLoss: 0.15681587995423316, Temperature 0.015689605665762902, step_size 0.905738224309334, test_acc: 0.1495\n",
            "Epoch 2062, BestLoss: 0.15681587995423316, Temperature 0.016515374385013583, step_size 0.905738224309334, test_acc: 0.1601\n",
            "Epoch 2063, BestLoss: 0.15764580060996303, Temperature 0.01738460461580377, step_size 0.9056476504869031, test_acc: 0.1124\n",
            "Epoch 2064, BestLoss: 0.1676330148946808, Temperature 0.016515374385013583, step_size 0.9055570857218544, test_acc: 0.1789\n",
            "Epoch 2065, BestLoss: 0.1676330148946808, Temperature 0.015689605665762902, step_size 0.9054665300132823, test_acc: 0.1696\n",
            "Epoch 2066, BestLoss: 0.15072842843189216, Temperature 0.016515374385013583, step_size 0.9055570857218544, test_acc: 0.087\n",
            "Epoch 2067, BestLoss: 0.15072842843189216, Temperature 0.015689605665762902, step_size 0.9054665300132823, test_acc: 0.0852\n",
            "Epoch 2068, BestLoss: 0.15072842843189216, Temperature 0.016515374385013583, step_size 0.9054665300132823, test_acc: 0.095\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[84], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m net \u001b[38;5;241m=\u001b[39m NeuralNet(layers)\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m ELM_SA_Optimizer(net, learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, loss\u001b[38;5;241m=\u001b[39mMeanSquaredError())\n\u001b[0;32m---> 16\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcy_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcy_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcooling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Infer on cx_test, cy_test\u001b[39;00m\n\u001b[1;32m     18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(cx_test)\n",
            "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mOptimizer.fit\u001b[0;34m(self, train_data, test_data, epochs, batch_size, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     x_batch \u001b[38;5;241m=\u001b[39m x_train[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]\n\u001b[1;32m     26\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_train[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]\n\u001b[0;32m---> 28\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     31\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39maccuracy(x_test, y_test)\n",
            "Cell \u001b[0;32mIn[82], line 15\u001b[0m, in \u001b[0;36mELM_SA_Optimizer.train_step\u001b[0;34m(self, x_batch, y_batch, alpha)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_layers()):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Dense):\n\u001b[0;32m---> 15\u001b[0m         x_inv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         weight_approx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x_inv, expected)\n\u001b[1;32m     17\u001b[0m         layer\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m+\u001b[39m weight_approx \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cupy/linalg/_solve.py:368\u001b[0m, in \u001b[0;36mpinv\u001b[0;34m(a, rcond)\u001b[0m\n\u001b[1;32m    366\u001b[0m leq \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m cutoff[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    367\u001b[0m cupy\u001b[38;5;241m.\u001b[39mreciprocal(s, out\u001b[38;5;241m=\u001b[39ms)\n\u001b[0;32m--> 368\u001b[0m \u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleq\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cupy\u001b[38;5;241m.\u001b[39mmatmul(vt\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), s[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m u\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[0;32mcupy/_core/core.pyx:1588\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__setitem__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mcupy/_core/_routines_indexing.pyx:50\u001b[0m, in \u001b[0;36mcupy._core._routines_indexing._ndarray_setitem\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mcupy/_core/_routines_indexing.pyx:1002\u001b[0m, in \u001b[0;36mcupy._core._routines_indexing._scatter_op\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mcupy/_core/_routines_indexing.pyx:968\u001b[0m, in \u001b[0;36mcupy._core._routines_indexing._scatter_op_mask_single\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mcupy/_core/_routines_manipulation.pyx:477\u001b[0m, in \u001b[0;36mcupy._core._routines_manipulation.broadcast_to\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:387\u001b[0m, in \u001b[0;36miterable\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03mCheck whether or not an object can be iterated over.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m \n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# MNIST\n",
        "np.random.seed(4)\n",
        "layers = [\n",
        "    SADense(784, 128, False),\n",
        "    # LeakyReLU(),\n",
        "    # Linear(),\n",
        "    # Dense(128, 128, False),\n",
        "    # LeakyReLU(),\n",
        "    SADense(128, 10, False),\n",
        "    Sigmoid(),\n",
        "]\n",
        "\n",
        "net = NeuralNet(layers)\n",
        "optimizer = ELM_SA_Optimizer(net, learning_rate = 1, loss=MeanSquaredError())\n",
        "\n",
        "optimizer.fit((cx_train, cy_train), (cx_test, cy_test), epochs=10000, batch_size=60000, sample_per_batch=1, initial_temp=1.0, cooling_rate=0.95, verbose=True)\n",
        "# Infer on cx_test, cy_test\n",
        "predictions = net.forward(cx_test)\n",
        "preds = np.array(antiCategorical(predictions))\n",
        "expected = np.array(antiCategorical(cy_test))\n",
        "\n",
        "print(f'Accuracy: {sklearn.metrics.accuracy_score(expected.get(), preds.get())}')\n",
        "print(f'Confusion Matrix: {sklearn.metrics.confusion_matrix(expected.get(), preds.get())}')\n",
        "print(f'Classification Report: {sklearn.metrics.classification_report(expected.get(), preds.get())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqA2JSPShvah"
      },
      "source": [
        "# K-BFGS Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_g0rlkJ5MPZ"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def forward(self, inputs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, n_input, n_output):\n",
        "        self.weights = np.random.randn(n_input, n_output) * np.sqrt(2.0 / n_input)\n",
        "        self.biases = np.zeros((1, n_output))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "        return self.output\n",
        "\n",
        "    def set_weights_biases(self, weights, biases):\n",
        "        self.weights = weights\n",
        "        self.biases = biases\n",
        "\n",
        "    def get_params(self):\n",
        "        return np.concatenate([self.weights.flatten(), self.biases.flatten()])\n",
        "\n",
        "    def set_params(self, params):\n",
        "        weight_size = self.weights.size\n",
        "        self.weights = params[:weight_size].reshape(self.weights.shape)\n",
        "        self.biases = params[weight_size:].reshape(self.biases.shape)\n",
        "\n",
        "class Sigmoid:\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(x):\n",
        "        sigmoid = Sigmoid.forward(x)\n",
        "        return sigmoid * (1 - sigmoid)\n",
        "\n",
        "class ReLU:\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "class Softmax:\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(x):\n",
        "        pass\n",
        "\n",
        "class MeanSquaredError:\n",
        "    @staticmethod\n",
        "    def forward(predictions, targets):\n",
        "        return np.mean((predictions - targets)**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(predictions, targets):\n",
        "        return 2 * (predictions - targets) / predictions.size\n",
        "\n",
        "class NeuralNet:\n",
        "    def __init__(self, layers, loss):\n",
        "        self.layers = layers\n",
        "        self.loss = loss\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        for layer in self.layers:\n",
        "            inputs = layer.forward(inputs)\n",
        "        return inputs\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"Returns all layer parameters as a single flattened array.\"\"\"\n",
        "        params = np.concatenate([layer.get_params() for layer in self.layers if isinstance(layer, Dense)])\n",
        "        return params\n",
        "\n",
        "    def set_params(self, params):\n",
        "        \"\"\"Sets all layer parameters from a single flattened array.\"\"\"\n",
        "        start_idx = 0\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Dense):\n",
        "                layer_params_size = layer.get_params().size\n",
        "                layer.set_params(params[start_idx:start_idx + layer_params_size])\n",
        "                start_idx += layer_params_size\n",
        "\n",
        "    def loss_and_grad(self, params, x_train, y_train):\n",
        "        \"\"\"Calculates loss and gradient based on provided parameters.\"\"\"\n",
        "        self.set_params(params)\n",
        "        predictions = self.forward(x_train)\n",
        "\n",
        "        loss_value = self.loss.forward(predictions, y_train)\n",
        "        grad_loss = self.loss.backward(predictions, y_train)\n",
        "\n",
        "        grad_params = []\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Dense):\n",
        "                grad_weights = np.dot(layer.inputs.T, grad_loss)\n",
        "                grad_biases = np.sum(grad_loss, axis=0, keepdims=True)\n",
        "\n",
        "                grad_params.append(grad_weights.flatten())\n",
        "                grad_params.append(grad_biases.flatten())\n",
        "\n",
        "                grad_loss = np.dot(grad_loss, layer.weights.T)\n",
        "\n",
        "        grad_params = np.concatenate(grad_params)\n",
        "        return loss_value, grad_params\n",
        "\n",
        "    def k_bfgs(self, x_train, y_train, epochs=1, verbose=True):\n",
        "        \"\"\"Trains the model using the K-BFGS algorithm.\"\"\"\n",
        "        params = self.get_params()\n",
        "\n",
        "        # Initialize memory for Hessian approximation\n",
        "        s_list, y_list = [], []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            loss_value, grad_params = self.loss_and_grad(params, x_train, y_train)\n",
        "\n",
        "            # Form Hessian approximation from s and y lists\n",
        "            H = np.eye(len(params))\n",
        "\n",
        "            for s, y in zip(s_list, y_list):\n",
        "                rho = 1 / np.dot(y, s)\n",
        "                V = H - rho * np.outer(np.dot(H, s), y)\n",
        "                H = V + rho * np.outer(s, s)\n",
        "\n",
        "            # Compute search direction\n",
        "            direction = -np.dot(H, grad_params)\n",
        "\n",
        "            # Line search to find suitable step size\n",
        "            step_size = 1.0\n",
        "\n",
        "            def line_search_loss(step):\n",
        "                new_params = params + step * direction\n",
        "                new_loss, _ = self.loss_and_grad(new_params, x_train, y_train)\n",
        "                return new_loss\n",
        "\n",
        "            # Try step size reductions until loss decreases\n",
        "            while line_search_loss(step_size) >= loss_value:\n",
        "                step_size *= 0.5\n",
        "                if step_size < 1e-10:\n",
        "                    break\n",
        "\n",
        "            new_params = params + step_size * direction\n",
        "\n",
        "            # Update s and y lists\n",
        "            s = new_params - params\n",
        "            new_loss, new_grad_params = self.loss_and_grad(new_params, x_train, y_train)\n",
        "            y = new_grad_params - grad_params\n",
        "\n",
        "            if len(s_list) >= 10:  # Limit to 10 recent updates\n",
        "                s_list.pop(0)\n",
        "                y_list.pop(0)\n",
        "\n",
        "            s_list.append(s)\n",
        "            y_list.append(y)\n",
        "\n",
        "            params = new_params\n",
        "\n",
        "            if verbose:\n",
        "                print(f'Epoch {epoch}, Loss: {loss_value}')\n",
        "\n",
        "        self.set_params(params)\n",
        "\n",
        "def antiCategorical(arr):\n",
        "    return np.argmax(arr, axis=1)\n",
        "\n",
        "def main():\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    y_train_c = np.array(to_categorical(y_train))\n",
        "    y_test_c = np.array(to_categorical(y_test))\n",
        "\n",
        "    x_train = np.array(x_train.reshape(-1, 784) / 255.)\n",
        "    x_test = np.array(x_test.reshape(-1, 784) / 255.)\n",
        "\n",
        "    layers = [\n",
        "        Dense(784, 128),\n",
        "        ReLU(),\n",
        "        Dense(128, 10),\n",
        "        Softmax(),\n",
        "    ]\n",
        "\n",
        "    nn = NeuralNet(layers, loss=MeanSquaredError())\n",
        "\n",
        "    nn.k_bfgs(x_train, y_train_c, epochs=10, verbose=True)\n",
        "\n",
        "    predictions = nn.forward(x_test)\n",
        "\n",
        "    preds = antiCategorical(predictions)\n",
        "    expected = antiCategorical(y_test_c)\n",
        "\n",
        "    print(f'Accuracy: {sklearn.metrics.accuracy_score(expected, preds)}')\n",
        "    print(f'Confusion Matrix: {sklearn.metrics.confusion_matrix(expected, preds)}')\n",
        "    print(f'Classification Report: {sklearn.metrics.classification_report(expected, preds)}')\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "walR85hABMuW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "LkT5n07Hc0et",
        "outputId": "3a626ecb-159e-4bec-bb5f-c11172af16d7"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "only integer scalar arrays can be converted to a scalar index",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-28f4c2d15334>\u001b[0m in \u001b[0;36m<cell line: 142>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-28f4c2d15334>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# Train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Evaluate on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-28f4c2d15334>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, epochs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-28f4c2d15334>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Initialize neuron activations with input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_neurons\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Propagate for multiple passes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_data():\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    y_train_c = np.array(to_categorical(y_train))\n",
        "    y_test_c = np.array(to_categorical(y_test))\n",
        "\n",
        "    x_train = np.array(x_train.reshape(-1, 784) / 255.)\n",
        "    x_test = np.array(x_test.reshape(-1, 784) / 255.)\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    return x_train, x_test, y_train_c, y_test_c\n",
        "\n",
        "# Utility functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Mesh Neural Network Class\n",
        "class MeshNeuralNetwork:\n",
        "    def __init__(self, num_neurons, input_size, output_size, num_passes=3, learning_rate=0.001):\n",
        "        self.num_neurons = num_neurons\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.num_passes = num_passes\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize neurons\n",
        "        self.neurons = [None] * num_neurons\n",
        "        self.connections = np.random.choice(\n",
        "            np.arange(num_neurons),\n",
        "            size=(num_neurons, num_neurons),\n",
        "            replace=True\n",
        "        )\n",
        "\n",
        "        # Initialize weights and biases randomly\n",
        "        self.weights = np.random.randn(num_neurons, num_neurons)\n",
        "        self.biases = np.random.randn(num_neurons)\n",
        "\n",
        "        # Select random input/output neurons\n",
        "        self.input_neurons = np.random.choice(np.arange(num_neurons), size=input_size, replace=False)\n",
        "        self.output_neurons = np.random.choice(np.arange(num_neurons), size=output_size, replace=False)\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        # Initialize neuron activations with input data\n",
        "        self.neurons[self.input_neurons] = X\n",
        "\n",
        "        # Propagate for multiple passes\n",
        "        for _ in range(self.num_passes):\n",
        "            for i in range(self.num_neurons):\n",
        "                # Update each neuron activation based on connections\n",
        "                inputs = self.weights[i, self.connections[i]]\n",
        "                activation_input = np.dot(inputs, self.neurons[self.connections[i]]) + self.biases[i]\n",
        "                self.neurons[i] = relu(activation_input)\n",
        "\n",
        "        # Output activations\n",
        "        return self.neurons[self.output_neurons]\n",
        "\n",
        "    def backward_pass(self, X, y_true, y_pred):\n",
        "        # Loss gradient (Cross-Entropy Loss derivative)\n",
        "        loss_grad = y_pred - y_true\n",
        "\n",
        "        # Backpropagate through output neurons first\n",
        "        for i in self.output_neurons:\n",
        "            # Compute gradients w.r.t weights and biases\n",
        "            neuron_output = self.neurons[i]\n",
        "            relu_grad = relu_derivative(neuron_output)\n",
        "            grad_w = np.outer(loss_grad[i], self.neurons[self.connections[i]]) * relu_grad\n",
        "            grad_b = loss_grad[i] * relu_grad\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights[i, self.connections[i]] -= self.learning_rate * grad_w\n",
        "            self.biases[i] -= self.learning_rate * grad_b\n",
        "\n",
        "        # Backpropagate further into the network\n",
        "        for i in reversed(range(self.num_neurons)):\n",
        "            if i in self.output_neurons:\n",
        "                continue\n",
        "\n",
        "            neuron_output = self.neurons[i]\n",
        "            relu_grad = relu_derivative(neuron_output)\n",
        "            grad_w = np.outer(loss_grad[i], self.neurons[self.connections[i]]) * relu_grad\n",
        "            grad_b = loss_grad[i] * relu_grad\n",
        "\n",
        "            self.weights[i, self.connections[i]] -= self.learning_rate * grad_w\n",
        "            self.biases[i] -= self.learning_rate * grad_b\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            for X_batch, y_batch in zip(X_train, y_train):\n",
        "                y_pred = self.forward_pass(X_batch)\n",
        "                self.backward_pass(X_batch, y_batch, y_pred)\n",
        "\n",
        "            # Output training metrics\n",
        "            train_loss, train_accuracy = self.evaluate(X_train, y_train)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        correct_predictions = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        for X_sample, y_true in zip(X, y):\n",
        "            y_pred = self.forward_pass(X_sample)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
        "            total_loss += loss\n",
        "\n",
        "            # Compute accuracy\n",
        "            correct_predictions += np.argmax(y_pred) == np.argmax(y_true)\n",
        "\n",
        "        return total_loss / len(X), correct_predictions / len(X)\n",
        "\n",
        "# Main Training Loop\n",
        "def main():\n",
        "    X_train, X_test, y_train, y_test = preprocess_data()\n",
        "\n",
        "    # Convert to arrays for easier manipulation\n",
        "    X_train, X_test = np.array(X_train), np.array(X_test)\n",
        "    y_train, y_test = np.array(y_train), np.array(y_test)\n",
        "\n",
        "    # Initialize network with a suitable number of neurons, input size, and output size\n",
        "    input_size, output_size = X_train.shape[1], y_train.shape[1]\n",
        "    num_neurons = 2000  # Example number\n",
        "    network = MeshNeuralNetwork(num_neurons=num_neurons, input_size=input_size, output_size=output_size, num_passes=3)\n",
        "\n",
        "    # Train the network\n",
        "    network.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_loss, test_accuracy = network.evaluate(X_test, y_test)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tunb6mihc5us"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pARZuRAtDgY0",
        "XXIwFDrQ_dBr"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
